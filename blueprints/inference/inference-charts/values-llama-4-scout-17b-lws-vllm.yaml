modelParameters:
  modelId: meta-llama/Llama-4-Scout-17B-16E-Instruct
  gpuMemoryUtilization: 0.8
  maxModelLen: 1024
  maxNumSeqs: 1
  maxNumBatchedTokens: 8192
  tokenizerPoolSize: 4
  maxParallelLoadingWorkers: 2
  pipelineParallelSize: 2
  tensorParallelSize: 8
  enablePrefixCaching: false
  numGpus: 8

inference:
  serviceName: llama-4-scout-17b-lws-vllm
  serviceNamespace: default
  accelerator: gpu
  framework: lws-vllm
  modelServer:
    image:
      repository: vllm/vllm-openai
      tag: v0.9.1
    deployment:
      resources:
        gpu:
          requests:
            nvidia.com/gpu: 8
          limits:
            nvidia.com/gpu: 8
      instanceType: g5.48xlarge # Highly recommended to add the instance type
