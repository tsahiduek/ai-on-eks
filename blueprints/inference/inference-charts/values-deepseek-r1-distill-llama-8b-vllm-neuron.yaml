modelParameters:
  modelId: deepseek-ai/DeepSeek-R1-Distill-Llama-8B
  gpuMemoryUtilization: 0.8
  maxModelLen: 1024
  maxNumSeqs: 1
  maxNumBatchedTokens: 1024
  tokenizerPoolSize: 4
  maxParallelLoadingWorkers: 2
  pipelineParallelSize: 1
  tensorParallelSize: 2
  enablePrefixCaching: false
  numGpus: 1

inference:
  serviceName: deepseekr1-dis-llama-8b-vllm-nrn
  serviceNamespace: default
  accelerator: neuron
  framework: vllm

  modelServer:
    image:
      repository: public.ecr.aws/q9t5s3a7/vllm-neuron-release-repo
      tag: v0.9.1
    deployment:
      resources:
        neuron:
          requests:
            memory: 50Gi
