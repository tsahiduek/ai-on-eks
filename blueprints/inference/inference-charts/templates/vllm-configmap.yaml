{{- if or (eq .Values.inference.framework "vllm") (eq .Values.inference.framework "aibrix") }}
apiVersion: v1
kind: ConfigMap
metadata:
  name: vllm-serve
  labels:
    "app.kubernetes.io/component": "{{.Values.inference.serviceName}}"
data:
  vllm_serve.py: |
    import re
    from typing import Optional
    from fastapi import FastAPI
    from prometheus_client import make_asgi_app
    from starlette.routing import Mount
    from vllm.engine.arg_utils import AsyncEngineArgs
    from vllm.engine.async_llm_engine import AsyncLLMEngine
    from vllm.entrypoints.openai.serving_models import BaseModelPath,OpenAIServingModels
    from starlette.requests import Request
    from starlette.responses import StreamingResponse, JSONResponse
    import os
    import logging
    from vllm.entrypoints.openai.serving_chat import OpenAIServingChat
    from vllm.entrypoints.openai.serving_completion import OpenAIServingCompletion
    from vllm.transformers_utils.config import get_config
    from huggingface_hub import login
    from vllm.entrypoints.openai.protocol import (
        CompletionRequest,
        CompletionResponse,
        ChatCompletionRequest,
        ChatCompletionResponse,
        ErrorResponse,
    )
    import uvicorn
    logger = logging.getLogger("vllm")
    app = FastAPI()
    deployment = None

    def get_model_config(model_path: str) -> dict:
        try:
            config = get_config(model_path)
            return {
                "architecture": config.architectures[0] if config.architectures else None,
                "vocab_size": config.vocab_size,
                "hidden_size": config.hidden_size,
                "num_hidden_layers": config.num_hidden_layers,
                "num_attention_heads": config.num_attention_heads,
            }
        except Exception as e:
            logger.warning(f"Error reading model config: {e}")
            return {}

    class VLLMDeployment:
        def __init__(self, response_role: str = "assistant",
                     chat_template: Optional[str] = None, **kwargs):
            route = Mount("/metrics", make_asgi_app())
            route.path_regex = re.compile('^/metrics(?P<path>.*)')
            app.routes.append(route)

            hf_token = os.getenv("HUGGING_FACE_HUB_TOKEN")
            if not hf_token:
                logger.warning("HUGGING_FACE_HUB_TOKEN environment variable is not set")
            else:
                login(token=hf_token)
                logger.info("Successfully logged in to Hugging Face Hub")

            self.engine_args = AsyncEngineArgs(
                model=os.getenv("MODEL_ID", "NousResearch/Llama-3.2-1B"),
                dtype="auto",
                gpu_memory_utilization=float(os.getenv("GPU_MEMORY_UTILIZATION", "0.8")),
                max_model_len=int(os.getenv("MAX_MODEL_LEN", "8192")),
                max_num_seqs=int(os.getenv("MAX_NUM_SEQ", "4")),
                max_num_batched_tokens=int(os.getenv("MAX_NUM_BATCHED_TOKENS", "8192")),
                trust_remote_code=True,
                enable_chunked_prefill=False,
                tokenizer_pool_size=int(os.getenv("TOKENIZER_POOL_SIZE", "4")),
                pipeline_parallel_size=int(os.getenv("PIPELINE_PARALLEL_SIZE", "1")),
                tensor_parallel_size=int(os.getenv("TENSOR_PARALLEL_SIZE", "1")),
                enable_prefix_caching=bool(os.getenv("ENABLE_PREFIX_CACHING", "true")),
                served_model_name=(os.getenv("SERVED_MODEL_NAME", os.getenv("MODEL_ID", "NousResearch/Llama-3.2-1B"))),
                enforce_eager=True,
            )
            self.openai_serving_chat = None
            self.openai_serving = None
            self.response_role = response_role
            self.chat_template = chat_template
            self.engine = AsyncLLMEngine.from_engine_args(self.engine_args)
            self.max_model_len = self.engine_args.max_model_len
            logger.info(f"VLLM Engine initialized with max_model_len: {self.max_model_len}")

        async def get_models(self):
            model_info = {
                "object": "list",
                "data": [
                    {
                        "id": self.engine_args.model,
                        "object": "model",
                        "owned_by": "organization",
                        "permission": []
                    }
                ]
            }
            return JSONResponse(content=model_info)

        async def create_completion(self, request: CompletionRequest, raw_request: Request):
            if not self.openai_serving:
                model_config = await self.engine.get_model_config()
                base_model_paths = [BaseModelPath(name=self.engine_args.served_model_name, model_path=self.engine_args.model)]
                openai_serving_models = OpenAIServingModels(
                    engine_client=self.engine,
                    model_config=model_config,
                    base_model_paths=base_model_paths,
                )
                self.openai_serving = OpenAIServingCompletion(
                    engine_client=self.engine,
                    model_config=model_config,
                    models=openai_serving_models,
                    request_logger=None
                )
            logger.info(f"Request: {request}")
            generator = await self.openai_serving.create_completion(
                request, raw_request
            )
            if isinstance(generator, ErrorResponse):
                return JSONResponse(
                    content=generator.model_dump(), status_code=generator.code
                )
            if request.stream:
                return StreamingResponse(content=generator, media_type="text/event-stream")
            else:
                assert isinstance(generator, CompletionResponse)
                return JSONResponse(content=generator.model_dump())

        async def create_chat_completion(self, request: ChatCompletionRequest, raw_request: Request):
            if not self.openai_serving_chat:
                model_config = await self.engine.get_model_config()
                base_model_paths = [BaseModelPath(name=self.engine_args.served_model_name, model_path=self.engine_args.model)]
                openai_serving_models = OpenAIServingModels(
                    engine_client=self.engine,
                    model_config=model_config,
                    base_model_paths=base_model_paths,
                )
                self.openai_serving_chat = OpenAIServingChat(
                    engine_client=self.engine,
                    model_config=model_config,
                    models=openai_serving_models,
                    response_role=self.response_role,
                    request_logger=None,
                    chat_template=self.chat_template,
                    chat_template_content_format="auto"
                )
            logger.info(f"Request: {request}")
            generator = await self.openai_serving_chat.create_chat_completion(
                request, raw_request
            )
            if isinstance(generator, ErrorResponse):
                return JSONResponse(
                    content=generator.model_dump(), status_code=generator.code
                )
            if request.stream:
                return StreamingResponse(content=generator, media_type="text/event-stream")
            else:
                assert isinstance(generator, ChatCompletionResponse)
                return JSONResponse(content=generator.model_dump())

    def create_deployment():
        global deployment
        deployment = VLLMDeployment()
        return deployment

    @app.on_event("startup")
    async def startup_event():
        create_deployment()

    @app.get("/v1/models")
    async def get_models():
        return await deployment.get_models()

    @app.post("/v1/completions")
    async def create_completion(request: CompletionRequest, raw_request: Request):
        return await deployment.create_completion(request, raw_request)

    @app.post("/v1/chat/completions")
    async def create_chat_completion(request: ChatCompletionRequest, raw_request: Request):
        return await deployment.create_chat_completion(request, raw_request)

    if __name__ == "__main__":
        uvicorn.run(app, host="0.0.0.0", port=8000)
{{- end }}
