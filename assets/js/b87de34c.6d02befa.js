"use strict";(self.webpackChunkdoeks_website=self.webpackChunkdoeks_website||[]).push([[3887],{28453:(e,n,l)=>{l.d(n,{R:()=>d,x:()=>c});var r=l(96540);const s={},i=r.createContext(s);function d(e){const n=r.useContext(i);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:d(e.components),r.createElement(i.Provider,{value:n},e.children)}},89220:(e,n,l)=>{l.r(n),l.d(n,{assets:()=>t,contentTitle:()=>c,default:()=>h,frontMatter:()=>d,metadata:()=>r,toc:()=>a});const r=JSON.parse('{"id":"blueprints/inference/inference-charts","title":"AI on EKS Inference Charts","description":"The AI on EKS Inference Charts provide a streamlined Helm-based approach to deploy AI/ML inference workloads on both GPU","source":"@site/docs/blueprints/inference/inference-charts.md","sourceDirName":"blueprints/inference","slug":"/blueprints/inference/inference-charts","permalink":"/ai-on-eks/docs/blueprints/inference/inference-charts","draft":false,"unlisted":false,"editUrl":"https://github.com/awslabs/ai-on-eks/blob/main/website/docs/blueprints/inference/inference-charts.md","tags":[],"version":"current","frontMatter":{"sidebar_label":"Inference Charts"},"sidebar":"blueprints","previous":{"title":"Overview","permalink":"/ai-on-eks/docs/blueprints/inference/"}}');var s=l(74848),i=l(28453);const d={sidebar_label:"Inference Charts"},c="AI on EKS Inference Charts",t={},a=[{value:"Overview",id:"overview",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Quick Start",id:"quick-start",level:2},{value:"1. Create Hugging Face Token Secret",id:"1-create-hugging-face-token-secret",level:3},{value:"2. Deploy a Pre-configured Model",id:"2-deploy-a-pre-configured-model",level:3},{value:"Supported Models",id:"supported-models",level:2},{value:"GPU Models",id:"gpu-models",level:3},{value:"Neuron Models (AWS Inferentia/Trainium)",id:"neuron-models-aws-inferentiatrainium",level:3},{value:"Deployment Examples",id:"deployment-examples",level:2},{value:"GPU Deployments",id:"gpu-deployments",level:3},{value:"Deploy Llama 3.2 1B with VLLM",id:"deploy-llama-32-1b-with-vllm",level:4},{value:"Deploy DeepSeek R1 Distill with Ray-VLLM",id:"deploy-deepseek-r1-distill-with-ray-vllm",level:4},{value:"Deploy Mistral Small 24B with Ray-VLLM",id:"deploy-mistral-small-24b-with-ray-vllm",level:4},{value:"Neuron Deployments",id:"neuron-deployments",level:3},{value:"Deploy Llama 3.1 8B with VLLM on Inferentia",id:"deploy-llama-31-8b-with-vllm-on-inferentia",level:4},{value:"Deploy Llama 3 70B with Ray-VLLM on Inferentia",id:"deploy-llama-3-70b-with-ray-vllm-on-inferentia",level:4},{value:"Configuration Options",id:"configuration-options",level:2},{value:"Key Parameters",id:"key-parameters",level:3},{value:"Custom Deployment",id:"custom-deployment",level:3},{value:"API Endpoints",id:"api-endpoints",level:2},{value:"Example API Usage",id:"example-api-usage",level:3},{value:"Monitoring and Observability",id:"monitoring-and-observability",level:2},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues",id:"common-issues",level:3},{value:"Logs",id:"logs",level:3},{value:"Next Steps",id:"next-steps",level:2}];function o(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"ai-on-eks-inference-charts",children:"AI on EKS Inference Charts"})}),"\n",(0,s.jsx)(n.p,{children:"The AI on EKS Inference Charts provide a streamlined Helm-based approach to deploy AI/ML inference workloads on both GPU\nand AWS Neuron (Inferentia/Trainium) hardware. This chart supports multiple deployment configurations and comes with\npre-configured values for popular models."}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"The inference charts support the following deployment types:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"GPU-based VLLM deployments"})," - Single-node VLLM inference"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"GPU-based Ray-VLLM deployments"})," - Distributed VLLM inference with Ray"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Neuron-based VLLM deployments"})," - VLLM inference on AWS Inferentia chips"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Neuron-based Ray-VLLM deployments"})," - Distributed VLLM inference with Ray on Inferentia"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsx)(n.p,{children:"Before deploying the inference charts, ensure you have:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Amazon EKS cluster with GPU or AWS Neuron nodes (",(0,s.jsx)(n.a,{href:"/ai-on-eks/docs/infra/ai-ml/jark",children:"JARK-stack"})," for a quick start)"]}),"\n",(0,s.jsx)(n.li,{children:"Helm 3.0+"}),"\n",(0,s.jsx)(n.li,{children:"For GPU deployments: NVIDIA device plugin installed"}),"\n",(0,s.jsx)(n.li,{children:"For Neuron deployments: AWS Neuron device plugin installed"}),"\n",(0,s.jsx)(n.li,{children:"Hugging Face Hub token (stored as a Kubernetes secret)"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"quick-start",children:"Quick Start"}),"\n",(0,s.jsx)(n.h3,{id:"1-create-hugging-face-token-secret",children:"1. Create Hugging Face Token Secret"}),"\n",(0,s.jsxs)(n.p,{children:["Create a Kubernetes secret with your ",(0,s.jsx)(n.a,{href:"https://huggingface.co/docs/hub/en/security-tokens",children:"Hugging Face token"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl create secret generic hf-token --from-literal=token=your_huggingface_token\n"})}),"\n",(0,s.jsx)(n.h3,{id:"2-deploy-a-pre-configured-model",children:"2. Deploy a Pre-configured Model"}),"\n",(0,s.jsx)(n.p,{children:"Choose from the available pre-configured models and deploy:"}),"\n",(0,s.jsx)(n.admonition,{type:"warning",children:(0,s.jsxs)(n.p,{children:["These deployments will need GPU/Neuron resources which need to\nbe ",(0,s.jsx)(n.a,{href:"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-resource-limits.html",children:"enabled"})," and cost more than CPU only\ninstances."]})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Deploy Llama 3.2 1B on GPU with VLLM\nhelm install llama-inference ./blueprints/inference/inference-charts \\\n  --values ./blueprints/inference/inference-charts/values-llama-32-1b-vllm.yaml\n\n# Deploy DeepSeek R1 Distill on GPU with Ray-VLLM\nhelm install deepseek-inference ./blueprints/inference/inference-charts \\\n  --values ./blueprints/inference/inference-charts/values-deepseek-r1-distill-llama-8b-ray-vllm-gpu.yaml\n"})}),"\n",(0,s.jsx)(n.h2,{id:"supported-models",children:"Supported Models"}),"\n",(0,s.jsx)(n.p,{children:"The inference charts include pre-configured values files for the following models:"}),"\n",(0,s.jsx)(n.h3,{id:"gpu-models",children:"GPU Models"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Model"}),(0,s.jsx)(n.th,{children:"Size"}),(0,s.jsx)(n.th,{children:"Framework"}),(0,s.jsx)(n.th,{children:"Values File"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"DeepSeek R1 Distill Llama"})}),(0,s.jsx)(n.td,{children:"8B"}),(0,s.jsx)(n.td,{children:"Ray-VLLM"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"values-deepseek-r1-distill-llama-8b-ray-vllm-gpu.yaml"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Llama 3.2"})}),(0,s.jsx)(n.td,{children:"1B"}),(0,s.jsx)(n.td,{children:"VLLM"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"values-llama-32-1b-vllm.yaml"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Llama 3.2"})}),(0,s.jsx)(n.td,{children:"1B"}),(0,s.jsx)(n.td,{children:"Ray-VLLM"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"values-llama-32-1b-ray-vllm.yaml"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Llama 4 Scout"})}),(0,s.jsx)(n.td,{children:"17B"}),(0,s.jsx)(n.td,{children:"VLLM"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"values-llama-4-scout-17b-vllm.yaml"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Mistral Small"})}),(0,s.jsx)(n.td,{children:"24B"}),(0,s.jsx)(n.td,{children:"Ray-VLLM"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"values-mistral-small-24b-ray-vllm.yaml"})})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"neuron-models-aws-inferentiatrainium",children:"Neuron Models (AWS Inferentia/Trainium)"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Model"}),(0,s.jsx)(n.th,{children:"Size"}),(0,s.jsx)(n.th,{children:"Framework"}),(0,s.jsx)(n.th,{children:"Values File"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"DeepSeek R1 Distill Llama"})}),(0,s.jsx)(n.td,{children:"8B"}),(0,s.jsx)(n.td,{children:"VLLM"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"values-deepseek-r1-distill-llama-8b-vllm-neuron.yaml"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Llama 2"})}),(0,s.jsx)(n.td,{children:"13B"}),(0,s.jsx)(n.td,{children:"Ray-VLLM"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"values-llama-2-13b-ray-vllm-neuron.yaml"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Llama 3"})}),(0,s.jsx)(n.td,{children:"70B"}),(0,s.jsx)(n.td,{children:"Ray-VLLM"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"values-llama-3-70b-ray-vllm-neuron.yaml"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Llama 3.1"})}),(0,s.jsx)(n.td,{children:"8B"}),(0,s.jsx)(n.td,{children:"VLLM"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"values-llama-31-8b-vllm-neuron.yaml"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Llama 3.1"})}),(0,s.jsx)(n.td,{children:"8B"}),(0,s.jsx)(n.td,{children:"Ray-VLLM"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"values-llama-31-8b-ray-vllm-neuron.yaml"})})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"deployment-examples",children:"Deployment Examples"}),"\n",(0,s.jsx)(n.h3,{id:"gpu-deployments",children:"GPU Deployments"}),"\n",(0,s.jsx)(n.h4,{id:"deploy-llama-32-1b-with-vllm",children:"Deploy Llama 3.2 1B with VLLM"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"helm install llama32-vllm ./blueprints/inference/inference-charts \\\n  --values ./blueprints/inference/inference-charts/values-llama-32-1b-vllm.yaml\n"})}),"\n",(0,s.jsx)(n.h4,{id:"deploy-deepseek-r1-distill-with-ray-vllm",children:"Deploy DeepSeek R1 Distill with Ray-VLLM"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"helm install deepseek-ray ./blueprints/inference/inference-charts \\\n  --values ./blueprints/inference/inference-charts/values-deepseek-r1-distill-llama-8b-ray-vllm-gpu.yaml\n"})}),"\n",(0,s.jsx)(n.h4,{id:"deploy-mistral-small-24b-with-ray-vllm",children:"Deploy Mistral Small 24B with Ray-VLLM"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"helm install mistral-ray ./blueprints/inference/inference-charts \\\n  --values ./blueprints/inference/inference-charts/values-mistral-small-24b-ray-vllm.yaml\n"})}),"\n",(0,s.jsx)(n.h3,{id:"neuron-deployments",children:"Neuron Deployments"}),"\n",(0,s.jsx)(n.h4,{id:"deploy-llama-31-8b-with-vllm-on-inferentia",children:"Deploy Llama 3.1 8B with VLLM on Inferentia"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"helm install llama31-neuron ./blueprints/inference/inference-charts \\\n  --values ./blueprints/inference/inference-charts/values-llama-31-8b-vllm-neuron.yaml\n"})}),"\n",(0,s.jsx)(n.h4,{id:"deploy-llama-3-70b-with-ray-vllm-on-inferentia",children:"Deploy Llama 3 70B with Ray-VLLM on Inferentia"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"helm install llama3-70b-neuron ./blueprints/inference/inference-charts \\\n  --values ./blueprints/inference/inference-charts/values-llama-3-70b-ray-vllm-neuron.yaml\n"})}),"\n",(0,s.jsx)(n.h2,{id:"configuration-options",children:"Configuration Options"}),"\n",(0,s.jsx)(n.h3,{id:"key-parameters",children:"Key Parameters"}),"\n",(0,s.jsx)(n.p,{children:"The chart provides extensive configuration options. Here are the most important parameters:"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Parameter"}),(0,s.jsx)(n.th,{children:"Description"}),(0,s.jsx)(n.th,{children:"Default"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"inference.accelerator"})}),(0,s.jsxs)(n.td,{children:["Accelerator type (",(0,s.jsx)(n.code,{children:"gpu"})," or ",(0,s.jsx)(n.code,{children:"neuron"}),")"]}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"gpu"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"inference.framework"})}),(0,s.jsxs)(n.td,{children:["Framework type (",(0,s.jsx)(n.code,{children:"vllm"})," or ",(0,s.jsx)(n.code,{children:"ray-vllm"}),")"]}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"vllm"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"inference.serviceName"})}),(0,s.jsx)(n.td,{children:"Name of the inference service"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"inference"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"inference.modelServer.deployment.replicas"})}),(0,s.jsx)(n.td,{children:"Number of replicas"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"1"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"modelParameters.modelId"})}),(0,s.jsx)(n.td,{children:"Model ID from Hugging Face Hub"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"NousResearch/Llama-3.2-1B"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"modelParameters.gpuMemoryUtilization"})}),(0,s.jsx)(n.td,{children:"GPU memory utilization"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"0.8"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"modelParameters.maxModelLen"})}),(0,s.jsx)(n.td,{children:"Maximum model sequence length"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"8192"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"modelParameters.tensorParallelSize"})}),(0,s.jsx)(n.td,{children:"Tensor parallel size"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"1"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"service.type"})}),(0,s.jsx)(n.td,{children:"Service type"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"ClusterIP"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"service.port"})}),(0,s.jsx)(n.td,{children:"Service port"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"8000"})})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"custom-deployment",children:"Custom Deployment"}),"\n",(0,s.jsx)(n.p,{children:"Create your own values file for custom configurations:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'inference:\n  accelerator: gpu  # or neuron\n  framework: vllm   # or ray-vllm\n  serviceName: custom-inference\n  modelServer:\n    deployment:\n      replicas: 2\n      resources:\n        gpu:\n          requests:\n            nvidia.com/gpu: 1\n          limits:\n            nvidia.com/gpu: 1\n\nmodelParameters:\n  modelId: "your-custom-model-id"\n  gpuMemoryUtilization: "0.9"\n  maxModelLen: "4096"\n  tensorParallelSize: "1"\n'})}),"\n",(0,s.jsx)(n.p,{children:"Deploy with custom values:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"helm install custom-inference ./blueprints/inference/inference-charts \\\n  --values custom-values.yaml\n"})}),"\n",(0,s.jsx)(n.h2,{id:"api-endpoints",children:"API Endpoints"}),"\n",(0,s.jsx)(n.p,{children:"Once deployed, the service exposes OpenAI-compatible API endpoints:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"/v1/models"})})," - List available models"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"/v1/completions"})})," - Text completion API"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"/v1/chat/completions"})})," - Chat completion API"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"/metrics"})})," - Prometheus metrics endpoint"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"example-api-usage",children:"Example API Usage"}),"\n",(0,s.jsxs)(n.p,{children:["Note: These deployments do not create an ingress, you will need to ",(0,s.jsx)(n.code,{children:"kubectl port-forward"})," to test from your machine,\neg (for deepseek):"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl get svc | grep deepseek\n# Note the service name for deepseek, in this case deepseekr1-dis-lllama-8b-ray-vllm-gpu-ray-vllm\nkubectl port-forward svc/deepseekr1-dis-llama-8b-ray-vllm-gpu-ray-vllm 8000\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'# List models\ncurl http://localhost:8000/v1/models\n\n# Chat completion\ncurl -X POST http://localhost:8000/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "your-model-name",\n    "messages": [{"role": "user", "content": "Hello!"}],\n    "max_tokens": 100\n  }\'\n'})}),"\n",(0,s.jsx)(n.h2,{id:"monitoring-and-observability",children:"Monitoring and Observability"}),"\n",(0,s.jsx)(n.p,{children:"The charts include built-in observability features:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Fluent Bit"})," for log collection"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Prometheus metrics"})," for monitoring"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Grafana dashboards"})," for visualizations"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["Access metrics at the ",(0,s.jsx)(n.code,{children:"/metrics"})," endpoint of your deployed service."]}),"\n",(0,s.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,s.jsx)(n.h3,{id:"common-issues",children:"Common Issues"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Pod stuck in Pending state"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Check if GPU/Neuron nodes are available"}),"\n",(0,s.jsx)(n.li,{children:"Verify resource requests match available hardware"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Model download failures"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Ensure Hugging Face token is correctly configured"}),"\n",(0,s.jsx)(n.li,{children:"Check network connectivity to Hugging Face Hub"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Out of memory errors"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Adjust ",(0,s.jsx)(n.code,{children:"gpuMemoryUtilization"})," parameter"]}),"\n",(0,s.jsx)(n.li,{children:"Consider using tensor parallelism for larger models"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"logs",children:"Logs"}),"\n",(0,s.jsx)(n.p,{children:"Check deployment logs:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl logs -l app=inference-server\n"})}),"\n",(0,s.jsx)(n.p,{children:"For Ray deployments, check Ray cluster status:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl exec -it <ray-head-pod> -- ray status\n"})}),"\n",(0,s.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Explore ",(0,s.jsx)(n.a,{href:"/docs/category/gpu-inference-on-eks",children:"GPU-specific configurations"})," for GPU deployments"]}),"\n",(0,s.jsxs)(n.li,{children:["Learn about ",(0,s.jsx)(n.a,{href:"/docs/category/neuron-inference-on-eks",children:"Neuron-specific configurations"})," for Inferentia deployments"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(o,{...e})}):o(e)}}}]);