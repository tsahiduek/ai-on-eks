"use strict";(self.webpackChunkdoeks_website=self.webpackChunkdoeks_website||[]).push([[440],{5683:(e,n,s)=>{s.d(n,{A:()=>r});const r=s.p+"assets/images/Slurm-on-EKS-3d159d1c9ddc8da93a871260ec2fd4d8.png"},28453:(e,n,s)=>{s.d(n,{R:()=>o,x:()=>l});var r=s(96540);const t={},i=r.createContext(t);function o(e){const n=r.useContext(i);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),r.createElement(i.Provider,{value:n},e.children)}},42450:(e,n,s)=>{s.d(n,{A:()=>m});var r=s(96540),t=s(5556),i=s.n(t),o=s(34164);const l="collapsibleContent_q3kw",a="header_QCEw",c="icon_PckA",d="content_qLC1",h="expanded_iGsi";var u=s(74848);function p({children:e,header:n}){const[s,t]=(0,r.useState)(!1);return(0,u.jsxs)("div",{className:l,children:[(0,u.jsxs)("div",{className:(0,o.A)(a,{[h]:s}),onClick:()=>{t(!s)},children:[n,(0,u.jsx)("span",{className:(0,o.A)(c,{[h]:s}),children:s?"\ud83d\udc47":"\ud83d\udc48"})]}),s&&(0,u.jsx)("div",{className:d,children:e})]})}p.propTypes={children:i().node.isRequired,header:i().node.isRequired};const m=p},47778:(e,n,s)=>{s.d(n,{A:()=>r});const r=s.p+"assets/images/GPU-Insights-d5689512edfa8778f7e26f85ece55a86.png"},69710:(e,n,s)=>{s.d(n,{A:()=>r});const r=s.p+"assets/images/EFA-Insights-529254b691ddd97e0c1076de518be80c.png"},73866:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>u,frontMatter:()=>l,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"blueprints/training/GPUs/slinky-slurm","title":"Slurm on EKS","description":"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn\'t working, it\u2019s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren\'t initializing, check the logs for Karpenter or Node groups to resolve the issue.","source":"@site/docs/blueprints/training/GPUs/slinky-slurm.md","sourceDirName":"blueprints/training/GPUs","slug":"/blueprints/training/GPUs/slinky-slurm","permalink":"/ai-on-eks/docs/blueprints/training/GPUs/slinky-slurm","draft":false,"unlisted":false,"editUrl":"https://github.com/awslabs/ai-on-eks/blob/main/website/docs/blueprints/training/GPUs/slinky-slurm.md","tags":[],"version":"current","frontMatter":{"sidebar_label":"Slurm on EKS"},"sidebar":"blueprints","previous":{"title":"BioNeMo on EKS","permalink":"/ai-on-eks/docs/blueprints/training/GPUs/bionemo"},"next":{"title":"Llama-2 with RayTrain on Trn1","permalink":"/ai-on-eks/docs/blueprints/training/Neuron/RayTrain-Llama2"}}');var t=s(74848),i=s(28453),o=s(42450);const l={sidebar_label:"Slurm on EKS"},a="Slurm on EKS",c={},d=[{value:"What is Slurm?",id:"what-is-slurm",level:3},{value:"What is the Slinky Project?",id:"what-is-the-slinky-project",level:3},{value:"Slurm on EKS Architecture",id:"slurm-on-eks-architecture",level:3},{value:"Key Features and Benefits",id:"key-features-and-benefits",level:3}];function h(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",span:"span",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"slurm-on-eks",children:"Slurm on EKS"})}),"\n",(0,t.jsx)(n.admonition,{type:"warning",children:(0,t.jsx)(n.p,{children:"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn't working, it\u2019s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren't initializing, check the logs for Karpenter or Node groups to resolve the issue."})}),"\n",(0,t.jsx)(n.h3,{id:"what-is-slurm",children:"What is Slurm?"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.a,{href:"https://slurm.schedmd.com/overview.html",children:"Slurm"})," is an open-source, highly scalable workload manager and job scheduler designed for managing compute resources on compute clusters of all sizes. It provides three core functions: allocating access to compute resources, providing a framework for launching and monitoring parallel computing jobs, and managing queues of pending work to resolve resource contention."]}),"\n",(0,t.jsx)(n.p,{children:"Slurm is widely used in AI training to manage and schedule large-scale, GPU-accelerated workloads across high-performance computing clusters. It allows researchers and engineers to efficiently allocate computing resources, including CPUs, GPUs and memory, enabling distributed training of deep learning models and large language models by spanning jobs across many nodes with fine-grained control over resource types and job priorities. Slurm\u2019s reliability, advanced scheduling features, and integration with both on-premise and cloud environments make it a preferred choice for handling the scale, throughput, and reproducibility that modern AI research and industry demand."}),"\n",(0,t.jsx)(n.h3,{id:"what-is-the-slinky-project",children:"What is the Slinky Project?"}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.a,{href:"https://github.com/SlinkyProject",children:"Slinky Project"})," is an open-source suite of integration tools designed by ",(0,t.jsx)(n.a,{href:"https://www.schedmd.com/",children:"SchedMD"})," (the lead developers of Slurm) to bring Slurm capabilities into Kubernetes, combining the best of both worlds for efficient resource management and scheduling. The Slinky Project includes a ",(0,t.jsx)(n.a,{href:"https://github.com/SlinkyProject/slurm-operator?tab=readme-ov-file#kubernetes-operator-for-slurm-clusters",children:"Kubernetes operator for Slurm clusters"}),", which implements ",(0,t.jsx)(n.a,{href:"https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/#custom-controllers",children:"custom-controllers"})," and ",(0,t.jsx)(n.a,{href:"https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/#customresourcedefinitions",children:"custom resource definitions (CRDs)"})," to manage the lifecycle of Slurm Cluster and NodeSet resources deployed within a Kubernetes environment."]}),"\n",(0,t.jsx)(n.p,{children:"This Slurm cluster includes the following components:"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Component"}),(0,t.jsx)(n.th,{children:"Description"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Controller (slurmctld)"}),(0,t.jsx)(n.td,{children:"The central management daemon that monitors resources, accepts jobs, and assigns work to compute nodes."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Accounting (slurmdbd)"}),(0,t.jsx)(n.td,{children:"Handles job accounting and user/project management through a MariaDB database backend."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Compute (slurmd)"}),(0,t.jsx)(n.td,{children:"The worker nodes that execute jobs, organized into NodeSets which can be grouped into different partitions."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Login"}),(0,t.jsx)(n.td,{children:"Provides SSH access points for users to interact with the Slurm cluster and submit jobs."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"REST API (slurmrestd)"}),(0,t.jsx)(n.td,{children:"Offers HTTP-based API access to Slurm functionality for programmatic interaction with the cluster."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Authentication (sackd)"}),(0,t.jsx)(n.td,{children:"Manages credential authentication for secure access to Slurm services."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"MariaDB"}),(0,t.jsx)(n.td,{children:"The database backend used by the accounting service to store job, user, and project information."})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Slurm Exporter"}),(0,t.jsx)(n.td,{children:"Collects and exports Slurm metrics for monitoring purposes."})]})]})]}),"\n",(0,t.jsx)(n.p,{children:"When paired with Amazon EKS, the Slinky Project unlocks the ability for enterprises who have standardized infrastructure management on Kubernetes to deliver a Slurm-based experience to their ML scientists. It also enables training, experimentation, and inference to happen on the same cluster of accelerated nodes."}),"\n",(0,t.jsx)(n.h3,{id:"slurm-on-eks-architecture",children:"Slurm on EKS Architecture"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"alt text",src:s(5683).A+"",width:"836",height:"689"})}),"\n",(0,t.jsx)(n.p,{children:"The diagram above depicts the Slurm on EKS deployment outlined in this guide. An Amazon EKS cluster acts as an orchestration layer, with core Slurm Cluster components hosted on a managed node group of m5.xlarge instances, while a Karpenter NodePool manages the deployment of GPU accelerated compute nodes for the slurmd pods to run on. The Slinky Slurm operator and Slurm cluster are automatically deployed as ArgoCD applications."}),"\n",(0,t.jsxs)(n.p,{children:["The login LoadBalancer type service is annotated to dynamically create an AWS Network Load Balancer using the ",(0,t.jsx)(n.a,{href:"https://github.com/kubernetes-sigs/aws-load-balancer-controller",children:"AWS Load Balancer Controller"}),", allowing ML scientists to SSH into the login pod without interfacing with the Kubernetes API server via kubectl."]}),"\n",(0,t.jsxs)(n.p,{children:["The login and slurmd pods also have an ",(0,t.jsx)(n.a,{href:"https://aws.amazon.com/fsx/lustre/",children:"Amazon FSx for Lustre"})," shared filesystem mounted. Having containerized slurmd pods allows many dependencies that would traditionally be installed manually using Conda or a Python virtual environment to be baked into the container image, but shared filesystems are still beneficial for storing training artifacts, data, logs, and checkpoints."]}),"\n",(0,t.jsx)(n.h3,{id:"key-features-and-benefits",children:"Key Features and Benefits"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Run Slurm workloads side by side with containerized Kubernetes applications on the same infrastructure. Both Slurm and Kubernetes workloads can be scheduled on the same node pools, increasing utilization and avoiding resource fragmentation."}),"\n",(0,t.jsx)(n.li,{children:"Manage both Slurm jobs and Kubernetes pods seamlessly, leveraging familiar tooling from both ecosystems without sacrificing control or performance."}),"\n",(0,t.jsx)(n.li,{children:"Dynamically add or removes compute nodes in response to workload demand, autoscaling allocated resources efficiently, handling spikes and lulls in demand to reduce infrastructure costs and idle resource waste."}),"\n",(0,t.jsx)(n.li,{children:"High-availability through Kubernetes orchestration. If a controller or worker pod fails, Kubernetes automatically restarts it, reducing manual intervention."}),"\n",(0,t.jsx)(n.li,{children:"Slurm\u2019s sophisticated scheduling features (fair-share allocation, dependency management, priority scheduling) are integrated into Kubernetes, maximizing compute utilization and aligning resources with workload requirements."}),"\n",(0,t.jsx)(n.li,{children:"Slurm and its dependencies are deployed as containers, ensuring consistent deployments across environments. This reduces configuration drift and streamlines dev-to-prod transitions."}),"\n",(0,t.jsx)(n.li,{children:"Users can build Slurm images tailored to specialized needs (e.g., custom dependencies, libraries), promoting consistency and repeatability in scientific or regulated environments."}),"\n",(0,t.jsx)(n.li,{children:"Administrators can define custom Slurm clusters and node sets directly using Kubernetes Custom Resources, including partitioning compute nodes for different types of jobs (e.g., stable vs. opportunistic/backfill partitions)"}),"\n",(0,t.jsx)(n.li,{children:"Slinky integrates with monitoring stacks for both Slurm and Kubernetes, providing robust metrics and visualization for administrators and users."}),"\n"]}),"\n",(0,t.jsxs)(o.A,{header:(0,t.jsx)(n.h2,{children:(0,t.jsx)(n.span,{children:"Deploying the Solution"})}),children:[(0,t.jsx)(n.p,{children:"In this example, you will provision a Slinky Slurm cluster on Amazon EKS."}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"0. Prerequisites:"})}),(0,t.jsx)(n.p,{children:"Ensure that you have installed the following tools on your machine."}),(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html",children:"aws cli"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://Kubernetes.io/docs/tasks/tools/",children:"kubectl"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://learn.hashicorp.com/tutorials/terraform/install-cli",children:"terraform"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://docs.docker.com/engine/install/",children:"docker"})}),"\n"]}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"1. Clone the repository:"})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"git clone https://github.com/awslabs/ai-on-eks.git\n"})}),(0,t.jsx)(n.admonition,{type:"info",children:(0,t.jsxs)(n.p,{children:["If you are using profile for authentication\nset your ",(0,t.jsx)(n.code,{children:'export AWS_PROFILE="<PROFILE_name>"'})," to the desired profile name"]})}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"2. Review and customize configurations:"})}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Check available addons in ",(0,t.jsx)(n.code,{children:"infra/base/terraform/variables.tf"})]}),"\n",(0,t.jsxs)(n.li,{children:["Modify addon settings in ",(0,t.jsx)(n.code,{children:"infra/slinky-slurm/terraform/blueprint.tfvars"})," as needed."]}),"\n"]}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"3. Review the slurmd container image build automation:"})}),(0,t.jsxs)(n.p,{children:["By default, the ",(0,t.jsx)(n.code,{children:"blueprints/training/slinky-slurm/install.sh"})," script will trigger setup steps to automatically build a new slurmd container image using the ",(0,t.jsx)(n.code,{children:"blueprints/training/slinky-slurm/dlc-slurmd.Dockerfile"}),", which builds on top of an ",(0,t.jsx)(n.a,{href:"https://github.com/aws/deep-learning-containers",children:"AWS Deep Learning Container (DLC)"})," to include Python 3.12.8 + PyTorch 2.6.0 + CUDA 12.6 + NCCL 2.23.4 + EFA Installer 1.38.0 (bundled with OFI NCCL plugin) pre-installed in the container image."]}),(0,t.jsxs)(n.p,{children:["It will then create a new ECR repository and push this image to the repository. If not already present on your machine, a new SSH key ",(0,t.jsx)(n.code,{children:"~/.ssh/id_ed25519_slurm"})," will be created for Slurm login pod access as well."]}),(0,t.jsxs)(n.p,{children:["The image repository URI, image tag, and public SSH key are then used to generate a new ",(0,t.jsx)(n.code,{children:"slurm-values.yaml"})," file based on the ",(0,t.jsx)(n.code,{children:"blueprints/training/slinky-slurm/slurm-values.yaml.template"})," file to be used in the deployment of the Slurm cluster."]}),(0,t.jsx)(n.p,{children:"To customize this behavior, you can add the following optional flags:"}),(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Component"}),(0,t.jsx)(n.th,{children:"Description"}),(0,t.jsx)(n.th,{children:"Default Value"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"--repo-name"})}),(0,t.jsx)(n.td,{children:"The name of the ECR repository"}),(0,t.jsx)(n.td,{children:"dlc-slurmd"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"--tag"})}),(0,t.jsx)(n.td,{children:"The image tag"}),(0,t.jsx)(n.td,{children:"25.05.0-ubuntu24.04"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"--region"})}),(0,t.jsx)(n.td,{children:"The AWS region of your ECR repository"}),(0,t.jsxs)(n.td,{children:["inferred from the AWS CLI configuration or set to ",(0,t.jsx)(n.code,{children:"us-west-2"})]})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"--skip-build"})}),(0,t.jsx)(n.td,{children:"Set if using an existing image already in ECR"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"false"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"--skip-setup"})}),(0,t.jsxs)(n.td,{children:["Set if you previously generate a ",(0,t.jsx)(n.code,{children:"blueprints/training/slinky-slurm/slurm-values.yaml"})," file"]}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"false"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"--help"})}),(0,t.jsx)(n.td,{children:"View flag options"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.code,{children:"false"})})]})]})]}),(0,t.jsx)(n.p,{children:"For example, if you've already built and pushed a custom slurmd container image to a custom ECR repository, add the following flags and values:"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"cd ai-on-eks/blueprints/training/slinky-slurm\n./install.sh --repo-name dlc-slurmd --tag 25.05.0-ubuntu24.04 --skip-build\n"})}),(0,t.jsx)(n.p,{children:"The script will then validate that the container image exists in your ECR repo before proceeding."}),(0,t.jsxs)(n.p,{children:["If you wish to use a custom Dockerfile, simply overwrite the contents of the ",(0,t.jsx)(n.code,{children:"blueprints/training/slinky-slurm/dlc-slurmd.Dockerfile"})," before executing ",(0,t.jsx)(n.code,{children:"blueprints/training/slinky-slurm/install.sh"}),"."]}),(0,t.jsxs)(n.p,{children:["If you wish to build and push your container image without triggering a Terraform deployment, you can also run the ",(0,t.jsx)(n.code,{children:"blueprints/training/slinky-slurm/setup.sh"})," script directly using the same flags. This script will also generate a new ",(0,t.jsx)(n.code,{children:"blueprints/training/slinky-slurm/slurm-values.yaml"})," file for you."]}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"4. Trigger deployment:"})}),(0,t.jsxs)(n.p,{children:["Navigate into the ",(0,t.jsx)(n.code,{children:"slinky-slurm"})," directory and run ",(0,t.jsx)(n.code,{children:"install.sh"})," script:"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"cd ai-on-eks/blueprints/training/slinky-slurm\n./install.sh\n"})})]}),"\n",(0,t.jsxs)(o.A,{header:(0,t.jsx)(n.h3,{children:(0,t.jsx)(n.span,{children:"Verify Deployment"})}),children:[(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"0. Check Kubernetes Resources for Slurm Deployment:"})}),(0,t.jsx)(n.p,{children:"Update your local kubeconfig to access your kubernetes cluster:"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"aws eks update-kubeconfig --name slurm-on-eks\n"})}),(0,t.jsx)(n.p,{children:"Verify the deployment status of the Slinky Slurm Operator:"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"kubectl get all -n slinky\n"})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"NAME                                         READY   STATUS    RESTARTS   AGE\npod/slurm-operator-bb5c58dc6-5rsjg           1/1     Running   0          41m\npod/slurm-operator-webhook-87bc59884-vw8rx   1/1     Running   0          41m\n\nNAME                             TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE\nservice/slurm-operator           ClusterIP   None             <none>        8080/TCP,8081/TCP   41m\nservice/slurm-operator-webhook   ClusterIP   172.20.229.194   <none>        443/TCP,8081/TCP    41m\n\nNAME                                     READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/slurm-operator           1/1     1            1           41m\ndeployment.apps/slurm-operator-webhook   1/1     1            1           41m\n\nNAME                                               DESIRED   CURRENT   READY   AGE\nreplicaset.apps/slurm-operator-bb5c58dc6           1         1         1       41m\nreplicaset.apps/slurm-operator-webhook-87bc59884   1         1         1       41m\n"})}),(0,t.jsx)(n.p,{children:"Verify the deployment status the Slurm Cluster:"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"kubectl get all -n slurm\n"})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"NAME                                  READY   STATUS    RESTARTS   AGE\npod/slurm-accounting-0                1/1     Running   0          40m\npod/slurm-compute-node-0              2/2     Running   0          40m\npod/slurm-compute-node-1              2/2     Running   0          40m\npod/slurm-compute-node-2              2/2     Running   0          40m\npod/slurm-compute-node-3              2/2     Running   0          40m\npod/slurm-controller-0                3/3     Running   0          40m\npod/slurm-exporter-79c5896fbc-wrgv9   1/1     Running   0          40m\npod/slurm-login-65bc775dbf-qwtpv      1/1     Running   0          40m\npod/slurm-mariadb-0                   1/1     Running   0          40m\npod/slurm-restapi-859879fbc9-89gjw    1/1     Running   0          40m\n\nNAME                             TYPE           CLUSTER-IP       EXTERNAL-IP                                                                  PORT(S)        AGE\nservice/slurm-accounting         ClusterIP      None             <none>                                                                       6819/TCP       40m\nservice/slurm-compute            ClusterIP      None             <none>                                                                       6818/TCP       40m\nservice/slurm-controller         ClusterIP      None             <none>                                                                       6817/TCP       40m\nservice/slurm-exporter           ClusterIP      None             <none>                                                                       8080/TCP       40m\nservice/slurm-login              LoadBalancer   172.20.58.220    k8s-slurm-slurmlog-40c37c19f0-8dcdb95fe5ccfb8b.elb.us-west-2.amazonaws.com   22:31720/TCP   40m\nservice/slurm-mariadb            ClusterIP      172.20.229.199   <none>                                                                       3306/TCP       40m\nservice/slurm-mariadb-headless   ClusterIP      None             <none>                                                                       3306/TCP       40m\nservice/slurm-restapi            ClusterIP      172.20.167.17    <none>                                                                       6820/TCP       40m\n\nNAME                             READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/slurm-exporter   1/1     1            1           40m\ndeployment.apps/slurm-login      1/1     1            1           40m\ndeployment.apps/slurm-restapi    1/1     1            1           40m\n\nNAME                                        DESIRED   CURRENT   READY   AGE\nreplicaset.apps/slurm-exporter-79c5896fbc   1         1         1       40m\nreplicaset.apps/slurm-login-65bc775dbf      1         1         1       40m\nreplicaset.apps/slurm-restapi-859879fbc9    1         1         1       40m\n\nNAME                                READY   AGE\nstatefulset.apps/slurm-accounting   1/1     40m\nstatefulset.apps/slurm-controller   1/1     40m\nstatefulset.apps/slurm-mariadb      1/1     40m\n"})}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"1. Access the Slurm login Pod:"})}),(0,t.jsx)(n.p,{children:"SSH into the login pod:"}),(0,t.jsx)(n.admonition,{type:"info",children:(0,t.jsxs)(n.p,{children:["For this demonstration, the ",(0,t.jsx)(n.code,{children:"slurm-login"})," service has been dynamically annotated using ",(0,t.jsx)(n.code,{children:"service.beta.kubernetes.io/load-balancer-source-ranges"})," to restrict access to the Network Load Balancer with your IP address only. The AWS Load Balancer Controller achieves this by modifying inbound security group rules. For more information see the documentation on ",(0,t.jsx)(n.a,{href:"https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.13/guide/ingress/annotations/#access-control",children:"access control annotations"}),"."]})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'SLURM_LOGIN_HOSTNAME="$(kubectl get services \\\n -n slurm -l app.kubernetes.io/instance=slurm,app.kubernetes.io/name=login \\\n -o jsonpath="{.items[0].status.loadBalancer.ingress[0].hostname}")"\n\nssh -i ~/.ssh/id_ed25519_slurm -p 22 root@$SLURM_LOGIN_HOSTNAME\n'})}),(0,t.jsx)(n.p,{children:"Check the available nodes:"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"sinfo\n"})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\nnode         up   infinite      4   idle node-[0-3]\nall*         up   infinite      4   idle node-[0-3]\n"})}),(0,t.jsx)(n.p,{children:"Verify that the Amazon FSx for Lustre shared file system is mounted to the login pod:"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"df -h\n"})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Filesystem                Size  Used Avail Use% Mounted on\noverlay                   100G  7.5G   93G   8% /\ntmpfs                      64M     0   64M   0% /dev\n/dev/nvme0n1p1            100G  7.5G   93G   8% /run\n10.1.0.194@tcp:/d2kbdb4v  2.3T   16M  2.3T   1% /fsx\ntmpfs                      15G  4.0K   15G   1% /etc/slurm\nshm                        64M     0   64M   0% /dev/shm\ntmpfs                      15G  4.0K   15G   1% /etc/sssd/sssd.conf\ntmpfs                      15G   12K   15G   1% /etc/ssh/ssh_host_rsa_key\ntmpfs                     7.7G     0  7.7G   0% /proc/acpi\ntmpfs                     7.7G     0  7.7G   0% /sys/firmware\n"})}),(0,t.jsx)(n.p,{children:"Exit back to your machine:"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"exit\n"})}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"2. Access a Slurm Compute Pod:"})}),(0,t.jsx)(n.p,{children:"Open an interactive terminal session with one of the slurm compute nodes:"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"kubectl -n slurm exec -it pod/slurm-compute-node-0 -- bash --login\n"})}),(0,t.jsx)(n.p,{children:"Verify that the Amazon FSx for Lustre shared file system is mounted to the login pod:"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"df -h\n"})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"overlay                   838G   43G  795G   6% /\ntmpfs                      64M     0   64M   0% /dev\n/dev/root                 798M  798M     0 100% /usr/local/sbin/modprobe\n/dev/nvme2n1              838G   43G  795G   6% /run\n10.1.0.194@tcp:/d2kbdb4v  2.3T   16M  2.3T   1% /fsx\ntmpfs                     122G  4.0K  122G   1% /etc/slurm\ntmpfs                      63G     0   63G   0% /dev/shm\ntmpfs                     122G  8.0K  122G   1% /var/spool/slurmd\ntmpfs                     122G     0  122G   0% /var/log/slurm\ntmpfs                      63G   12K   63G   1% /proc/driver/nvidia\ntmpfs                      25G  3.9M   25G   1% /run/nvidia-persistenced/socket\n"})}),(0,t.jsx)(n.p,{children:"Check the installed CUDA compiler version on compute node pods:"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"nvcc --version\n"})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"# nvcc: NVIDIA (R) Cuda compiler driver\n# Copyright (c) 2005-2024 NVIDIA Corporation\n# Built on Tue_Oct_29_23:50:19_PDT_2024\n# Cuda compilation tools, release 12.6, V12.6.85\n# Build cuda_12.6.r12.6/compiler.35059454_0\n"})}),(0,t.jsx)(n.p,{children:"Check the NCCL version on compute node pods:"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"ldconfig -v | grep \"libnccl.so\" | tail -n1 | sed -r 's/^.*\\.so\\.//'\n"})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"# 2.23.4\n"})}),(0,t.jsx)(n.p,{children:"Check EFA availability:"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"fi_info -p efa\n"})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"provider: efa\n    fabric: efa\n    domain: rdmap0s29-rdm\n    version: 122.0\n    type: FI_EP_RDM\n    protocol: FI_PROTO_EFA\nprovider: efa\n    fabric: efa\n    domain: rdmap0s29-dgrm\n    version: 122.0\n    type: FI_EP_DGRAM\n    protocol: FI_PROTO_EFA\n"})}),(0,t.jsx)(n.p,{children:"Check the libfabric libraries:"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"ls /opt/amazon/efa/lib\n"})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"libfabric.a  libfabric.so  libfabric.so.1  libfabric.so.1.25.0  pkgconfig\n"})}),(0,t.jsx)(n.p,{children:"Check the OFI NCCL plugin libraries:"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"ls /opt/amazon/ofi-nccl/lib/x86_64-linux-gnu\n"})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"libnccl-net.so  libnccl-ofi-tuner.so\n"})}),(0,t.jsx)(n.p,{children:"Verify intra-node GPU topology:"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"nvidia-smi topo -m\n"})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"\tGPU0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\nGPU0\t X \t0-31\t0\t\tN/A\n"})}),(0,t.jsx)(n.p,{children:"Exit back to your machine:"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"exit\n"})})]}),"\n",(0,t.jsxs)(o.A,{header:(0,t.jsx)(n.h3,{children:(0,t.jsx)(n.span,{children:"Run FSDP Example"})}),children:[(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"0. Stage Training Artifacts:"})}),(0,t.jsx)(n.p,{children:"SSH into the login pod:"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'SLURM_LOGIN_HOSTNAME="$(kubectl get services \\\n -n slurm -l app.kubernetes.io/instance=slurm,app.kubernetes.io/name=login \\\n -o jsonpath="{.items[0].status.loadBalancer.ingress[0].hostname}")"\n\nssh -i ~/.ssh/id_ed25519_slurm -p 22 root@$SLURM_LOGIN_HOSTNAME\n'})}),(0,t.jsx)(n.p,{children:"Install Git on the login pod:"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"apt update\napt install -y git\ngit --version\n"})}),(0,t.jsx)(n.p,{children:"Change directories into the FSx mount:"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"cd /fsx\n"})}),(0,t.jsxs)(n.p,{children:["Clone the ",(0,t.jsx)(n.a,{href:"https://github.com/aws-samples/awsome-distributed-training",children:"awsome-distributed-training"})," repo:"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"git clone https://github.com/aws-samples/awsome-distributed-training/\n"})}),(0,t.jsx)(n.p,{children:"Change into the FSDP for Slurm example directory:"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"cd awsome-distributed-training/3.test_cases/pytorch/FSDP/slurm\n"})}),(0,t.jsx)(n.p,{children:"Make a new directory for your checkpoints"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"mkdir -p checkpoints\n"})}),(0,t.jsxs)(n.admonition,{type:"info",children:[(0,t.jsxs)(n.p,{children:["By default the ",(0,t.jsx)(n.code,{children:"llama2_7b-training.sbatch"})," batch training script is configured to distribute the FSDP workload across 4 nodes. The Slurm NodeSet maps to the ",(0,t.jsx)(n.code,{children:"g5-gpu-karpenter"})," NodePool via a ",(0,t.jsx)(n.code,{children:"NodeSelector.instanceType"})," value. If Karpenter is unable to find 4 on-demand or spot instances, you may need to make adjustment to the batch training script, then upload a new copy to your provisioned S3 bucket, which will sync it to the FSx for Lustre file system via a ",(0,t.jsx)(n.a,{href:"https://docs.aws.amazon.com/fsx/latest/LustreGuide/create-dra-linked-data-repo.html",children:"data repository association"}),":"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"cd ../../../infra/slinky-slurm/terraform/_LOCAL\nS3_BUCKET_NAME=$(terraform output -raw fsx_s3_bucket_name)\ncd ../../../../blueprints/training/slinky-slurm\naws s3 cp llama2_7b-training.sbatch s3://${S3_BUCKET_NAME}/\n"})})]}),(0,t.jsxs)(n.p,{children:["Copy the ",(0,t.jsx)(n.code,{children:"llama2_7b-training.sbatch"})," batch training script into the FSDP for Slurm example directory:"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"cp /fsx/data/llama2_7b-training.sbatch ./llama2_7b-training.sbatch\n"})}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"1. Configure Hugging Face Access Token:"})}),(0,t.jsxs)(n.p,{children:["Create a new ",(0,t.jsx)(n.a,{href:"https://huggingface.co/",children:"Hugging Face"})," read ",(0,t.jsx)(n.a,{href:"https://huggingface.co/docs/hub/en/security-tokens",children:"user access token"})," to stream the ",(0,t.jsx)(n.a,{href:"https://huggingface.co/datasets/allenai/c4",children:"allenai/c4"})," dataset without throttling."]}),(0,t.jsx)(n.p,{children:"Inject the new Hugging Face token into the training script:"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'NEW_TOKEN="<you-token-here>"\n'})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'sed -i "s/export HF_TOKEN=.*$/export HF_TOKEN=$NEW_TOKEN/" llama2_7b-training.sbatch\n'})}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"2. Start the training job:"})}),(0,t.jsxs)(n.p,{children:["Submit the batch training script to the Slurm Controller using the ",(0,t.jsx)(n.a,{href:"https://slurm.schedmd.com/sbatch.html",children:"sbatch"})," command:"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"sbatch llama2_7b-training.sbatch\n"})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Submitted batch job 1\n"})}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"3. Monitor trianing progess:"})}),(0,t.jsx)(n.p,{children:"Watch the output logs from the login pod:"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'export JOB_ID=$(squeue -h -u root -o "%i" | head -1)\n\ntail -f logs/llama2_7b-FSDP_${JOB_ID}.out\n'})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"2: node-0:309:309 [0] NCCL INFO AllGather: 37756928 Bytes -> Algo 1 proto 2 time 2860.208008\n1: node-3:278:278 [0] NCCL INFO AllGather: opCount 1606 sendbuff 0x93fd44a00 recvbuff 0x96d44e200 count 2359808 datatype 7 op 0 root 0 comm 0x555e2c6387e0 [nranks=4] stream 0x555e2b3a4370\n0: node-1:397:397 [0] NCCL INFO AllGather: opCount 1606 sendbuff 0x93fd44a00 recvbuff 0x96d44e200 count 2359808 datatype 7 op 0 root 0 comm 0x55c1eaee2a20 [nranks=4] stream 0x55c1e9c4f3d0\n3: node-2:278:278 [0] NCCL INFO AllGather: opCount 1606 sendbuff 0x93fd44a00 recvbuff 0x96d44e200 count 2359808 datatype 7 op 0 root 0 comm 0x5611d45babf0 [nranks=4] stream 0x5611d3327480\n1: node-3:278:278 [0] NCCL INFO AllGather: opCount 1607 sendbuff 0x940645200 recvbuff 0x96d44e200 count 2359808 datatype 7 op 0 root 0 comm 0x555e2c6387e0 [nranks=4] stream 0x555e2b3a4370\n3: node-2:278:278 [0] NCCL INFO AllGather: opCount 1607 sendbuff 0x940645200 recvbuff 0x96d44e200 count 2359808 datatype 7 op 0 root 0 comm 0x5611d45babf0 [nranks=4] stream 0x5611d3327480\n1: node-3:278:278 [0] NCCL INFO Broadcast: opCount 1608 sendbuff 0x302287400 recvbuff 0x302287400 count 1 datatype 4 op 0 root 0 comm 0x555e2c6387e0 [nranks=4] stream 0x555e2b3a4370\n0: node-1:397:397 [0] NCCL INFO AllGather: opCount 1607 sendbuff 0x940645200 recvbuff 0x96d44e200 count 2359808 datatype 7 op 0 root 0 comm 0x55c1eaee2a20 [nranks=4] stream 0x55c1e9c4f3d0\n3: node-2:278:278 [0] NCCL INFO Broadcast: opCount 1608 sendbuff 0x302287400 recvbuff 0x302287400 count 1 datatype 4 op 0 root 0 comm 0x5611d45babf0 [nranks=4] stream 0x5611d3327480\n1: node-3:278:278 [0] NCCL INFO Broadcast: opCount 1609 sendbuff 0x302287600 recvbuff 0x302287600 count 3145 datatype 1 op 0 root 0 comm 0x555e2c6387e0 [nranks=4] stream 0x555e2b3a4370\n"})}),(0,t.jsxs)(n.p,{children:["Watch the error logs from ",(0,t.jsx)(n.code,{children:"slurm-compute-node-0"})," (in a new terminal window):"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"kubectl -n slurm exec -it pod/slurm-compute-node-0 -- bash --login\n"})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'cd /fsx/awsome-distributed-training/3.test_cases/pytorch/FSDP/slurm\nexport JOB_ID=$(squeue -h -u root -o "%i" | head -1)\n\nwatch "grep \'Batch.*Loss\' logs/llama2_7b-FSDP_${JOB_ID}.err"\n'})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"2: 2025-07-17 13:14:16 I [train.py:103] Batch 0 Loss: 11.11333, Speed: 0.78 samples/sec, lr: 0.000031\n2: 2025-07-17 13:14:16 I [train.py:103] Batch 1 Loss: 11.03250, Speed: 5.84 samples/sec, lr: 0.000063\n2: 2025-07-17 13:14:17 I [train.py:103] Batch 2 Loss: 10.93887, Speed: 5.81 samples/sec, lr: 0.000094\n2: 2025-07-17 13:14:18 I [train.py:103] Batch 3 Loss: 10.66936, Speed: 5.87 samples/sec, lr: 0.000100\n2: 2025-07-17 13:14:18 I [train.py:103] Batch 4 Loss: 10.24629, Speed: 5.83 samples/sec, lr: 0.000100\n2: 2025-07-17 13:14:19 I [train.py:103] Batch 5 Loss: 10.05146, Speed: 5.87 samples/sec, lr: 0.000100\n2: 2025-07-17 13:14:20 I [train.py:103] Batch 6 Loss: 9.88661, Speed: 5.91 samples/sec, lr: 0.000100\n2: 2025-07-17 13:14:20 I [train.py:103] Batch 7 Loss: 9.78694, Speed: 5.87 samples/sec, lr: 0.000100\n2: 2025-07-17 13:14:21 I [train.py:103] Batch 8 Loss: 9.61180, Speed: 5.90 samples/sec, lr: 0.000100\n2: 2025-07-17 13:14:22 I [train.py:103] Batch 9 Loss: 9.34421, Speed: 5.83 samples/sec, lr: 0.000100\n"})}),(0,t.jsxs)(n.p,{children:["Watch squeue from ",(0,t.jsx)(n.code,{children:"slurm-compute-node-1"})," (in a new terminal window):"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"kubectl -n slurm exec -it pod/slurm-compute-node-1 -- bash --login\n"})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"# 1 second updates\nwatch -n 1 squeue\n"})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Every 1.0s: squeue                                                                                                                                node-1: Thu Jul 17 13:18:13 2025\n\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n                 1       all llama2_7     root  R       4:47      4 node-[0-3]\n"})}),(0,t.jsxs)(n.p,{children:["Watch checkpoints from ",(0,t.jsx)(n.code,{children:"slurm-compute-node-2"})," (in a new terminal window):"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"kubectl -n slurm exec -it pod/slurm-compute-node-2 -- bash --login\n"})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'cd /fsx/awsome-distributed-training/3.test_cases/pytorch/FSDP/slurm\n\n# highlight changes, show timestamps, 5 second updates\nwatch -n 5 -d "ls -lh checkpoints"\n'})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Every 5.0s: ls -lh checkpoints                                                                                                                    node-2: Thu Jul 17 13:35:32 2025\n\ntotal 175K\ndrwxr-xr-x. 2 root root 25K Jul 17 13:15 llama_v2-100steps\ndrwxr-xr-x. 2 root root 25K Jul 17 13:16 llama_v2-200steps\ndrwxr-xr-x. 2 root root 25K Jul 17 13:28 llama_v2-300steps\ndrwxr-xr-x. 2 root root 25K Jul 17 13:29 llama_v2-400steps\ndrwxr-xr-x. 2 root root 25K Jul 17 13:30 llama_v2-500steps\ndrwxr-xr-x. 2 root root 25K Jul 17 13:32 llama_v2-600steps\ndrwxr-xr-x. 2 root root 25K Jul 17 13:33 llama_v2-700steps\n"})}),(0,t.jsx)(n.p,{children:"Exit back to your machine:"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"exit\n"})})]}),"\n",(0,t.jsxs)(o.A,{header:(0,t.jsx)(n.h3,{children:(0,t.jsx)(n.span,{children:"CloudWatch Container Insights"})}),children:[(0,t.jsxs)(n.p,{children:["Navigate to ",(0,t.jsx)(n.a,{href:"https://console.aws.amazon.com/cloudwatch/home?#container-insights:?~(query~()~context~(orchestrationService~'eks))",children:"Amazon CloudWatch Container Insights"})," to view GPU utilization and EFA network metrics:"]}),(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.img,{alt:"alt text",src:s(47778).A+"",width:"1177",height:"362"}),"\n",(0,t.jsx)(n.img,{alt:"alt text",src:s(69710).A+"",width:"1175",height:"364"})]})]}),"\n",(0,t.jsxs)(o.A,{header:(0,t.jsx)(n.h3,{children:(0,t.jsx)(n.span,{children:"Clean Up"})}),children:[(0,t.jsx)(n.admonition,{type:"caution",children:(0,t.jsx)(n.p,{children:"To avoid unwanted charges to your AWS account, delete all the AWS resources created during this deployment."})}),(0,t.jsxs)(n.p,{children:["This script will cleanup the environment using ",(0,t.jsx)(n.code,{children:"-target"})," option to ensure all the resources are deleted in correct order."]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"cd ai-on-eks/blueprints/training/slinky-slurm\n"})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"./cleanup.sh\n"})})]})]})}function u(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(h,{...e})}):h(e)}}}]);