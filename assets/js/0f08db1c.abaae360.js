"use strict";(self.webpackChunkdoeks_website=self.webpackChunkdoeks_website||[]).push([[1407],{21844:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>d,frontMatter:()=>s,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"resources/observability","title":"Observability","description":"Observability for AI/ML workloads requires a holistic view of multiple hardware/software components alongside multiple sources of data such as logs, metrics, and traces. Piecing together these components is challenging and time-consuming; therefore, we leverage the AI/ML Observability available in Github to bootstrap this environment.","source":"@site/docs/resources/observability.md","sourceDirName":"resources","slug":"/resources/observability","permalink":"/ai-on-eks/docs/resources/observability","draft":false,"unlisted":false,"editUrl":"https://github.com/awslabs/ai-on-eks/blob/main/website/docs/resources/observability.md","tags":[],"version":"current","frontMatter":{},"sidebar":"resources","previous":{"title":"Dynamic Resource Allocation on EKS","permalink":"/ai-on-eks/docs/resources/dynamic-resource-allocation"}}');var r=t(74848),a=t(28453);const s={},l="Observability",o={},c=[{value:"Architecture",id:"architecture",level:2},{value:"What&#39;s Included",id:"whats-included",level:2},{value:"Why",id:"why",level:2},{value:"How",id:"how",level:2},{value:"Usage",id:"usage",level:2},{value:"Training",id:"training",level:3},{value:"Example",id:"example",level:4},{value:"Inference",id:"inference",level:3},{value:"FluentBit Config",id:"fluentbit-config",level:4},{value:"FluentBit Sidecar",id:"fluentbit-sidecar",level:4},{value:"FluentBit Volume",id:"fluentbit-volume",level:4},{value:"vLLM Metrics",id:"vllm-metrics",level:4},{value:"Example",id:"example-1",level:4}];function h(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",img:"img",li:"li",p:"p",pre:"pre",ul:"ul",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"observability",children:"Observability"})}),"\n",(0,r.jsxs)(n.p,{children:["Observability for AI/ML workloads requires a holistic view of multiple hardware/software components alongside multiple sources of data such as logs, metrics, and traces. Piecing together these components is challenging and time-consuming; therefore, we leverage the ",(0,r.jsx)(n.a,{href:"https://github.com/awslabs/ai-ml-observability-reference-architecture",children:"AI/ML Observability"})," available in Github to bootstrap this environment."]}),"\n",(0,r.jsx)(n.h2,{id:"architecture",children:"Architecture"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{src:"https://github.com/awslabs/ai-ml-observability-reference-architecture/raw/main/static/reference_architecture.png",alt:"Architecture"})}),"\n",(0,r.jsx)(n.h2,{id:"whats-included",children:"What's Included"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Prometheus"}),"\n",(0,r.jsx)(n.li,{children:"OpenSearch"}),"\n",(0,r.jsx)(n.li,{children:"FluentBit"}),"\n",(0,r.jsx)(n.li,{children:"Kube State Metrics"}),"\n",(0,r.jsx)(n.li,{children:"Metrics Server"}),"\n",(0,r.jsx)(n.li,{children:"Alertmanager"}),"\n",(0,r.jsx)(n.li,{children:"Grafana"}),"\n",(0,r.jsx)(n.li,{children:"Pod/Service monitors for AI/ML workloads"}),"\n",(0,r.jsx)(n.li,{children:"AI/ML Dashboards"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"why",children:"Why"}),"\n",(0,r.jsx)(n.p,{children:"Understanding the performance of AI/ML workloads is challenging: Is the GPU getting data fast enough? Is the CPU the bottleneck? Is the storage fast enough? These are questions that are hard to answer in isolation. The more of the picture one is able to see, the more clarity there is in identifying performance bottlenecks."}),"\n",(0,r.jsx)(n.h2,{id:"how",children:"How"}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.a,{href:"https://awslabs.github.io/ai-on-eks/docs/infra/ai-ml/jark",children:"JARK"})," infrastructure already comes with this architecture enabled by default, if you would like to add it to your infrastructure, you need to ensure 2 variables are set to ",(0,r.jsx)(n.code,{children:"true"})," in ",(0,r.jsx)(n.code,{children:"blueprint.tfvars"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"enable_argocd                    = true\nenable_ai_ml_observability_stack = true\n"})}),"\n",(0,r.jsx)(n.p,{children:"The first variable deploys ArgoCD, which is used to deploy the observability architecture, the second variable deploys the architecture."}),"\n",(0,r.jsx)(n.h2,{id:"usage",children:"Usage"}),"\n",(0,r.jsxs)(n.p,{children:["The architecture is entirely deployed into the ",(0,r.jsx)(n.code,{children:"monitoring"})," namespace. To access Grafana: ",(0,r.jsx)(n.code,{children:"kubectl port-forward -n monitoring svc/kube-prometheus-stack-grafana 3000:80"}),". You can then open ",(0,r.jsx)(n.a,{href:"https://localhost:3000",children:"https://localhost:3000"})," to log into grafana with username ",(0,r.jsx)(n.code,{children:"admin"})," and password ",(0,r.jsx)(n.code,{children:"prom-operator"}),". You can refer to the ",(0,r.jsx)(n.a,{href:"https://github.com/awslabs/ai-ml-observability-reference-architecture?tab=readme-ov-file#security",children:"security"})," section in the Readme to see how to change the username/password"]}),"\n",(0,r.jsx)(n.h3,{id:"training",children:"Training"}),"\n",(0,r.jsxs)(n.p,{children:["Ray training job logs and metrics will be automatically collected by the Observability architecture and can be found in the ",(0,r.jsx)(n.a,{href:"http://localhost:3000/d/ee6mbjghme96oc/gpu-training?orgId=1&refresh=5s&var-namespace=default&var-job=ray-train&var-instance=All",children:"training dashboard"}),"."]}),"\n",(0,r.jsx)(n.h4,{id:"example",children:"Example"}),"\n",(0,r.jsxs)(n.p,{children:["A full example of this can be found in the ",(0,r.jsx)(n.a,{href:"https://github.com/awslabs/ai-ml-observability-reference-architecture/tree/main/examples/training",children:"AI/ML observability repo"}),". We will also be updating the Blueprints here to make use of this architecture."]}),"\n",(0,r.jsx)(n.h3,{id:"inference",children:"Inference"}),"\n",(0,r.jsxs)(n.p,{children:["Ray inference metrics should be automatically picked up by the observability infrastructure and can be found in the ",(0,r.jsx)(n.a,{href:"http://localhost:3000/d/bec31e71-3ac5-4133-b2e3-b9f75c8ab56c/inference-dashboard?orgId=1&refresh=5s",children:"inference dashboard"}),". To instrument your inference workloads for logging, you will need to add a few items:"]}),"\n",(0,r.jsx)(n.h4,{id:"fluentbit-config",children:"FluentBit Config"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: fluentbit-config\n  namespace: default\ndata:\n  fluent-bit.conf: |-\n    [INPUT]\n        Name tail\n        Path /tmp/ray/session_latest/logs/*\n        Tag ray\n        Path_Key true\n        Refresh_Interval 5\n    [FILTER]\n        Name modify\n        Match ray\n        Add POD_LABELS ${POD_LABELS}\n    [OUTPUT]\n        Name stdout\n        Format json\n"})}),"\n",(0,r.jsx)(n.p,{children:"Deploy this into the namespace in which you intend to run your inference workload. You only need one in each namespace to tell the FluentBit sidecar how to output the logs."}),"\n",(0,r.jsx)(n.h4,{id:"fluentbit-sidecar",children:"FluentBit Sidecar"}),"\n",(0,r.jsx)(n.p,{children:"We will need to add a sidecar to the Ray inference service so FluentBit can write the logs to STDOUT"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"              - name: fluentbit\n                image: fluent/fluent-bit:3.2.2\n                env:\n                  - name: POD_LABELS\n                    valueFrom:\n                      fieldRef:\n                        fieldPath: metadata.labels['ray.io/cluster']\n                resources:\n                  requests:\n                    cpu: 100m\n                    memory: 128Mi\n                  limits:\n                    cpu: 100m\n                    memory: 128Mi\n                volumeMounts:\n                  - mountPath: /tmp/ray\n                    name: ray-logs\n                  - mountPath: /fluent-bit/etc/fluent-bit.conf\n                    subPath: fluent-bit.conf\n                    name: fluentbit-config\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Add this section to the ",(0,r.jsx)(n.code,{children:"workerGroupSpecs"})," containers"]}),"\n",(0,r.jsx)(n.h4,{id:"fluentbit-volume",children:"FluentBit Volume"}),"\n",(0,r.jsxs)(n.p,{children:["Finally, we need to add the configmap volume to our ",(0,r.jsx)(n.code,{children:"volumes"})," section:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"              - name: fluentbit-config\n                configMap:\n                  name: fluentbit-config\n"})}),"\n",(0,r.jsx)(n.h4,{id:"vllm-metrics",children:"vLLM Metrics"}),"\n",(0,r.jsx)(n.p,{children:"vLLM also outputs useful metrics like the Time to First Token, throughput, latencies, cache utilization and more. To get access to these metrics, we need to add a route to our pod for the metrics path:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Imports\nimport re\nfrom prometheus_client import make_asgi_app\nfrom fastapi import FastAPI\nfrom starlette.routing import Mount\n\napp = FastAPI()\n\nclass Deployment:\n    def _init__(selfself, **kwargs):\n        ...\n        route = Mount(\"/metrics\", make_asgi_app())\n        # Workaround for 307 Redirect for /metrics\n        route.path_regex = re.compile('^/metrics(?P<path>.*)$')\n        app.routes.append(route)\n"})}),"\n",(0,r.jsx)(n.p,{children:"This will allow the deployed monitor to collect the vLLM metrics and display them in the inference dashboard."}),"\n",(0,r.jsx)(n.h4,{id:"example-1",children:"Example"}),"\n",(0,r.jsxs)(n.p,{children:["A full example of this can be found in the ",(0,r.jsx)(n.a,{href:"https://github.com/awslabs/ai-ml-observability-reference-architecture/tree/main/examples/inference",children:"AI/ML observability repo"}),". We will also be updating the Blueprints here to make use of this architecture."]})]})}function d(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(h,{...e})}):h(e)}},28453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>l});var i=t(96540);const r={},a=i.createContext(r);function s(e){const n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),i.createElement(a.Provider,{value:n},e.children)}}}]);