"use strict";(self.webpackChunkdoeks_website=self.webpackChunkdoeks_website||[]).push([[9419],{71985:e=>{e.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"blueprints":[{"type":"link","label":"Overview","href":"/ai-on-eks/docs/blueprints/","docId":"blueprints/index","unlisted":false},{"type":"category","label":"Training on EKS","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"GPUs","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"BioNeMo on EKS","href":"/ai-on-eks/docs/blueprints/training/GPUs/bionemo","docId":"blueprints/training/GPUs/bionemo","unlisted":false},{"type":"link","label":"Slurm on EKS","href":"/ai-on-eks/docs/blueprints/training/GPUs/slinky-slurm","docId":"blueprints/training/GPUs/slinky-slurm","unlisted":false}]},{"type":"category","label":"Neuron","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Llama-2 with RayTrain on Trn1","href":"/ai-on-eks/docs/blueprints/training/Neuron/RayTrain-Llama2","docId":"blueprints/training/Neuron/RayTrain-Llama2","unlisted":false},{"type":"link","label":"Llama-2 with Nemo-Megatron on Trn1","href":"/ai-on-eks/docs/blueprints/training/Neuron/Llama2","docId":"blueprints/training/Neuron/Llama2","unlisted":false},{"type":"link","label":"BERT-Large on Trainium","href":"/ai-on-eks/docs/blueprints/training/Neuron/BERT-Large","docId":"blueprints/training/Neuron/BERT-Large","unlisted":false},{"type":"link","label":"Llama 3 Fine-tuning with LoRA","href":"/ai-on-eks/docs/blueprints/training/Neuron/Llama-LoRA-Finetuning","docId":"blueprints/training/Neuron/Llama-LoRA-Finetuning","unlisted":false}]}],"href":"/ai-on-eks/docs/category/training-on-eks"},{"type":"category","label":"Inference on EKS","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"GPU Inference on EKS","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"RayServe with vLLM","href":"/ai-on-eks/docs/blueprints/inference/GPUs/vLLM-rayserve","docId":"blueprints/inference/GPUs/vLLM-rayserve","unlisted":false},{"type":"link","label":"NVIDIA Triton Server with vLLM","href":"/ai-on-eks/docs/blueprints/inference/GPUs/vLLM-NVIDIATritonServer","docId":"blueprints/inference/GPUs/vLLM-NVIDIATritonServer","unlisted":false},{"type":"link","label":"Stable Diffusion on GPU","href":"/ai-on-eks/docs/blueprints/inference/GPUs/stablediffusion-gpus","docId":"blueprints/inference/GPUs/stablediffusion-gpus","unlisted":false},{"type":"link","label":"NVIDIA NIM LLM on Amazon EKS","href":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-nim-llama3","docId":"blueprints/inference/GPUs/nvidia-nim-llama3","unlisted":false},{"type":"link","label":"NVIDIA NIM Operator on EKS","href":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-nim-operator","docId":"blueprints/inference/GPUs/nvidia-nim-operator","unlisted":false},{"type":"link","label":"DeepSeek-R1 on EKS","href":"/ai-on-eks/docs/blueprints/inference/GPUs/ray-vllm-deepseek","docId":"blueprints/inference/GPUs/ray-vllm-deepseek","unlisted":false},{"type":"link","label":"NVIDIA Dynamo on Amazon EKS","href":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-dynamo","docId":"blueprints/inference/GPUs/nvidia-dynamo","unlisted":false},{"type":"link","label":"AIBrix on EKS","href":"/ai-on-eks/docs/blueprints/inference/GPUs/aibrix-deepseek-distill","docId":"blueprints/inference/GPUs/aibrix-deepseek-distill","unlisted":false}],"href":"/ai-on-eks/docs/category/gpu-inference-on-eks"},{"type":"category","label":"Neuron Inference on EKS","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Llama-3-8B with vLLM on Inferentia2","href":"/ai-on-eks/docs/blueprints/inference/Neuron/vllm-ray-inf2","docId":"blueprints/inference/Neuron/vllm-ray-inf2","unlisted":false},{"type":"link","label":"Mistral-7B on Inferentia2","href":"/ai-on-eks/docs/blueprints/inference/Neuron/Mistral-7b-inf2","docId":"blueprints/inference/Neuron/Mistral-7b-inf2","unlisted":false},{"type":"link","label":"Llama-3-8B on Inferentia2","href":"/ai-on-eks/docs/blueprints/inference/Neuron/llama3-inf2","docId":"blueprints/inference/Neuron/llama3-inf2","unlisted":false},{"type":"link","label":"Llama-2 on Inferentia2","href":"/ai-on-eks/docs/blueprints/inference/Neuron/llama2-inf2","docId":"blueprints/inference/Neuron/llama2-inf2","unlisted":false},{"type":"link","label":"Stable Diffusion on Inferentia2","href":"/ai-on-eks/docs/blueprints/inference/Neuron/stablediffusion-inf2","docId":"blueprints/inference/Neuron/stablediffusion-inf2","unlisted":false},{"type":"link","label":"Ray Serve High Availability","href":"/ai-on-eks/docs/blueprints/inference/Neuron/rayserve-ha","docId":"blueprints/inference/Neuron/rayserve-ha","unlisted":false}],"href":"/ai-on-eks/docs/category/neuron-inference-on-eks"},{"type":"link","label":"Overview","href":"/ai-on-eks/docs/blueprints/inference/","docId":"blueprints/inference/index","unlisted":false},{"type":"link","label":"Inference Charts","href":"/ai-on-eks/docs/blueprints/inference/inference-charts","docId":"blueprints/inference/inference-charts","unlisted":false}],"href":"/ai-on-eks/docs/category/inference-on-eks"}],"infra":[{"type":"category","label":"AI/ML on EKS","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Introduction","href":"/ai-on-eks/docs/infra/ai-ml/","docId":"infra/ai-ml/index","unlisted":false},{"type":"link","label":"JARK on EKS","href":"/ai-on-eks/docs/infra/ai-ml/jark","docId":"infra/ai-ml/jark","unlisted":false},{"type":"link","label":"AIBrix on EKS","href":"/ai-on-eks/docs/infra/ai-ml/aibrix","docId":"infra/ai-ml/aibrix","unlisted":false},{"type":"link","label":"EMR NVIDIA Spark-RAPIDS","href":"/ai-on-eks/docs/infra/ai-ml/emr-spark-rapids","docId":"infra/ai-ml/emr-spark-rapids","unlisted":false},{"type":"link","label":"JupyterHub on EKS","href":"/ai-on-eks/docs/infra/ai-ml/jupyterhub","docId":"infra/ai-ml/jupyterhub","unlisted":false},{"type":"link","label":"Trainium on EKS","href":"/ai-on-eks/docs/infra/ai-ml/trainium","docId":"infra/ai-ml/trainium","unlisted":false}],"href":"/ai-on-eks/docs/category/aiml-on-eks"},{"type":"link","label":"Troubleshooting","href":"/ai-on-eks/docs/infra/troubleshooting/","docId":"infra/troubleshooting/troubleshooting","unlisted":false}],"resources":[{"type":"link","label":"Introduction","href":"/ai-on-eks/docs/resources/intro","docId":"resources/intro","unlisted":false},{"type":"link","label":"EKS Best Practices","href":"/ai-on-eks/docs/resources/eks-best-practices","docId":"resources/eks-best-practices","unlisted":false},{"type":"link","label":"Mounpoint-S3 on EKS","href":"/ai-on-eks/docs/resources/mountpoint-s3","docId":"resources/mountpoint-s3","unlisted":false},{"type":"link","label":"Networking for AI","href":"/ai-on-eks/docs/resources/networking","docId":"resources/networking","unlisted":false},{"type":"link","label":"Bin packing for Amazon EKS","href":"/ai-on-eks/docs/resources/binpacking-custom-scheduler-eks","docId":"resources/binpacking-custom-scheduler-eks","unlisted":false},{"type":"link","label":"Dynamic Resource Allocation on EKS","href":"/ai-on-eks/docs/resources/dynamic-resource-allocation","docId":"resources/dynamic-resource-allocation","unlisted":false},{"type":"link","label":"Observability","href":"/ai-on-eks/docs/resources/observability","docId":"resources/observability","unlisted":false}],"guidance":[{"type":"link","label":"Guidance for AI workloads","href":"/ai-on-eks/docs/guidance/","docId":"guidance/index","unlisted":false},{"type":"category","label":"Solving cold start challenges for AI/ML inference applications on Amazon EKS","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Reducing container image size","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Optimizing container images size","href":"/ai-on-eks/docs/guidance/container-startup-time/reduce-container-image-size/optimize-image-size","docId":"guidance/container-startup-time/reduce-container-image-size/optimize-image-size","unlisted":false},{"type":"link","label":"Decoupling model artifacts from container image","href":"/ai-on-eks/docs/guidance/container-startup-time/reduce-container-image-size/decoupling-model-artifacts","docId":"guidance/container-startup-time/reduce-container-image-size/decoupling-model-artifacts","unlisted":false}],"href":"/ai-on-eks/docs/guidance/container-startup-time/reduce-container-image-size/"},{"type":"category","label":"Accelerating pull process","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Using containerd snapshotter","href":"/ai-on-eks/docs/guidance/container-startup-time/accelerate-pull-process/containerd-snapshotter","docId":"guidance/container-startup-time/accelerate-pull-process/containerd-snapshotter","unlisted":false},{"type":"link","label":"Preload container images into data volumes","href":"/ai-on-eks/docs/guidance/container-startup-time/accelerate-pull-process/prefecthing-images-on-br","docId":"guidance/container-startup-time/accelerate-pull-process/prefecthing-images-on-br","unlisted":false}],"href":"/ai-on-eks/docs/guidance/container-startup-time/accelerate-pull-process/"}],"href":"/ai-on-eks/docs/guidance/container-startup-time/"}]},"docs":{"blueprints/index":{"id":"blueprints/index","title":"AI on EKS","description":"Welcome to AI on Amazon Elastic Kubernetes Service (EKS), your gateway to harnessing the power of Large Language Models (LLMs) for a wide range of applications. This introduction page serves as your starting point to explore our architectural patterns and blueprints for Training, Fine-tuning, and Inference using the latest LLMs.","sidebar":"blueprints"},"blueprints/inference/GPUs/aibrix-deepseek-distill":{"id":"blueprints/inference/GPUs/aibrix-deepseek-distill","title":"AIBrix","description":"AIBrix is an open source initiative designed to provide essential building blocks to construct scalable GenAI inference infrastructure. AIBrix delivers a cloud-native solution optimized for deploying, managing, and scaling large language model (LLM) inference, tailored specifically to enterprise needs.","sidebar":"blueprints"},"blueprints/inference/GPUs/nvidia-dynamo":{"id":"blueprints/inference/GPUs/nvidia-dynamo","title":"NVIDIA Dynamo on Amazon EKS","description":"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn\'t working, it\'s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren\'t initializing, check the logs for Karpenter or Node groups to resolve the issue.","sidebar":"blueprints"},"blueprints/inference/GPUs/nvidia-nim-llama3":{"id":"blueprints/inference/GPUs/nvidia-nim-llama3","title":"NVIDIA NIM LLM on Amazon EKS","description":"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn\'t working, it\u2019s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren\'t initializing, check the logs for Karpenter or Node groups to resolve the issue.","sidebar":"blueprints"},"blueprints/inference/GPUs/nvidia-nim-operator":{"id":"blueprints/inference/GPUs/nvidia-nim-operator","title":"NVIDIA NIM Operator on EKS","description":"What is NVIDIA NIM?","sidebar":"blueprints"},"blueprints/inference/GPUs/ray-vllm-deepseek":{"id":"blueprints/inference/GPUs/ray-vllm-deepseek","title":"DeepSeek-R1 on EKS","description":"In this guide, we\'ll explore deploying DeepSeek-R1-Distill-Llama-8B model inference using Ray with a vLLM backend on Amazon EKS.","sidebar":"blueprints"},"blueprints/inference/GPUs/stablediffusion-gpus":{"id":"blueprints/inference/GPUs/stablediffusion-gpus","title":"Stable Diffusion on GPU","description":"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn\'t working, it\u2019s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren\'t initializing, check the logs for Karpenter or Node groups to resolve the issue.","sidebar":"blueprints"},"blueprints/inference/GPUs/vLLM-NVIDIATritonServer":{"id":"blueprints/inference/GPUs/vLLM-NVIDIATritonServer","title":"NVIDIA Triton Server with vLLM","description":"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn\'t working, it\u2019s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren\'t initializing, check the logs for Karpenter or Node groups to resolve the issue.","sidebar":"blueprints"},"blueprints/inference/GPUs/vLLM-rayserve":{"id":"blueprints/inference/GPUs/vLLM-rayserve","title":"RayServe with vLLM","description":"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn\'t working, it\u2019s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren\'t initializing, check the logs for Karpenter or Node groups to resolve the issue.","sidebar":"blueprints"},"blueprints/inference/index":{"id":"blueprints/inference/index","title":"Inference on EKS","description":"AI on EKS provides comprehensive solutions for deploying AI/ML inference workloads on Amazon EKS, supporting both GPU and AWS Neuron (Inferentia/Trainium) hardware configurations.","sidebar":"blueprints"},"blueprints/inference/inference-charts":{"id":"blueprints/inference/inference-charts","title":"AI on EKS Inference Charts","description":"The AI on EKS Inference Charts provide a streamlined Helm-based approach to deploy AI/ML inference workloads on both GPU","sidebar":"blueprints"},"blueprints/inference/Neuron/llama2-inf2":{"id":"blueprints/inference/Neuron/llama2-inf2","title":"Llama-2 on Inferentia2","description":"Serve Llama-2 models on AWS Inferentia accelerators for efficient inference.","sidebar":"blueprints"},"blueprints/inference/Neuron/llama3-inf2":{"id":"blueprints/inference/Neuron/llama3-inf2","title":"Llama-3-8B on Inferentia2","description":"Serve Llama-3 models on AWS Inferentia accelerators for efficient inference.","sidebar":"blueprints"},"blueprints/inference/Neuron/Mistral-7b-inf2":{"id":"blueprints/inference/Neuron/Mistral-7b-inf2","title":"Mistral-7B on Inferentia2","description":"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn\'t working, it\u2019s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren\'t initializing, check the logs for Karpenter or Node groups to resolve the issue.","sidebar":"blueprints"},"blueprints/inference/Neuron/rayserve-ha":{"id":"blueprints/inference/Neuron/rayserve-ha","title":"Ray Serve High Availability","description":"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn\'t working, it\u2019s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren\'t initializing, check the logs for Karpenter or Node groups to resolve the issue.","sidebar":"blueprints"},"blueprints/inference/Neuron/stablediffusion-inf2":{"id":"blueprints/inference/Neuron/stablediffusion-inf2","title":"Stable Diffusion on Inferentia2","description":"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn\'t working, it\u2019s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren\'t initializing, check the logs for Karpenter or Node groups to resolve the issue.","sidebar":"blueprints"},"blueprints/inference/Neuron/vllm-ray-inf2":{"id":"blueprints/inference/Neuron/vllm-ray-inf2","title":"Llama-3-8B with vLLM on Inferentia2","description":"Serving Meta-Llama-3-8B-Instruct model on AWS Inferentia2 using Ray and vLLM for optimized inference performance.","sidebar":"blueprints"},"blueprints/training/GPUs/bionemo":{"id":"blueprints/training/GPUs/bionemo","title":"BioNeMo on EKS","description":"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn\'t working, it\u2019s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren\'t initializing, check the logs for Karpenter or Node groups to resolve the issue.","sidebar":"blueprints"},"blueprints/training/GPUs/slinky-slurm":{"id":"blueprints/training/GPUs/slinky-slurm","title":"Slurm on EKS","description":"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn\'t working, it\u2019s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren\'t initializing, check the logs for Karpenter or Node groups to resolve the issue.","sidebar":"blueprints"},"blueprints/training/Neuron/BERT-Large":{"id":"blueprints/training/Neuron/BERT-Large","title":"BERT-Large on Trainium","description":"COMING SOON","sidebar":"blueprints"},"blueprints/training/Neuron/Llama-LoRA-Finetuning":{"id":"blueprints/training/Neuron/Llama-LoRA-Finetuning","title":"Llama-LoRA-Finetuning","description":"To deploy this example for fine-tuning a LLM on EKS, you need access to AWS Trainium ec2 instance. If deployment fails, check if you have access to this instance type. If nodes aren\'t starting, check Karpenter or Node group logs.","sidebar":"blueprints"},"blueprints/training/Neuron/Llama2":{"id":"blueprints/training/Neuron/Llama2","title":"Llama-2 with Nemo-Megatron on Trn1","description":"Training a Llama-2 Model using Trainium, Neuronx-Nemo-Megatron and MPI operator","sidebar":"blueprints"},"blueprints/training/Neuron/RayTrain-Llama2":{"id":"blueprints/training/Neuron/RayTrain-Llama2","title":"RayTrain-Llama2","description":"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn\'t working, it\u2019s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren\'t initializing, check the logs for Karpenter or Node groups to resolve the issue.","sidebar":"blueprints"},"guidance/container-startup-time/accelerate-pull-process/containerd-snapshotter":{"id":"guidance/container-startup-time/accelerate-pull-process/containerd-snapshotter","title":"Using containerd snapshotter","description":"Using SOCI snapshotter","sidebar":"guidance"},"guidance/container-startup-time/accelerate-pull-process/index":{"id":"guidance/container-startup-time/accelerate-pull-process/index","title":"Accelerating pull process","description":"The solutions in this section reduce the container startup time by improving the image pull process. They do it by either relying on the images\u2019 layered internal structure and changing how their layers are retrieved over the network from a container registry or by skipping the registry altogether.","sidebar":"guidance"},"guidance/container-startup-time/accelerate-pull-process/prefecthing-images-on-br":{"id":"guidance/container-startup-time/accelerate-pull-process/prefecthing-images-on-br","title":"Preload container images into Bottlerocket data volumes with EBS Snapshots","description":"The purpose of this pattern is to reduce the cold start time of containers with large images by caching the images in the data volume of Bottlerocket OS.","sidebar":"guidance"},"guidance/container-startup-time/index":{"id":"guidance/container-startup-time/index","title":"Solving cold start challenges for AI/ML inference applications on Amazon EKS","description":"This guide will get regular updates based on observed patterns. New content might be added, and existing might be changed.","sidebar":"guidance"},"guidance/container-startup-time/reduce-container-image-size/decoupling-model-artifacts":{"id":"guidance/container-startup-time/reduce-container-image-size/decoupling-model-artifacts","title":"Decoupling model artifacts from container image","description":"While the main purpose of extracting model artifacts is to reduce the container image size, this approach also provides additional operational and functional advantages. The operational improvements include separate versioning, streamlined auditing, and lifecycle control for model artifacts, while lazy-loading, hot-swapping, and reuse across different applications offer additional functional flexibility and performance. Depending on the use case these advantages can be decisive factors when selecting among the solutions outlined in this section.","sidebar":"guidance"},"guidance/container-startup-time/reduce-container-image-size/index":{"id":"guidance/container-startup-time/reduce-container-image-size/index","title":"Reducing container image size","description":"Image compressed size \u2013 the size it occupies in the container image registry, directly correlates with pull time, while image uncompressed size \u2013 the size of the image once downloaded, impacts the time it takes to bootstrap the container \u2013 the larger the size the longer it takes to decompress and extract the image layers, mount them, and combine them into the container file system.","sidebar":"guidance"},"guidance/container-startup-time/reduce-container-image-size/optimize-image-size":{"id":"guidance/container-startup-time/reduce-container-image-size/optimize-image-size","title":"Optimizing container images size","description":"Selecting appropriate base images","sidebar":"guidance"},"guidance/index":{"id":"guidance/index","title":"Guidance for AI workloads","description":"This section is a collection of guidance on different topics around building and deploying AI workloads on Amazon EKS.","sidebar":"guidance"},"infra/ai-ml/aibrix":{"id":"infra/ai-ml/aibrix","title":"AIBrix on EKS","description":"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn\'t working, it\u2019s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren\'t initializing, check the logs for Karpenter or Node groups to resolve the issue.","sidebar":"infra"},"infra/ai-ml/emr-spark-rapids":{"id":"infra/ai-ml/emr-spark-rapids","title":"EMR on EKS NVIDIA RAPIDS Accelerator for Apache Spark","description":"The NVIDIA RAPIDS Accelerator for Apache Spark is a powerful tool that builds on the capabilities of NVIDIA CUDA\xae - a transformative parallel computing platform designed for enhancing computational processes on NVIDIA\'s GPU architecture. RAPIDS, a project developed by NVIDIA, comprises a suite of open-source libraries that are hinged upon CUDA, thereby enabling GPU-accelerated data science workflows.","sidebar":"infra"},"infra/ai-ml/index":{"id":"infra/ai-ml/index","title":"Introduction","description":"The AIoEKS foundational infrastructure lives in the infra/base directory. This directory contains the base","sidebar":"infra"},"infra/ai-ml/jark":{"id":"infra/ai-ml/jark","title":"JARK on EKS","description":"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn\'t working, it\u2019s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren\'t initializing, check the logs for Karpenter or Node groups to resolve the issue.","sidebar":"infra"},"infra/ai-ml/jupyterhub":{"id":"infra/ai-ml/jupyterhub","title":"jupyterhub","description":"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn\'t working, it\u2019s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren\'t initializing, check the logs for Karpenter or Node groups to resolve the issue.","sidebar":"infra"},"infra/ai-ml/trainium":{"id":"infra/ai-ml/trainium","title":"trainium","description":"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn\'t working, it\u2019s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren\'t initializing, check the logs for Karpenter or Node groups to resolve the issue.","sidebar":"infra"},"infra/troubleshooting/troubleshooting":{"id":"infra/troubleshooting/troubleshooting","title":"Troubleshooting","description":"You will find troubleshooting info for AI on Amazon EKS(AIoEKS) installation issues","sidebar":"infra"},"intro":{"id":"intro","title":"intro","description":""},"resources/binpacking-custom-scheduler-eks":{"id":"resources/binpacking-custom-scheduler-eks","title":"Bin packing for Amazon EKS","description":"Introduction","sidebar":"resources"},"resources/dynamic-resource-allocation":{"id":"resources/dynamic-resource-allocation","title":"Dynamic Resource Allocation for GPUs on Amazon EKS","description":"\ud83d\ude80 TL;DR \u2013 Dynamic GPU Scheduling with DRA on EKS","sidebar":"resources"},"resources/eks-best-practices":{"id":"resources/eks-best-practices","title":"EKS Best Practices","description":"Amazon EKS Best Practices guide for AI/ML applications is located and manged at the AWS documentation site for Amazon EKS. Please use this resource for any best practice recommendations.","sidebar":"resources"},"resources/intro":{"id":"resources/intro","title":"Introduction to Resources","description":"Welcome to the Resources section. This area is dedicated to providing you with a wealth of information, insights, and learning materials to enhance your understanding and skills in Data and AI workloads running on EKS.","sidebar":"resources"},"resources/mountpoint-s3":{"id":"resources/mountpoint-s3","title":"Mounpoint S3: Enhancing Amazon S3 File Access for Data & AI Workloads on Amazon EKS","description":"What is Mountpoint-S3?","sidebar":"resources"},"resources/networking":{"id":"resources/networking","title":"Networking for AI","description":"VPC and IP Considerations","sidebar":"resources"},"resources/observability":{"id":"resources/observability","title":"Observability","description":"Observability for AI/ML workloads requires a holistic view of multiple hardware/software components alongside multiple sources of data such as logs, metrics, and traces. Piecing together these components is challenging and time-consuming; therefore, we leverage the AI/ML Observability available in Github to bootstrap this environment.","sidebar":"resources"}}}}')}}]);