"use strict";(self.webpackChunkdoeks_website=self.webpackChunkdoeks_website||[]).push([[6407],{26633:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/nim-dashboard-2-7fa24a28624902d2a665f515ab2b4fd1.png"},28453:(e,n,s)=>{s.d(n,{R:()=>l,x:()=>a});var i=s(96540);const r={},t=i.createContext(r);function l(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),i.createElement(t.Provider,{value:n},e.children)}},30717:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/nim-ngc-api-key-8f10f8d047f3721dbeddd112c6b69a81.png"},39233:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/NIMOperatoronEKS-25e8b19c29b3881fa3f9b58036135179.png"},42450:(e,n,s)=>{s.d(n,{A:()=>u});var i=s(96540),r=s(5556),t=s.n(r),l=s(34164);const a="collapsibleContent_q3kw",o="header_QCEw",c="icon_PckA",d="content_qLC1",h="expanded_iGsi";var p=s(74848);function m({children:e,header:n}){const[s,r]=(0,i.useState)(!1);return(0,p.jsxs)("div",{className:a,children:[(0,p.jsxs)("div",{className:(0,l.A)(o,{[h]:s}),onClick:()=>{r(!s)},children:[n,(0,p.jsx)("span",{className:(0,l.A)(c,{[h]:s}),children:s?"\ud83d\udc47":"\ud83d\udc48"})]}),s&&(0,p.jsx)("div",{className:d,children:e})]})}m.propTypes={children:t().node.isRequired,header:t().node.isRequired};const u=m},50334:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/NIMOperatorArchitecture-547ebef43bd769516d44bc9f1bf45b18.png"},59100:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/openweb-ui-nim-2-e9a836cbafc53c50b8437d766b3adbd4.png"},77944:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>o,contentTitle:()=>a,default:()=>h,frontMatter:()=>l,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"blueprints/inference/GPUs/nvidia-nim-operator","title":"NVIDIA NIM Operator on EKS","description":"What is NVIDIA NIM?","source":"@site/docs/blueprints/inference/GPUs/nvidia-nim-operator.md","sourceDirName":"blueprints/inference/GPUs","slug":"/blueprints/inference/GPUs/nvidia-nim-operator","permalink":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-nim-operator","draft":false,"unlisted":false,"editUrl":"https://github.com/awslabs/ai-on-eks/blob/main/website/docs/blueprints/inference/GPUs/nvidia-nim-operator.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"NVIDIA NIM Operator on EKS","sidebar_position":4},"sidebar":"blueprints","previous":{"title":"NVIDIA NIM LLM on Amazon EKS","permalink":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-nim-llama3"},"next":{"title":"DeepSeek-R1 on EKS","permalink":"/ai-on-eks/docs/blueprints/inference/GPUs/ray-vllm-deepseek"}}');var r=s(74848),t=s(28453);s(42450);const l={title:"NVIDIA NIM Operator on EKS",sidebar_position:4},a="NVIDIA NIM Operator on Amazon EKS",o={},c=[{value:"What is NVIDIA NIM?",id:"what-is-nvidia-nim",level:2},{value:"NVIDIA NIM Operator for Kubernetes",id:"nvidia-nim-operator-for-kubernetes",level:2},{value:"NIMCache \u2013 Model Caching for Faster Load Times",id:"nimcache--model-caching-for-faster-load-times",level:3},{value:"NIMService \u2013 Deploying and Managing the Model Server",id:"nimservice--deploying-and-managing-the-model-server",level:3},{value:"NIMPipeline",id:"nimpipeline",level:3},{value:"Overview of this deployment pattern on Amazon EKS",id:"overview-of-this-deployment-pattern-on-amazon-eks",level:2},{value:"Deploying the Solution",id:"deploying-the-solution",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Deploy",id:"deploy",level:3},{value:"Deploy llama-3.1-8b-instruct with NIM Operator",id:"deploy-llama-31-8b-instruct-with-nim-operator",level:3},{value:"Step 1: Create Secrets for Authentication",id:"step-1-create-secrets-for-authentication",level:4},{value:"Step 2: Cache the Model to EFS using NIMCache CRD",id:"step-2-cache-the-model-to-efs-using-nimcache-crd",level:4},{value:"Step 3: Deploy the Model using NIMService CRD",id:"step-3-deploy-the-model-using-nimservice-crd",level:4},{value:"\ud83d\ude80 Model Startup Timeline",id:"-model-startup-timeline",level:3},{value:"Test the Model with a Prompt",id:"test-the-model-with-a-prompt",level:3},{value:"Step 1: Port Forward the Model Service",id:"step-1-port-forward-the-model-service",level:4},{value:"Step 2: Send a Sample Prompt Using curl",id:"step-2-send-a-sample-prompt-using-curl",level:4},{value:"Open WebUI Deployment",id:"open-webui-deployment",level:2},{value:"Performance Testing with NVIDIA GenAI-Perf Tool",id:"performance-testing-with-nvidia-genai-perf-tool",level:2},{value:"Grafana Dashboard",id:"grafana-dashboard",level:3},{value:"Conclusion",id:"conclusion",level:2},{value:"Key Benefits:",id:"key-benefits",level:3},{value:"Cleanup",id:"cleanup",level:2},{value:"Step 1: Delete Model Resources",id:"step-1-delete-model-resources",level:3},{value:"Step 2: Destroy AWS Infrastructure",id:"step-2-destroy-aws-infrastructure",level:3}];function d(e){const n={a:"a",admonition:"admonition",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components},{Details:i}=n;return i||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"nvidia-nim-operator-on-amazon-eks",children:"NVIDIA NIM Operator on Amazon EKS"})}),"\n",(0,r.jsx)(n.h2,{id:"what-is-nvidia-nim",children:(0,r.jsx)(n.a,{href:"https://docs.nvidia.com/nim/large-language-models/latest/introduction.html",children:"What is NVIDIA NIM?"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"NVIDIA NIM"})," (",(0,r.jsx)(n.a,{href:"https://docs.nvidia.com/nim/large-language-models/latest/introduction.html",children:"NVIDIA Inference Microservices"}),") is a set of containerized microservices that make it easier to deploy and host large language models (LLMs) and other AI models in your own environment. NIM provides standard APIs (similar to OpenAI or other AI services) for developers to build applications like chatbots and AI assistants, while leveraging NVIDIA\u2019s GPU acceleration for high-performance inference. In essence, NIM abstracts away the complexities of model runtime and optimization, offering a fast path to inference with optimized backends (e.g., TensorRT-LLM, FasterTransformer, etc.) under the hood."]}),"\n",(0,r.jsx)(n.h2,{id:"nvidia-nim-operator-for-kubernetes",children:(0,r.jsx)(n.a,{href:"https://docs.nvidia.com/nim-operator/latest/index.html#",children:"NVIDIA NIM Operator for Kubernetes"})}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.strong,{children:"NVIDIA NIM Operator"})," is a Kubernetes operator that automates the deployment, scaling, and management of NVIDIA NIM microservices on a Kubernetes cluster."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"NVIDIA NIM Operator Architecture",src:s(50334).A+"",width:"1920",height:"1080"})}),"\n",(0,r.jsxs)(n.p,{children:["Instead of manually pulling containers, provisioning GPU nodes, or writing YAML for every model, the NIM Operator introduces three primary ",(0,r.jsx)(n.a,{href:"https://docs.nvidia.com/nim-operator/latest/crds.html",children:"Custom Resource Definitions (CRDs)"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://docs.nvidia.com/nim-operator/latest/cache.html",children:(0,r.jsx)(n.code,{children:"NIMCache"})})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://docs.nvidia.com/nim-operator/latest/service.html",children:(0,r.jsx)(n.code,{children:"NIMService"})})}),"\n",(0,r.jsxs)(n.li,{children:["[",(0,r.jsx)(n.code,{children:"NIMPipeline"}),"] (",(0,r.jsx)(n.a,{href:"https://docs.nvidia.com/nim-operator/latest/pipelines.html",children:"https://docs.nvidia.com/nim-operator/latest/pipelines.html"}),")"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"These CRDs allow you to declaratively define model deployments using native Kubernetes syntax."}),"\n",(0,r.jsx)(n.p,{children:"The Operator handles:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Pulling the model image from NVIDIA GPU Cloud (NGC)"}),"\n",(0,r.jsx)(n.li,{children:"Caching model weights and optimized runtime profiles"}),"\n",(0,r.jsx)(n.li,{children:"Launching model-serving pods with GPU allocation"}),"\n",(0,r.jsx)(n.li,{children:"Exposing inference endpoints via Kubernetes Services"}),"\n",(0,r.jsx)(n.li,{children:"Integrating with autoscaling (e.g., HPA + Karpenter)"}),"\n",(0,r.jsx)(n.li,{children:"Chaining multiple models together into inference pipelines using NIMPipeline"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"nimcache--model-caching-for-faster-load-times",children:(0,r.jsx)(n.a,{href:"https://docs.nvidia.com/nim-operator/latest/cache.html",children:"NIMCache \u2013 Model Caching for Faster Load Times"})}),"\n",(0,r.jsxs)(n.p,{children:["A ",(0,r.jsx)(n.code,{children:"NIMCache"})," (",(0,r.jsx)(n.code,{children:"nimcaches.apps.nvidia.com"}),") is a custom resource that pre-downloads and stores a model\u2019s weights, tokenizer, and runtime-optimized engine files (such as TensorRT-LLM profiles) into a shared persistent volume."]}),"\n",(0,r.jsx)(n.p,{children:"This ensures:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Faster cold start times"}),": no repeated downloads from NGC"]}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Storage reuse across nodes and replicas"})}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Centralized, shared model store"})," (typically on EFS or FSx for Lustre in EKS)"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Model profiles are optimized for specific GPUs (e.g., A10G, L4) and precisions (e.g., FP16). When NIMCache is created, the Operator discovers available model profiles and selects the best one for your cluster."}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:["\ud83d\udccc Tip: Using ",(0,r.jsx)(n.code,{children:"NIMCache"})," is highly recommended for production, especially when running multiple replicas or restarting models frequently."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"nimservice--deploying-and-managing-the-model-server",children:(0,r.jsx)(n.a,{href:"https://docs.nvidia.com/nim-operator/latest/service.html",children:"NIMService \u2013 Deploying and Managing the Model Server"})}),"\n",(0,r.jsxs)(n.p,{children:["A ",(0,r.jsx)(n.code,{children:"NIMService"})," (",(0,r.jsx)(n.code,{children:"nimservices.apps.nvidia.com"}),") represents a running instance of a NIM model server on your cluster. It specifies the container image, GPU resources, number of replicas, and optionally the name of a ",(0,r.jsx)(n.code,{children:"NIMCache"}),"."]}),"\n",(0,r.jsx)(n.p,{children:"Key benefits:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Declarative model deployment"})," using Kubernetes YAML"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Automatic node scheduling"})," for GPU nodes"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Shared cache support"})," using ",(0,r.jsx)(n.code,{children:"NIMCache"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Autoscaling"})," via HPA or external triggers"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ClusterIP or Ingress support"})," to expose APIs"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"For example, deploying the Meta Llama 3.1 8B Instruct model involves creating:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["A ",(0,r.jsx)(n.code,{children:"NIMCache"})," to store the model (optional but recommended)"]}),"\n",(0,r.jsxs)(n.li,{children:["A ",(0,r.jsx)(n.code,{children:"NIMService"})," pointing to the cached model and allocating GPUs"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["If ",(0,r.jsx)(n.code,{children:"NIMCache"})," is not used, the model will be downloaded each time a pod starts, which may increase startup latency."]}),"\n",(0,r.jsx)(n.h3,{id:"nimpipeline",children:(0,r.jsx)(n.a,{href:"https://docs.nvidia.com/nim-operator/latest/pipeline.html",children:"NIMPipeline"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"NIMPipeline"})," is another CRD that can group multiple ",(0,r.jsx)(n.code,{children:"NIMService"})," resources into an ordered inference pipeline. This is useful for multi-model workflows like:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Retrieval-Augmented Generation (RAG)"}),"\n",(0,r.jsx)(n.li,{children:"Embeddings + LLM chaining"}),"\n",(0,r.jsx)(n.li,{children:"Preprocessing + classification pipelines"}),"\n"]}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:["In this tutorial, we focus on a single model deployment using ",(0,r.jsx)(n.code,{children:"NIMCache"})," and ",(0,r.jsx)(n.code,{children:"NIMService"}),"."]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"overview-of-this-deployment-pattern-on-amazon-eks",children:"Overview of this deployment pattern on Amazon EKS"}),"\n",(0,r.jsxs)(n.p,{children:["This deployment blueprint demonstrates how to run the ",(0,r.jsx)(n.strong,{children:"Meta Llama 3.1 8B Instruct"})," model on ",(0,r.jsx)(n.strong,{children:"Amazon EKS"})," using the ",(0,r.jsx)(n.strong,{children:"NVIDIA NIM Operator"})," with multi-GPU support and optimized model caching for fast startup times."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"NVIDIA NIM Operator Architecture",src:s(39233).A+"",width:"1920",height:"1080"})}),"\n",(0,r.jsx)(n.p,{children:"The model is served using:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"G5 instances (g5.12xlarge)"}),": These instances come with ",(0,r.jsx)(n.strong,{children:"4 NVIDIA A10G GPUs"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Tensor Parallelism (TP)"}),": Set to ",(0,r.jsx)(n.code,{children:"2"}),", meaning the model will run in parallel across ",(0,r.jsx)(n.strong,{children:"2 GPUs"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Persistent Shared Cache"}),": Backed by Amazon ",(0,r.jsx)(n.strong,{children:"EFS"})," to speed up model startup by reusing previously generated engine files"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"By combining these components, the model is deployed as a scalable Kubernetes workload that supports:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Efficient GPU scheduling with ",(0,r.jsx)(n.a,{href:"https://karpenter.sh/",children:"Karpenter"})]}),"\n",(0,r.jsxs)(n.li,{children:["Fast model load using the ",(0,r.jsx)(n.a,{href:"https://docs.nvidia.com/nim-operator/latest/cache.html",children:(0,r.jsx)(n.code,{children:"NIMCache"})})]}),"\n",(0,r.jsxs)(n.li,{children:["Scalable serving endpoint via ",(0,r.jsx)(n.a,{href:"https://docs.nvidia.com/nim-operator/latest/service.html",children:(0,r.jsx)(n.code,{children:"NIMService"})})]}),"\n"]}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:["\ud83d\udccc Note: You can modify the ",(0,r.jsx)(n.code,{children:"tensorParallelism"})," setting or select a different instance type (e.g., G6 with L4 GPUs) based on your performance and cost requirements."]}),"\n"]}),"\n",(0,r.jsxs)(n.admonition,{type:"warning",children:[(0,r.jsxs)(n.p,{children:["Note: Before implementing NVIDIA NIM, please be aware it is part of ",(0,r.jsx)(n.a,{href:"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/",children:"NVIDIA AI Enterprise"}),", which may introduce potential cost and licensing for production use."]}),(0,r.jsxs)(n.p,{children:["For evaluation, NVIDIA also offers a free evaluation license to try NVIDIA AI Enterprise for 90 days, and you can ",(0,r.jsx)(n.a,{href:"https://enterpriseproductregistration.nvidia.com/?LicType=EVAL&ProductFamily=NVAIEnterprise",children:"register"})," it with your corporate email."]})]}),"\n",(0,r.jsx)(n.h2,{id:"deploying-the-solution",children:"Deploying the Solution"}),"\n",(0,r.jsxs)(n.p,{children:["In this tutorial, the entire AWS infrastructure is provisioned using ",(0,r.jsx)(n.strong,{children:"Terraform"}),", including:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Amazon VPC with public and private subnets"}),"\n",(0,r.jsx)(n.li,{children:"Amazon EKS cluster"}),"\n",(0,r.jsxs)(n.li,{children:["GPU nodepools using ",(0,r.jsx)(n.strong,{children:"Karpenter"})]}),"\n",(0,r.jsxs)(n.li,{children:["Addons such as:","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"NVIDIA device plugin"}),"\n",(0,r.jsx)(n.li,{children:"EFS CSI driver"}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"NVIDIA NIM Operator"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["As a demonstration, the ",(0,r.jsx)(n.strong,{children:"Meta Llama-3.1 8B Instruct"})," model will be deployed using a ",(0,r.jsx)(n.code,{children:"NIMService"}),", optionally backed by a ",(0,r.jsx)(n.code,{children:"NIMCache"})," for improved cold start performance."]}),"\n",(0,r.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsx)(n.p,{children:"Before getting started with NVIDIA NIM, ensure you have the following:"}),"\n",(0,r.jsxs)(i,{children:[(0,r.jsx)("summary",{children:"Click to expand the NVIDIA NIM account setup details"}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"NVIDIA AI Enterprise Account"})}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Register for an NVIDIA AI Enterprise account. If you don't have one, you can sign up for a trial account using this ",(0,r.jsx)(n.a,{href:"https://enterpriseproductregistration.nvidia.com/?LicType=EVAL&ProductFamily=NVAIEnterprise",children:"link"}),"."]}),"\n"]}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"NGC API Key"})}),(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Log in to your NVIDIA AI Enterprise account"}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Navigate to the NGC (NVIDIA GPU Cloud) ",(0,r.jsx)(n.a,{href:"https://org.ngc.nvidia.com/",children:"portal"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Generate a personal API key:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Go to your account settings or navigate directly to: ",(0,r.jsx)(n.a,{href:"https://org.ngc.nvidia.com/setup/personal-keys",children:"https://org.ngc.nvidia.com/setup/personal-keys"})]}),"\n",(0,r.jsx)(n.li,{children:'Click on "Generate Personal Key"'}),"\n",(0,r.jsx)(n.li,{children:'Ensure that at least "NGC Catalog" is selected from the "Services Included" dropdown'}),"\n",(0,r.jsxs)(n.li,{children:["Copy and securely store your API key, the key should have a prefix with ",(0,r.jsx)(n.code,{children:"nvapi-"})]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"NGC API KEY",src:s(30717).A+"",width:"2822",height:"1524"})}),"\n"]}),"\n"]}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Validate NGC API Key and Test Image Pull"})}),(0,r.jsx)(n.p,{children:"To ensure your API key is valid and working correctly:"}),(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Set up your NGC API key as an environment variable:"}),"\n"]}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"export NGC_API_KEY=<your_api_key_here>\n"})}),(0,r.jsxs)(n.ol,{start:"2",children:["\n",(0,r.jsx)(n.li,{children:"Authenticate Docker with the NVIDIA Container Registry:"}),"\n"]}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"echo \"$NGC_API_KEY\" | docker login nvcr.io --username '$oauthtoken' --password-stdin\n"})}),(0,r.jsxs)(n.ol,{start:"3",children:["\n",(0,r.jsx)(n.li,{children:"Test pulling an image from NGC:"}),"\n"]}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"docker pull nvcr.io/nim/meta/llama3-8b-instruct:latest\n"})}),(0,r.jsx)(n.p,{children:"You do not have to wait for it to complete, just to make sure the API key is valid to pull the image."})]}),"\n",(0,r.jsx)(n.p,{children:"The following are required to run this tutorial"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"An active AWS account with admin equivalent permissions"}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html",children:"aws cli"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://Kubernetes.io/docs/tasks/tools/",children:"kubectl"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://developer.hashicorp.com/terraform/tutorials/aws-get-started/install-cli",children:"Terraform"})}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"deploy",children:"Deploy"}),"\n",(0,r.jsxs)(n.p,{children:["Clone the ",(0,r.jsx)(n.a,{href:"https://github.com/awslabs/ai-on-eks",children:"ai-on-eks"})," repository that contains the Terraform code for this deployment pattern:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"git clone https://github.com/awslabs/ai-on-eks.git\n"})}),"\n",(0,r.jsx)(n.p,{children:"Navigate to the NVIDIA NIM deployment directory and run the install script to deploy the infrastructure:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"cd ai-on-eks/infra/nvidia-nim\n./install.sh\n"})}),"\n",(0,r.jsxs)(n.p,{children:["This deployment will take approximately ",(0,r.jsx)(n.code,{children:"~20 minute"})," to complete."]}),"\n",(0,r.jsxs)(n.p,{children:["Once the installation finishes, you may find the ",(0,r.jsx)(n.code,{children:"configure_kubectl"})," command from the output. Run the following to configure EKS cluster access"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Creates k8s config file to authenticate with EKS\naws eks --region us-west-2 update-kubeconfig --name nvidia-nim-eks\n"})}),"\n",(0,r.jsxs)(i,{children:[(0,r.jsx)("summary",{children:"Verify the deployments - Click to expand the deployment details"}),(0,r.jsx)(n.p,{children:"$ kubectl get all -n nim-operator"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"kubectl get all -n nim-operator\nNAME                                                 READY   STATUS    RESTARTS   AGE\npod/nim-operator-k8s-nim-operator-6fdffdf97f-56fxc   1/1     Running   0          26h\n\nNAME                                       TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE\nservice/k8s-nim-operator-metrics-service   ClusterIP   172.20.148.6   <none>        8080/TCP   26h\n\nNAME                                            READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/nim-operator-k8s-nim-operator   1/1     1            1           26h\n\nNAME                                                       DESIRED   CURRENT   READY   AGE\nreplicaset.apps/nim-operator-k8s-nim-operator-6fdffdf97f   1         1         1       26h\n"})}),(0,r.jsx)(n.p,{children:"$ kubectl get crds | grep nim"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"nimcaches.apps.nvidia.com                    2025-03-27T17:39:00Z\nnimpipelines.apps.nvidia.com                 2025-03-27T17:39:00Z\nnimservices.apps.nvidia.com                  2025-03-27T17:39:01Z\n"})}),(0,r.jsx)(n.p,{children:"$ kubectl get crds | grep nemo"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"nemocustomizers.apps.nvidia.com              2025-03-27T17:38:59Z\nnemodatastores.apps.nvidia.com               2025-03-27T17:38:59Z\nnemoentitystores.apps.nvidia.com             2025-03-27T17:38:59Z\nnemoevaluators.apps.nvidia.com               2025-03-27T17:39:00Z\nnemoguardrails.apps.nvidia.com               2025-03-27T17:39:00Z\n"})}),(0,r.jsx)(n.p,{children:"To list Karpenter autoscaling Nodepools"}),(0,r.jsx)(n.p,{children:"$ kubectl get nodepools"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"NAME                NODECLASS           NODES   READY   AGE\ng5-gpu-karpenter    g5-gpu-karpenter    1       True    47h\ng6-gpu-karpenter    g6-gpu-karpenter    0       True    7h56m\ninferentia-inf2     inferentia-inf2     0       False   47h\ntrainium-trn1       trainium-trn1       0       False   47h\nx86-cpu-karpenter   x86-cpu-karpenter   0       True    47h\n"})})]}),"\n",(0,r.jsx)(n.h3,{id:"deploy-llama-31-8b-instruct-with-nim-operator",children:"Deploy llama-3.1-8b-instruct with NIM Operator"}),"\n",(0,r.jsx)(n.h4,{id:"step-1-create-secrets-for-authentication",children:"Step 1: Create Secrets for Authentication"}),"\n",(0,r.jsxs)(n.p,{children:["To access the NVIDIA container registry and model artifacts, you'll need to provide your NGC API key. This script creates two Kubernetes secrets: ",(0,r.jsx)(n.code,{children:"ngc-secret"})," for Docker image pulls and ",(0,r.jsx)(n.code,{children:"ngc-api-secret"})," for model authorization."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'cd blueprints/inference/gpu/nvidia-nim-operator-llama3-8b\n\nNGC_API_KEY="your-real-ngc-key" ./deploy-nim-auth.sh\n'})}),"\n",(0,r.jsx)(n.h4,{id:"step-2-cache-the-model-to-efs-using-nimcache-crd",children:"Step 2: Cache the Model to EFS using NIMCache CRD"}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.code,{children:"NIMCache"})," custom resource will pull the model and cache optimized engine profiles to EFS. This dramatically reduces startup time when launching the model later via ",(0,r.jsx)(n.code,{children:"NIMService"}),"."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"cd blueprints/inference/gpu/nvidia-nim-operator-llama3-8b\n\nkubectl apply -f nim-cache-llama3-8b-instruct.yaml\n"})}),"\n",(0,r.jsx)(n.p,{children:"Check status:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"kubectl get nimcaches.apps.nvidia.com -n nim-service\n"})}),"\n",(0,r.jsx)(n.p,{children:"Expected output:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"NAME                      STATUS   PVC                           AGE\nmeta-llama3-8b-instruct   Ready    meta-llama3-8b-instruct-pvc   21h\n"})}),"\n",(0,r.jsx)(n.p,{children:"Display cached model profiles:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'kubectl get nimcaches.apps.nvidia.com -n nim-service \\\n  meta-llama3-8b-instruct -o=jsonpath="{.status.profiles}" | jq .\n'})}),"\n",(0,r.jsx)(n.p,{children:"Sample output:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-json",children:'[\n  {\n    "config": {\n      "feat_lora": "false",\n      "gpu": "A10G",\n      "llm_engine": "tensorrt_llm",\n      "precision": "fp16",\n      "profile": "throughput",\n      "tp": "2"\n    }\n  }\n]\n'})}),"\n",(0,r.jsx)(n.h4,{id:"step-3-deploy-the-model-using-nimservice-crd",children:"Step 3: Deploy the Model using NIMService CRD"}),"\n",(0,r.jsx)(n.p,{children:"Now launch the model service using the cached engine profiles."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"cd blueprints/inference/gpu/nvidia-nim-operator-llama3-8b\n\nkubectl apply -f nim-service-llama3-8b-instruct.yaml\n"})}),"\n",(0,r.jsx)(n.p,{children:"Check the deployed resources:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"kubectl get all -n nim-service\n"})}),"\n",(0,r.jsx)(n.p,{children:"Exepcted Output:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"NAME                                           READY   STATUS    RESTARTS   AGE\npod/meta-llama3-8b-instruct-6cdf47d6f6-hlbnf   1/1     Running   0          6h35m\n\nNAME                              TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)    AGE\nservice/meta-llama3-8b-instruct   ClusterIP   172.20.85.8   <none>        8000/TCP   6h35m\n\nNAME                                      READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/meta-llama3-8b-instruct   1/1     1            1           6h35m\n\nNAME                                                 DESIRED   CURRENT   READY   AGE\nreplicaset.apps/meta-llama3-8b-instruct-6cdf47d6f6   1         1         1       6h35m\n"})}),"\n",(0,r.jsx)(n.h3,{id:"-model-startup-timeline",children:"\ud83d\ude80 Model Startup Timeline"}),"\n",(0,r.jsxs)(n.p,{children:["The following sample is captured from the ",(0,r.jsx)(n.code,{children:"pod/meta-llama3-8b-instruct-6cdf47d6f6-hlbnf"})," log"]}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Step"}),(0,r.jsx)(n.th,{children:"Timestamp"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Start"}),(0,r.jsx)(n.td,{children:"~20:00:50"}),(0,r.jsx)(n.td,{children:"Pod starts, NIM container logs begin"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Profile Match"}),(0,r.jsx)(n.td,{children:"20:00:50.100"}),(0,r.jsx)(n.td,{children:"Detects and selects cached profile (tp=2)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Workspace Ready"}),(0,r.jsx)(n.td,{children:"20:00:50.132"}),(0,r.jsx)(n.td,{children:"Model workspace initialized via EFS in 0.126s"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"TensorRT Init"}),(0,r.jsx)(n.td,{children:"20:00:51.168"}),(0,r.jsx)(n.td,{children:"TensorRT-LLM engine begins setup"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Engine Ready"}),(0,r.jsx)(n.td,{children:"20:01:06"}),(0,r.jsx)(n.td,{children:"Engine loaded and profiles activated (~16.6 GiB across 2 GPUs)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"API Server Ready"}),(0,r.jsx)(n.td,{children:"20:02:11.036"}),(0,r.jsx)(n.td,{children:"FastAPI + Uvicorn starts"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Health Check OK"}),(0,r.jsx)(n.td,{children:"20:02:18.781"}),(0,r.jsxs)(n.td,{children:[(0,r.jsx)(n.code,{children:"/v1/health/ready"})," endpoint returns 200 OK"]})]})]})]}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:["\u26a1 ",(0,r.jsx)(n.strong,{children:"Startup time (cold boot to ready): ~81 seconds"})," thanks to cached engine on EFS."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"test-the-model-with-a-prompt",children:"Test the Model with a Prompt"}),"\n",(0,r.jsx)(n.h4,{id:"step-1-port-forward-the-model-service",children:"Step 1: Port Forward the Model Service"}),"\n",(0,r.jsx)(n.p,{children:"Expose the model locally using port forwarding:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"kubectl port-forward -n nim-service service/meta-llama3-8b-instruct 8001:8000\n"})}),"\n",(0,r.jsx)(n.h4,{id:"step-2-send-a-sample-prompt-using-curl",children:"Step 2: Send a Sample Prompt Using curl"}),"\n",(0,r.jsx)(n.p,{children:"Run the following command to test the model with a chat prompt:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sh",children:'curl -X POST \\\n  http://localhost:8001/v1/chat/completions \\\n  -H \'Accept: application/json\' \\\n  -H \'Content-Type: application/json\' \\\n  -d \'{\n    "model": "meta/llama-3.1-8b-instruct",\n    "messages": [\n      {\n        "role": "user",\n        "content": "What should I do for a 4 day vacation at Cape Hatteras National Seashore?"\n      }\n    ],\n    "top_p": 1,\n    "n": 1,\n    "max_tokens": 1024,\n    "stream": false,\n    "frequency_penalty": 0.0,\n    "stop": ["STOP"]\n  }\'\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Sample Response (Shortened):"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'{"id":"chat-061a9dba9179437fa24cab7f7c767f19","object":"chat.completion","created":1743215809,"model":"meta/llama-3.1-8b-instruct","choices":[{"index":0,"message":{"role":"assistant","content":"Cape Hatteras National Seashore is a beautiful coastal destination with a rich history, pristine beaches,\n...\nexploration of the area\'s natural beauty and history. Feel free to modify it to suit your interests and preferences. Safe travels!"},"logprobs":null,"finish_reason":"stop","stop_reason":null}],"usage":{"prompt_tokens":30,"total_tokens":773,"completion_tokens":743},"prompt_logprobs":null}%\n\n'})}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.p,{children:"\ud83e\udde0 The model is now running with Tensor Parallelism = 2 across two A10G GPUs, each utilizing approximately 21.4 GiB of memory. Thanks to NIMCache backed by EFS, the model loaded quickly and is ready for low-latency inference."}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"open-webui-deployment",children:"Open WebUI Deployment"}),"\n",(0,r.jsx)(n.admonition,{type:"info",children:(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"https://github.com/open-webui/open-webui",children:"Open WebUI"})," is compatible only with models that work with the OpenAI API server and Ollama."]})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"1. Deploy the WebUI"})}),"\n",(0,r.jsxs)(n.p,{children:["Deploy the ",(0,r.jsx)(n.a,{href:"https://github.com/open-webui/open-webui",children:"Open WebUI"})," by running the following command:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sh",children:"kubectl apply -f ai-on-eks/blueprints/inference/gpu/nvidia-nim-operator-llama3-8b/openai-webui-deployment.yaml\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"2. Port Forward to Access WebUI"})}),"\n",(0,r.jsx)(n.p,{children:"Use kubectl port-forward to access the WebUI locally:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sh",children:"kubectl port-forward svc/open-webui 8081:80 -n openai-webui\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"3. Access the WebUI"})}),"\n",(0,r.jsxs)(n.p,{children:["Open your browser and go to ",(0,r.jsx)(n.a,{href:"http://localhost:8081",children:"http://localhost:8081"})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"4. Sign Up"})}),"\n",(0,r.jsx)(n.p,{children:"Sign up using your name, email, and a dummy password."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"5. Start a New Chat"})}),"\n",(0,r.jsx)(n.p,{children:"Click on New Chat and select the model from the dropdown menu, as shown in the screenshot below:"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"alt text",src:s(83031).A+"",width:"2076",height:"936"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"6. Enter Test Prompt"})}),"\n",(0,r.jsx)(n.p,{children:"Enter your prompt, and you will see the streaming results, as shown below:"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"alt text",src:s(59100).A+"",width:"2738",height:"1382"})}),"\n",(0,r.jsx)(n.h2,{id:"performance-testing-with-nvidia-genai-perf-tool",children:"Performance Testing with NVIDIA GenAI-Perf Tool"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/client/src/c%2B%2B/perf_analyzer/genai-perf/README.html",children:"GenAI-Perf"})," is a command line tool for measuring the throughput and latency of generative AI models as served through an inference server."]}),"\n",(0,r.jsxs)(n.p,{children:["GenAI-Perf can be used as standard tool to benchmark with other models deployed with inference server. But this tool requires a GPU. To make it easier, we provide you a pre-configured manifest ",(0,r.jsx)(n.code,{children:"genaiperf-deploy.yaml"})," to run the tool."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"cd ai-on-eks/blueprints/inference/gpu/nvidia-nim-operator-llama3-8b\nkubectl apply -f genaiperf-deploy.yaml\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Once the pod is ready with running status ",(0,r.jsx)(n.code,{children:"1/1"}),", can execute into the pod."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"export POD_NAME=$(kubectl get po -l app=genai-perf -ojsonpath='{.items[0].metadata.name}')\nkubectl exec -it $POD_NAME -- bash\n"})}),"\n",(0,r.jsx)(n.p,{children:"Run the testing to the deployed NIM Llama3 model"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"genai-perf profile -m meta/llama-3.1-8b-instruct \\\n  --url meta-llama3-8b-instruct.nim-service:8000 \\\n  --service-kind openai \\\n  --endpoint-type chat \\\n  --num-prompts 100 \\\n  --synthetic-input-tokens-mean 200 \\\n  --synthetic-input-tokens-stddev 0 \\\n  --output-tokens-mean 100 \\\n  --output-tokens-stddev 0 \\\n  --concurrency 20 \\\n  --streaming \\\n  --tokenizer hf-internal-testing/llama-tokenizer\n"})}),"\n",(0,r.jsx)(n.p,{children:"You should see similar output like the following"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"NIM Operator genai-perf result",src:s(84744).A+"",width:"1688",height:"404"})}),"\n",(0,r.jsxs)(n.p,{children:["You should be able to see the ",(0,r.jsx)(n.a,{href:"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/perf_analyzer/genai-perf/README.html#metrics",children:"metrics"})," that genai-perf collects, including Request latency, Out token throughput, Request throughput."]}),"\n",(0,r.jsxs)(n.p,{children:["To understand the command line options, please refer to ",(0,r.jsx)(n.a,{href:"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/perf_analyzer/genai-perf/README.html#command-line-options",children:"this documentation"}),"."]}),"\n",(0,r.jsx)(n.h3,{id:"grafana-dashboard",children:"Grafana Dashboard"}),"\n",(0,r.jsxs)(n.p,{children:["NVIDIA provided a Grafana ",(0,r.jsx)(n.a,{href:"https://docs.nvidia.com/nim/large-language-models/latest/_downloads/66e67782ce543dcccec574b1483f0ea0/nim-dashboard-example.json",children:"dashboard"})," to better visualize NIM status. In the Grafana dashboard, it contains several important metrics:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Time to First Token (TTFT)"}),": The latency between the initial inference request to the model and the return of the first token."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Inter-Token Latency (ITL)"}),": The latency between each token after the first."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Total Throughput"}),": The total number of tokens generated per second by the NIM."]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["You can find more metrics description from this ",(0,r.jsx)(n.a,{href:"https://docs.nvidia.com/nim/large-language-models/latest/observability.html",children:"document"}),"."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"NVIDIA LLM Server",src:s(89580).A+"",width:"3300",height:"1730"})}),"\n",(0,r.jsx)(n.p,{children:"You can monitor metrics such as Time-to-First-Token, Inter-Token-Latency, KV Cache Utilization metrics."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"NVIDIA NIM Metrics",src:s(26633).A+"",width:"3300",height:"1730"})}),"\n",(0,r.jsx)(n.p,{children:"To view the Grafana dashboard to monitor these metrics, follow the steps below:"}),"\n",(0,r.jsxs)(i,{children:[(0,r.jsx)("summary",{children:"Click to expand details"}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"1. Retrieve the Grafana password."})}),(0,r.jsx)(n.p,{children:"The password is saved in the AWS Secret Manager. Below Terraform command will show you the secret name."}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"terraform output grafana_secret_name\n"})}),(0,r.jsx)(n.p,{children:"Then use the output secret name to run below command,"}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'aws secretsmanager get-secret-value --secret-id <grafana_secret_name_output> --region $AWS_REGION --query "SecretString" --output text\n'})}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"2. Expose the Grafana Service"})}),(0,r.jsx)(n.p,{children:"Use port-forward to expose the Grafana service."}),(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"kubectl port-forward svc/kube-prometheus-stack-grafana 3000:80 -n monitoring\n"})}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"3. Login to Grafana:"})}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Open your web browser and navigate to ",(0,r.jsx)(n.a,{href:"http://localhost:3000",children:"http://localhost:3000"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["Login with the username ",(0,r.jsx)(n.code,{children:"admin"})," and the password retrieved from AWS Secrets Manager."]}),"\n"]}),(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"4. Open the NIM Monitoring Dashboard:"})}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:'Once logged in, click "Dashboards" on the left sidebar and search "nim"'}),"\n",(0,r.jsxs)(n.li,{children:["You can find the Dashboard ",(0,r.jsx)(n.code,{children:"NVIDIA NIM Monitoring"})," from the list"]}),"\n",(0,r.jsx)(n.li,{children:"Click and entering to the dashboard."}),"\n"]}),(0,r.jsx)(n.p,{children:"You should now see the metrics displayed on the Grafana dashboard, allowing you to monitor the performance your NVIDIA NIM service deployment."})]}),"\n",(0,r.jsx)(n.admonition,{type:"info",children:(0,r.jsxs)(n.p,{children:["As of writing this guide, NVIDIA also provides an example Grafana dashboard. You can check it from ",(0,r.jsx)(n.a,{href:"https://docs.nvidia.com/nim/large-language-models/latest/observability.html#grafana",children:"here"}),"."]})}),"\n",(0,r.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,r.jsxs)(n.p,{children:["This blueprint showcases how to deploy and scale ",(0,r.jsx)(n.strong,{children:"large language models like Meta\u2019s Llama 3.1 8B Instruct"})," efficiently on ",(0,r.jsx)(n.strong,{children:"Amazon EKS"})," using the ",(0,r.jsx)(n.strong,{children:"NVIDIA NIM Operator"}),"."]}),"\n",(0,r.jsxs)(n.p,{children:["By combining ",(0,r.jsx)(n.strong,{children:"OpenAI-compatible APIs"})," with ",(0,r.jsx)(n.strong,{children:"GPU-accelerated inference"}),", ",(0,r.jsx)(n.strong,{children:"declarative Kubernetes CRDs"})," (",(0,r.jsx)(n.code,{children:"NIMCache"}),", ",(0,r.jsx)(n.code,{children:"NIMService"}),"), and ",(0,r.jsx)(n.strong,{children:"fast model startup via EFS-based caching"}),", you get a streamlined, production-grade model deployment experience."]}),"\n",(0,r.jsx)(n.h3,{id:"key-benefits",children:"Key Benefits:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Faster cold starts"})," through shared, persistent model cache"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Declarative and repeatable deployments"})," with CRDs"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Dynamic GPU autoscaling"})," powered by Karpenter"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"One-click infrastructure provisioning"})," using Terraform"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["In just ",(0,r.jsx)(n.strong,{children:"~20 minutes"}),", you can go from zero to a ",(0,r.jsx)(n.strong,{children:"scalable LLM service on Kubernetes"})," \u2014 ready to serve real-world prompts with low latency and high efficiency."]}),"\n",(0,r.jsx)(n.h2,{id:"cleanup",children:"Cleanup"}),"\n",(0,r.jsx)(n.p,{children:"To tear down the deployed model and associated infrastructure:"}),"\n",(0,r.jsx)(n.h3,{id:"step-1-delete-model-resources",children:"Step 1: Delete Model Resources"}),"\n",(0,r.jsxs)(n.p,{children:["Delete the deployed ",(0,r.jsx)(n.code,{children:"NIMService"})," and ",(0,r.jsx)(n.code,{children:"NIMCache"})," objects from your cluster:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"cd blueprints/inference/gpu/nvidia-nim-operator-llama3-8b\n\nkubectl delete -f nim-service-llama3-8b-instruct.yaml\nkubectl delete -f nim-cache-llama3-8b-instruct.yaml\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Verify deletion:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"kubectl get nimservices.apps.nvidia.com -n nim-service\nkubectl get nimcaches.apps.nvidia.com -n nim-service\n"})}),"\n",(0,r.jsx)(n.h3,{id:"step-2-destroy-aws-infrastructure",children:"Step 2: Destroy AWS Infrastructure"}),"\n",(0,r.jsx)(n.p,{children:"Navigate back to the root Terraform module and run the cleanup script. This will destroy all AWS resources created for this blueprint, including the VPC, EKS cluster, EFS, and node groups:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"cd ai-on-eks/infra/nvidia-nim\n./cleanup.sh\n"})})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},83031:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/openweb-ui-nim-1-7fddca9b300b28af7260b0bc12b69ff0.png"},84744:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/nim-operator-genaiperf-63f2fb8f12dfd067b016c6c11d271765.png"},89580:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/nim-dashboard-c8dad192e95da70bd2474db5a466b448.png"}}]);