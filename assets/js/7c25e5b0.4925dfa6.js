"use strict";(self.webpackChunkdoeks_website=self.webpackChunkdoeks_website||[]).push([[8418],{10278:(e,n,i)=>{i.d(n,{A:()=>s});const s='# ConfigMap containing Python scripts for MPS pods\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: mps-scripts-configmap\n  namespace: mps-gpu\ndata:\n  inference-script.py: |\n    import torch\n    import torch.nn as nn\n    import time\n    import os\n\n    print(f"=== INFERENCE CONTAINER STARTING ===")\n    print(f"Process ID: {os.getpid()}")\n    print(f"GPU available: {torch.cuda.is_available()}")\n    print(f"GPU count: {torch.cuda.device_count()}")\n\n    if torch.cuda.is_available():\n        device = torch.cuda.current_device()\n        print(f"Current GPU: {torch.cuda.get_device_name(device)}")\n        print(f"GPU Memory: {torch.cuda.get_device_properties(device).total_memory / 1024**3:.1f} GB")\n\n        # Create inference model\n        model = nn.Sequential(\n            nn.Linear(1000, 500),\n            nn.ReLU(),\n            nn.Linear(500, 100)\n        ).cuda()\n\n        # Run inference\n        for i in range(1, 999999):\n            with torch.no_grad():\n                x = torch.randn(128, 1000).cuda()\n                output = model(x)\n                result = torch.sum(output)\n                print(f"Inference Container PID {os.getpid()}: Batch {i}, Result: {result.item():.2f} at {time.strftime(\'%H:%M:%S\')}")\n            time.sleep(2)\n    else:\n        print("No GPU available!")\n        time.sleep(60)\n\n  training-script.py: |\n    import torch\n    import torch.nn as nn\n    import time\n    import os\n\n    print(f"=== TRAINING CONTAINER STARTING ===")\n    print(f"Process ID: {os.getpid()}")\n    print(f"GPU available: {torch.cuda.is_available()}")\n    print(f"GPU count: {torch.cuda.device_count()}")\n\n    if torch.cuda.is_available():\n        device = torch.cuda.current_device()\n        print(f"Current GPU: {torch.cuda.get_device_name(device)}")\n        print(f"GPU Memory: {torch.cuda.get_device_properties(device).total_memory / 1024**3:.1f} GB")\n\n        # Create training model\n        model = nn.Sequential(\n            nn.Linear(2000, 1000),\n            nn.ReLU(),\n            nn.Linear(1000, 500),\n            nn.ReLU(),\n            nn.Linear(500, 10)\n        ).cuda()\n\n        criterion = nn.MSELoss()\n        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n        # Run training\n        for epoch in range(1, 999999):\n            x = torch.randn(64, 2000).cuda()\n            target = torch.randn(64, 10).cuda()\n\n            optimizer.zero_grad()\n            output = model(x)\n            loss = criterion(output, target)\n            loss.backward()\n            optimizer.step()\n\n            print(f"Training Container PID {os.getpid()}: Epoch {epoch}, Loss: {loss.item():.4f} at {time.strftime(\'%H:%M:%S\')}")\n            time.sleep(3)\n    else:\n        print("No GPU available!")\n        time.sleep(60)\n---\n# Single Pod with Multiple Containers sharing GPU via MPS\napiVersion: v1\nkind: Pod\nmetadata:\n  name: mps-multi-container-pod\n  namespace: mps-gpu\n  labels:\n    app: mps-demo\nspec:\n  restartPolicy: Never\n  containers:\n  # Container 1 - Inference workload\n  - name: inference-container\n    image: nvcr.io/nvidia/pytorch:25.04-py3\n    command: ["python", "/scripts/inference-script.py"]\n    volumeMounts:\n    - name: script-volume\n      mountPath: /scripts\n      readOnly: true\n    resources:\n      claims:\n      - name: shared-gpu-claim\n        request: shared-gpu\n  # Container 2 - Training workload\n  - name: training-container\n    image: nvcr.io/nvidia/pytorch:25.04-py3\n    command: ["python", "/scripts/training-script.py"]\n    volumeMounts:\n    - name: script-volume\n      mountPath: /scripts\n      readOnly: true\n    resources:\n      claims:\n      - name: shared-gpu-claim\n        request: shared-gpu\n  resourceClaims:\n  - name: shared-gpu-claim\n    resourceClaimTemplateName: mps-gpu-template\n  nodeSelector:\n    NodeGroupType: g6-mng\n    nvidia.com/gpu.present: "true"\n  tolerations:\n  - key: nvidia.com/gpu\n    operator: Exists\n    effect: NoSchedule\n  volumes:\n  - name: script-volume\n    configMap:\n      name: mps-scripts-configmap\n      defaultMode: 0755\n'},20245:(e,n,i)=>{i.d(n,{A:()=>s});const s='apiVersion: v1\nkind: Pod\nmetadata:\n  namespace: gpu-test1\n  name: gpu-pod\n  labels:\n    app: pod\nspec:\n  containers:\n  - name: ctr0\n    image: ubuntu:22.04\n    command: ["bash", "-c"]\n    args: ["nvidia-smi -L; trap \'exit 0\' TERM; sleep 9999 & wait"]\n    resources:\n      claims:\n      - name: gpu0\n  resourceClaims:\n  - name: gpu0\n    resourceClaimTemplateName: single-gpu\n  nodeSelector:\n    NodeGroupType: g6-mng\n    nvidia.com/gpu.present: "true"\n  tolerations:\n  - key: "nvidia.com/gpu"\n    operator: "Exists"\n    effect: "NoSchedule"\n'},21438:(e,n,i)=>{i.d(n,{A:()=>s});const s='# ConfigMap containing Python scripts for timeslicing pods\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: timeslicing-scripts-configmap\n  namespace: timeslicing-gpu\ndata:\n  inference-script.py: |\n    import torch\n    import time\n    import os\n    print(f"=== POD 1 STARTING ===")\n    print(f"GPU available: {torch.cuda.is_available()}")\n    print(f"GPU count: {torch.cuda.device_count()}")\n    if torch.cuda.is_available():\n        device = torch.cuda.current_device()\n        print(f"Current GPU: {torch.cuda.get_device_name(device)}")\n        print(f"GPU Memory: {torch.cuda.get_device_properties(device).total_memory / 1024**3:.1f} GB")\n        # Simulate inference workload\n        for i in range(20):\n            x = torch.randn(1000, 1000).cuda()\n            y = torch.mm(x, x.t())\n            print(f"Pod 1 - Iteration {i+1} completed at {time.strftime(\'%H:%M:%S\')}")\n            time.sleep(5)\n    else:\n        print("No GPU available!")\n        time.sleep(60)\n\n  training-script.py: |\n    import torch\n    import time\n    import os\n    print(f"=== POD 2 STARTING ===")\n    print(f"GPU available: {torch.cuda.is_available()}")\n    print(f"GPU count: {torch.cuda.device_count()}")\n    if torch.cuda.is_available():\n        device = torch.cuda.current_device()\n        print(f"Current GPU: {torch.cuda.get_device_name(device)}")\n        print(f"GPU Memory: {torch.cuda.get_device_properties(device).total_memory / 1024**3:.1f} GB")\n        # Simulate training workload with heavier compute\n        for i in range(15):\n            x = torch.randn(2000, 2000).cuda()\n            y = torch.mm(x, x.t())\n            loss = torch.sum(y)\n            print(f"Pod 2 - Training step {i+1}, Loss: {loss.item():.2f} at {time.strftime(\'%H:%M:%S\')}")\n            time.sleep(5)\n    else:\n        print("No GPU available!")\n        time.sleep(60)\n---\n# Pod 1 - Inference workload\napiVersion: v1\nkind: Pod\nmetadata:\n  name: inference-pod-1\n  namespace: timeslicing-gpu\n  labels:\n    app: gpu-inference\nspec:\n  restartPolicy: Never\n  containers:\n  - name: inference-container\n    image: nvcr.io/nvidia/pytorch:25.04-py3\n    command: ["python", "/scripts/inference-script.py"]\n    volumeMounts:\n    - name: script-volume\n      mountPath: /scripts\n      readOnly: true\n    resources:\n      claims:\n      - name: shared-gpu-claim\n  resourceClaims:\n  - name: shared-gpu-claim\n    resourceClaimTemplateName: timeslicing-gpu-template\n  nodeSelector:\n    NodeGroupType: g6-mng\n    nvidia.com/gpu.present: "true"\n  tolerations:\n  - key: nvidia.com/gpu\n    operator: Exists\n    effect: NoSchedule\n  volumes:\n  - name: script-volume\n    configMap:\n      name: timeslicing-scripts-configmap\n      defaultMode: 0755\n---\n# Pod 2 - Training workload\napiVersion: v1\nkind: Pod\nmetadata:\n  name: training-pod-2\n  namespace: timeslicing-gpu\n  labels:\n    app: gpu-training\nspec:\n  restartPolicy: Never\n  containers:\n  - name: training-container\n    image: nvcr.io/nvidia/pytorch:25.04-py3\n    command: ["python", "/scripts/training-script.py"]\n    volumeMounts:\n    - name: script-volume\n      mountPath: /scripts\n      readOnly: true\n    resources:\n      claims:\n      - name: shared-gpu-claim-2\n  resourceClaims:\n  - name: shared-gpu-claim-2\n    resourceClaimTemplateName: timeslicing-gpu-template\n  nodeSelector:\n    NodeGroupType: g6-mng\n    nvidia.com/gpu.present: "true"\n  tolerations:\n  - key: nvidia.com/gpu\n    operator: Exists\n    effect: NoSchedule\n  volumes:\n  - name: script-volume\n    configMap:\n      name: timeslicing-scripts-configmap\n      defaultMode: 0755\n'},25204:(e,n,i)=>{i.d(n,{A:()=>s});const s='apiVersion: v1\nkind: Namespace\nmetadata:\n  name: timeslicing-gpu\n---\napiVersion: resource.k8s.io/v1beta1\nkind: ResourceClaimTemplate\nmetadata:\n  name: timeslicing-gpu-template\n  namespace: timeslicing-gpu\nspec:\n  spec:\n    devices:\n      requests:\n      - name: shared-gpu\n        deviceClassName: gpu.nvidia.com\n      config:\n      - requests: ["shared-gpu"]\n        opaque:\n          driver: gpu.nvidia.com\n          parameters:\n            apiVersion: resource.nvidia.com/v1beta1\n            kind: GpuConfig\n            sharing:\n              strategy: TimeSlicing\n'},37981:(e,n,i)=>{i.d(n,{A:()=>s});const s="apiVersion: v1\nkind: Namespace\nmetadata:\n  name: gpu-test1\n---\napiVersion: resource.k8s.io/v1beta1\nkind: ResourceClaimTemplate\nmetadata:\n  namespace: gpu-test1\n  name: single-gpu\nspec:\n  spec:\n    devices:\n      requests:\n      - name: gpu\n        deviceClassName: gpu.nvidia.com\n"},47338:(e,n,i)=>{i.d(n,{A:()=>s});const s='# ConfigMap containing Python scripts for MIG pods\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: mig-scripts-configmap\n  namespace: mig-gpu\ndata:\n  large-training-script.py: |\n    import torch\n    import torch.nn as nn\n    import torch.optim as optim\n    import time\n    import os\n\n    print(f"=== LARGE TRAINING POD (3g.40gb) ===")\n    print(f"Process ID: {os.getpid()}")\n    print(f"GPU available: {torch.cuda.is_available()}")\n    print(f"GPU count: {torch.cuda.device_count()}")\n\n    if torch.cuda.is_available():\n        device = torch.cuda.current_device()\n        print(f"Using GPU: {torch.cuda.get_device_name(device)}")\n        print(f"GPU Memory: {torch.cuda.get_device_properties(device).total_memory / 1e9:.1f} GB")\n\n        # Large model for 3g.40gb instance\n        model = nn.Sequential(\n            nn.Linear(2048, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Linear(256, 10)\n        ).cuda()\n\n        optimizer = optim.Adam(model.parameters())\n        criterion = nn.CrossEntropyLoss()\n\n        print(f"Model parameters: {sum(p.numel() for p in model.parameters())}")\n\n        # Training loop\n        for epoch in range(100):\n            # Large batch for 3g.40gb\n            x = torch.randn(256, 2048).cuda()\n            y = torch.randint(0, 10, (256,)).cuda()\n\n            optimizer.zero_grad()\n            output = model(x)\n            loss = criterion(output, y)\n            loss.backward()\n            optimizer.step()\n\n            if epoch % 10 == 0:\n                print(f"Large Training - Epoch {epoch}, Loss: {loss.item():.4f}, GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f}GB")\n            time.sleep(3)\n\n        print("Large training completed on 3g.40gb MIG instance")\n\n  medium-training-script.py: |\n    import torch\n    import torch.nn as nn\n    import torch.optim as optim\n    import time\n    import os\n\n    print(f"=== MEDIUM TRAINING POD (2g.20gb) ===")\n    print(f"Process ID: {os.getpid()}")\n    print(f"GPU available: {torch.cuda.is_available()}")\n    print(f"GPU count: {torch.cuda.device_count()}")\n\n    if torch.cuda.is_available():\n        device = torch.cuda.current_device()\n        print(f"Using GPU: {torch.cuda.get_device_name(device)}")\n        print(f"GPU Memory: {torch.cuda.get_device_properties(device).total_memory / 1e9:.1f} GB")\n\n        # Medium model for 2g.20gb instance\n        model = nn.Sequential(\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Linear(256, 10)\n        ).cuda()\n\n        optimizer = optim.Adam(model.parameters())\n        criterion = nn.CrossEntropyLoss()\n\n        print(f"Model parameters: {sum(p.numel() for p in model.parameters())}")\n\n        # Training loop\n        for epoch in range(100):\n            # Medium batch for 2g.20gb\n            x = torch.randn(128, 1024).cuda()\n            y = torch.randint(0, 10, (128,)).cuda()\n\n            optimizer.zero_grad()\n            output = model(x)\n            loss = criterion(output, y)\n            loss.backward()\n            optimizer.step()\n\n            if epoch % 10 == 0:\n                print(f"Medium Training - Epoch {epoch}, Loss: {loss.item():.4f}, GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f}GB")\n            time.sleep(4)\n\n        print("Medium training completed on 2g.20gb MIG instance")\n\n  small-inference-script.py: |\n    import torch\n    import torch.nn as nn\n    import time\n    import os\n\n    print(f"=== SMALL INFERENCE POD (1g.10gb) ===")\n    print(f"Process ID: {os.getpid()}")\n    print(f"GPU available: {torch.cuda.is_available()}")\n    print(f"GPU count: {torch.cuda.device_count()}")\n\n    if torch.cuda.is_available():\n        device = torch.cuda.current_device()\n        print(f"Using GPU: {torch.cuda.get_device_name(device)}")\n        print(f"GPU Memory: {torch.cuda.get_device_properties(device).total_memory / 1e9:.1f} GB")\n\n        # Small model for 1g.10gb instance\n        model = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Linear(256, 10)\n        ).cuda()\n\n        print(f"Model parameters: {sum(p.numel() for p in model.parameters())}")\n\n        # Inference loop\n        for i in range(200):\n            with torch.no_grad():\n                # Small batch for 1g.10gb\n                x = torch.randn(32, 512).cuda()\n                output = model(x)\n                prediction = torch.argmax(output, dim=1)\n\n                if i % 20 == 0:\n                    print(f"Small Inference - Batch {i}, Predictions: {prediction[:5].tolist()}, GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f}GB")\n            time.sleep(2)\n\n        print("Small inference completed on 1g.10gb MIG instance")\n---\n# Pod 1: Large training workload (3g.40gb)\napiVersion: v1\nkind: Pod\nmetadata:\n  name: mig-large-training-pod\n  namespace: mig-gpu\n  labels:\n    app: mig-large-training\n    workload-type: training\nspec:\n  restartPolicy: Never\n  containers:\n  - name: large-training-container\n    image: nvcr.io/nvidia/pytorch:25.04-py3\n    command: ["python", "/scripts/large-training-script.py"]\n    volumeMounts:\n    - name: script-volume\n      mountPath: /scripts\n      readOnly: true\n    resources:\n      claims:\n      - name: mig-large-claim\n  resourceClaims:\n  - name: mig-large-claim\n    resourceClaimTemplateName: mig-large-template\n  nodeSelector:\n    node.kubernetes.io/instance-type: p4de.24xlarge\n    nvidia.com/gpu.present: "true"\n  tolerations:\n  - key: nvidia.com/gpu\n    operator: Exists\n    effect: NoSchedule\n  volumes:\n  - name: script-volume\n    configMap:\n      name: mig-scripts-configmap\n      defaultMode: 0755\n---\n# Pod 2: Medium training workload (2g.20gb) - can run on SAME GPU as Pod 1\napiVersion: v1\nkind: Pod\nmetadata:\n  name: mig-medium-training-pod\n  namespace: mig-gpu\n  labels:\n    app: mig-medium-training\n    workload-type: training\nspec:\n  restartPolicy: Never\n  containers:\n  - name: medium-training-container\n    image: nvcr.io/nvidia/pytorch:25.04-py3\n    command: ["python", "/scripts/medium-training-script.py"]\n    volumeMounts:\n    - name: script-volume\n      mountPath: /scripts\n      readOnly: true\n    resources:\n      claims:\n      - name: mig-medium-claim\n  resourceClaims:\n  - name: mig-medium-claim\n    resourceClaimTemplateName: mig-medium-template\n  nodeSelector:\n    node.kubernetes.io/instance-type: p4de.24xlarge\n    nvidia.com/gpu.present: "true"\n  tolerations:\n  - key: nvidia.com/gpu\n    operator: Exists\n    effect: NoSchedule\n  volumes:\n  - name: script-volume\n    configMap:\n      name: mig-scripts-configmap\n      defaultMode: 0755\n---\n# Pod 3: Small inference workload (1g.10gb) - can run on SAME GPU as Pod 1 & 2\napiVersion: v1\nkind: Pod\nmetadata:\n  name: mig-small-inference-pod\n  namespace: mig-gpu\n  labels:\n    app: mig-small-inference\n    workload-type: inference\nspec:\n  restartPolicy: Never\n  containers:\n  - name: small-inference-container\n    image: nvcr.io/nvidia/pytorch:25.04-py3\n    command: ["python", "/scripts/small-inference-script.py"]\n    volumeMounts:\n    - name: script-volume\n      mountPath: /scripts\n      readOnly: true\n    resources:\n      claims:\n      - name: mig-small-claim\n  resourceClaims:\n  - name: mig-small-claim\n    resourceClaimTemplateName: mig-small-template\n  nodeSelector:\n    node.kubernetes.io/instance-type: p4de.24xlarge\n    nvidia.com/gpu.present: "true"\n  tolerations:\n  - key: nvidia.com/gpu\n    operator: Exists\n    effect: NoSchedule\n  volumes:\n  - name: script-volume\n    configMap:\n      name: mig-scripts-configmap\n      defaultMode: 0755\n'},74864:(e,n,i)=>{i.d(n,{A:()=>s});const s="apiVersion: v1\nkind: Namespace\nmetadata:\n  name: mig-gpu\n---\n# Template for 3g.40gb MIG instance (Large training)\napiVersion: resource.k8s.io/v1beta1\nkind: ResourceClaimTemplate\nmetadata:\n  name: mig-large-template\n  namespace: mig-gpu\nspec:\n  spec:\n    devices:\n      requests:\n      - name: mig-large\n        deviceClassName: mig.nvidia.com\n        selectors:\n        - cel:\n            expression: |\n              device.attributes['gpu.nvidia.com'].profile == '3g.40gb'\n---\n# Template for 2g.20gb MIG instance (Medium training)\napiVersion: resource.k8s.io/v1beta1\nkind: ResourceClaimTemplate\nmetadata:\n  name: mig-medium-template\n  namespace: mig-gpu\nspec:\n  spec:\n    devices:\n      requests:\n      - name: mig-medium\n        deviceClassName: mig.nvidia.com\n        selectors:\n        - cel:\n            expression: |\n              device.attributes['gpu.nvidia.com'].profile == '2g.20gb'\n---\n# Template for 1g.10gb MIG instance (Small inference)\napiVersion: resource.k8s.io/v1beta1\nkind: ResourceClaimTemplate\nmetadata:\n  name: mig-small-template\n  namespace: mig-gpu\nspec:\n  spec:\n    devices:\n      requests:\n      - name: mig-small\n        deviceClassName: mig.nvidia.com\n        selectors:\n        - cel:\n            expression: |\n              device.attributes['gpu.nvidia.com'].profile == '1g.10gb'\n"},79572:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>B,contentTitle:()=>z,default:()=>F,frontMatter:()=>V,metadata:()=>s,toc:()=>q});const s=JSON.parse('{"id":"resources/dynamic-resource-allocation","title":"Dynamic Resource Allocation for GPUs on Amazon EKS","description":"\ud83d\ude80 TL;DR \u2013 Dynamic GPU Scheduling with DRA on EKS","source":"@site/docs/resources/dynamic-resource-allocation.md","sourceDirName":"resources","slug":"/resources/dynamic-resource-allocation","permalink":"/ai-on-eks/docs/resources/dynamic-resource-allocation","draft":false,"unlisted":false,"editUrl":"https://github.com/awslabs/ai-on-eks/blob/main/website/docs/resources/dynamic-resource-allocation.md","tags":[],"version":"current","frontMatter":{"sidebar_label":"Dynamic Resource Allocation on EKS","mermaid":true},"sidebar":"resources","previous":{"title":"Bin packing for Amazon EKS","permalink":"/ai-on-eks/docs/resources/binpacking-custom-scheduler-eks"},"next":{"title":"Observability","permalink":"/ai-on-eks/docs/resources/observability"}}');var t=i(74848),a=i(28453),r=i(96540),o=i(34164),l=i(23104),c=i(56347),d=i(205),u=i(57485),p=i(31682),m=i(70679);function h(e){return r.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,r.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function g(e){const{values:n,children:i}=e;return(0,r.useMemo)(()=>{const e=n??function(e){return h(e).map(({props:{value:e,label:n,attributes:i,default:s}})=>({value:e,label:n,attributes:i,default:s}))}(i);return function(e){const n=(0,p.XI)(e,(e,n)=>e.value===n.value);if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[n,i])}function x({value:e,tabValues:n}){return n.some(n=>n.value===e)}function v({queryString:e=!1,groupId:n}){const i=(0,c.W6)(),s=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,u.aZ)(s),(0,r.useCallback)(e=>{if(!s)return;const n=new URLSearchParams(i.location.search);n.set(s,e),i.replace({...i.location,search:n.toString()})},[s,i])]}function f(e){const{defaultValue:n,queryString:i=!1,groupId:s}=e,t=g(e),[a,o]=(0,r.useState)(()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!x({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const i=n.find(e=>e.default)??n[0];if(!i)throw new Error("Unexpected error: 0 tabValues");return i.value}({defaultValue:n,tabValues:t})),[l,c]=v({queryString:i,groupId:s}),[u,p]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[i,s]=(0,m.Dv)(n);return[i,(0,r.useCallback)(e=>{n&&s.set(e)},[n,s])]}({groupId:s}),h=(()=>{const e=l??u;return x({value:e,tabValues:t})?e:null})();(0,d.A)(()=>{h&&o(h)},[h]);return{selectedValue:a,selectValue:(0,r.useCallback)(e=>{if(!x({value:e,tabValues:t}))throw new Error(`Can't select invalid tab value=${e}`);o(e),c(e),p(e)},[c,p,t]),tabValues:t}}var j=i(92303);const y={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};function b({className:e,block:n,selectedValue:i,selectValue:s,tabValues:a}){const r=[],{blockElementScrollPositionUntilNextRender:c}=(0,l.a_)(),d=e=>{const n=e.currentTarget,t=r.indexOf(n),o=a[t].value;o!==i&&(c(n),s(o))},u=e=>{let n=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{const i=r.indexOf(e.currentTarget)+1;n=r[i]??r[0];break}case"ArrowLeft":{const i=r.indexOf(e.currentTarget)-1;n=r[i]??r[r.length-1];break}}n?.focus()};return(0,t.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,o.A)("tabs",{"tabs--block":n},e),children:a.map(({value:e,label:n,attributes:s})=>(0,t.jsx)("li",{role:"tab",tabIndex:i===e?0:-1,"aria-selected":i===e,ref:e=>{r.push(e)},onKeyDown:u,onClick:d,...s,className:(0,o.A)("tabs__item",y.tabItem,s?.className,{"tabs__item--active":i===e}),children:n??e},e))})}function w({lazy:e,children:n,selectedValue:i}){const s=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=s.find(e=>e.props.value===i);return e?(0,r.cloneElement)(e,{className:(0,o.A)("margin-top--md",e.props.className)}):null}return(0,t.jsx)("div",{className:"margin-top--md",children:s.map((e,n)=>(0,r.cloneElement)(e,{key:n,hidden:e.props.value!==i}))})}function P(e){const n=f(e);return(0,t.jsxs)("div",{className:(0,o.A)("tabs-container",y.tabList),children:[(0,t.jsx)(b,{...n,...e}),(0,t.jsx)(w,{...n,...e})]})}function k(e){const n=(0,j.A)();return(0,t.jsx)(P,{...e,children:h(e.children)},String(n))}const A={tabItem:"tabItem_Ymn6"};function D({children:e,hidden:n,className:i}){return(0,t.jsx)("div",{role:"tabpanel",className:(0,o.A)(A.tabItem,i),hidden:n,children:e})}var G=i(21028),R=i(27293);const C={statusGrid:"statusGrid_yukc",statusBadge:"statusBadge_vHeB",statusBadgeBeta:"statusBadgeBeta_HeJs",statusBadgeProduction:"statusBadgeProduction_H1Xf",statusBadgeWarning:"statusBadgeWarning_NMyw",statusBadgeInfo:"statusBadgeInfo_l0Xz",badgeIcon:"badgeIcon_t9jD",badgeContent:"badgeContent_otGx",badgeTitle:"badgeTitle_t1R8",badgeValue:"badgeValue_YSlu",badgeNote:"badgeNote_j_gI"};function N({badges:e}){return(0,t.jsx)("div",{className:C.statusGrid,children:e.map((e,n)=>(0,t.jsxs)("div",{className:`${C.statusBadge} ${C[`statusBadge${e.type.charAt(0).toUpperCase()+e.type.slice(1)}`]}`,children:[(0,t.jsx)("span",{className:C.badgeIcon,children:e.icon}),(0,t.jsxs)("div",{className:C.badgeContent,children:[(0,t.jsx)("div",{className:C.badgeTitle,children:e.title}),(0,t.jsx)("div",{className:C.badgeValue,children:e.value}),e.note&&(0,t.jsx)("div",{className:C.badgeNote,children:e.note})]})]},n))})}const S={callout:"callout_FLm9",calloutCritical:"calloutCritical_bQ1o",calloutWarning:"calloutWarning_HQtz",calloutInfo:"calloutInfo_xSrk",calloutHeader:"calloutHeader_B5_v",calloutIcon:"calloutIcon_GXtC",calloutContent:"calloutContent_aVah",statHighlight:"statHighlight_FjI5",statNumber:"statNumber_ZY16",statLabel:"statLabel_TIh8"};function I({icon:e,title:n,statNumber:i,statLabel:s,description:a,type:r="critical"}){return(0,t.jsxs)("div",{className:`${S.callout} ${S[`callout${r.charAt(0).toUpperCase()+r.slice(1)}`]}`,children:[(0,t.jsxs)("div",{className:S.calloutHeader,children:[(0,t.jsx)("span",{className:S.calloutIcon,children:e}),(0,t.jsx)("h4",{children:n})]}),(0,t.jsxs)("div",{className:S.calloutContent,children:[(0,t.jsxs)("div",{className:S.statHighlight,children:[(0,t.jsx)("span",{className:S.statNumber,children:i}),(0,t.jsx)("span",{className:S.statLabel,children:s})]}),(0,t.jsx)("p",{children:a})]})]})}const U={sectionDivider:"sectionDivider_z07Q",dividerLine:"dividerLine_oAnl",dividerIcon:"dividerIcon_Soi9"};function _({icon:e}){return(0,t.jsxs)("div",{className:U.sectionDivider,children:[(0,t.jsx)("div",{className:U.dividerLine}),(0,t.jsx)("div",{className:U.dividerIcon,children:e}),(0,t.jsx)("div",{className:U.dividerLine})]})}const M={capabilityComparison:"capabilityComparison_G6Wg",comparisonHeader:"comparisonHeader_puLN",comparisonRow:"comparisonRow_muiZ",capabilityName:"capabilityName_VMRR",traditionalCell:"traditionalCell_nR1h",draCell:"draCell_y36_",capabilityStatus:"capabilityStatus_V2IX",capabilityStatusFull:"capabilityStatusFull_MpPh",capabilityStatusLimited:"capabilityStatusLimited_tWm3",capabilityStatusNone:"capabilityStatusNone_kihA",capabilityDesc:"capabilityDesc_xW7N"};function T({capabilities:e}){return(0,t.jsxs)("div",{className:M.capabilityComparison,children:[(0,t.jsxs)("div",{className:M.comparisonHeader,children:[(0,t.jsx)("div",{className:M.capabilityCol,children:"Capability"}),(0,t.jsx)("div",{className:M.traditionalCol,children:"\ud83d\udd34 Traditional Device Plugin"}),(0,t.jsx)("div",{className:M.draCol,children:"\ud83d\udfe2 Dynamic Resource Allocation (DRA)"})]}),e.map((e,n)=>(0,t.jsxs)("div",{className:M.comparisonRow,children:[(0,t.jsx)("div",{className:M.capabilityName,children:(0,t.jsx)("strong",{children:e.name})}),(0,t.jsxs)("div",{className:M.traditionalCell,children:[(0,t.jsx)("span",{className:`${M.capabilityStatus} ${M[`capabilityStatus${e.traditional.status.charAt(0).toUpperCase()+e.traditional.status.slice(1)}`]}`,children:e.traditional.icon}),(0,t.jsxs)("div",{className:M.capabilityDesc,children:[e.traditional.description,e.traditional.code&&(0,t.jsx)("div",{children:(0,t.jsx)("code",{children:e.traditional.code})})]})]}),(0,t.jsxs)("div",{className:M.draCell,children:[(0,t.jsx)("span",{className:`${M.capabilityStatus} ${M[`capabilityStatus${e.dra.status.charAt(0).toUpperCase()+e.dra.status.slice(1)}`]}`,children:e.dra.icon}),(0,t.jsxs)("div",{className:M.capabilityDesc,children:[e.dra.description,e.dra.code&&(0,t.jsx)("div",{children:(0,t.jsx)("code",{children:e.dra.code})})]})]})]},n))]})}const E={progressSteps:"progressSteps_j3Cn",step:"step_wyHJ",stepNumber:"stepNumber_JvFJ",stepCurrent:"stepCurrent_baCR",stepCompleted:"stepCompleted_BhoT",stepContent:"stepContent_kF1B"};function L({steps:e,currentStep:n=0}){return(0,t.jsx)("div",{className:E.progressSteps,children:e.map((e,i)=>(0,t.jsxs)("div",{className:`${E.step} ${i===n?E.stepCurrent:i<n?E.stepCompleted:""}`,children:[(0,t.jsx)("div",{className:E.stepNumber,children:i+1}),(0,t.jsxs)("div",{className:E.stepContent,children:[(0,t.jsx)("h4",{children:e.title}),(0,t.jsx)("p",{children:e.description})]})]},i))})}const V={sidebar_label:"Dynamic Resource Allocation on EKS",mermaid:!0},z="Dynamic Resource Allocation for GPUs on Amazon EKS",B={},q=[{value:"DRA Advantages over Traditional GPU Scheduling",id:"dra-advantages-over-traditional-gpu-scheduling",level:3},{value:"Why Managed/Self-Managed Node Groups vs Karpenter for DRA?",id:"why-managedself-managed-node-groups-vs-karpenter-for-dra",level:3},{value:"Can I Use Both Traditional GPU Allocation and DRA Together?",id:"can-i-use-both-traditional-gpu-allocation-and-dra-together",level:3},{value:"Production Readiness",id:"production-readiness",level:3},{value:"The GPU Scheduling Challenge in Kubernetes",id:"the-gpu-scheduling-challenge-in-kubernetes",level:2},{value:"Current State: Traditional GPU Allocation",id:"current-state-traditional-gpu-allocation",level:3},{value:"The GPU Utilization Crisis",id:"the-gpu-utilization-crisis",level:3},{value:"Enter Dynamic Resource Allocation (DRA)",id:"enter-dynamic-resource-allocation-dra",level:2},{value:"What DRA Changes",id:"what-dra-changes",level:3},{value:"Key DRA Innovations",id:"key-dra-innovations",level:3},{value:"Understanding IMEX, ComputeDomains, and Amazon EC2 P6e-GB200 Multi-Node Scheduling",id:"understanding-imex-computedomains-and-amazon-ec2-p6e-gb200-multi-node-scheduling",level:3},{value:"Implementation Considerations for EKS",id:"implementation-considerations-for-eks",level:2},{value:"Managed Node Groups vs Karpenter for P-Series GPU Instances and DRA",id:"managed-node-groups-vs-karpenter-for-p-series-gpu-instances-and-dra",level:3},{value:"DRA and Traditional GPU Allocation Coexistence",id:"dra-and-traditional-gpu-allocation-coexistence",level:3},{value:"Visual Comparison: Traditional vs DRA",id:"visual-comparison-traditional-vs-dra",level:3},{value:"Technical Capabilities Comparison",id:"technical-capabilities-comparison",level:3},{value:"How DRA Actually Works: The Complete Technical Flow",id:"how-dra-actually-works-the-complete-technical-flow",level:2},{value:"Step-by-step DRA Workflow",id:"step-by-step-dra-workflow",level:3},{value:"1. Resource Discovery and Advertisement",id:"1-resource-discovery-and-advertisement",level:4},{value:"2. DeviceClass Registration",id:"2-deviceclass-registration",level:4},{value:"3. Resource Claim Creation",id:"3-resource-claim-creation",level:4},{value:"4. Intelligent Scheduling",id:"4-intelligent-scheduling",level:4},{value:"5. Dynamic Allocation",id:"5-dynamic-allocation",level:4},{value:"Deploying the Solution",id:"deploying-the-solution",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Deploy",id:"deploy",level:3},{value:"1. Clone the repository:",id:"1-clone-the-repository",level:4},{value:"2. Review and customize configurations:",id:"2-review-and-customize-configurations",level:4},{value:"3. Navigate to the deployment directory and run the install script:",id:"3-navigate-to-the-deployment-directory-and-run-the-install-script",level:4},{value:"4. Verify Deployment",id:"4-verify-deployment",level:4},{value:"Component Architecture",id:"component-architecture",level:3},{value:"GPU Sharing Strategies: Technical Deep Dive",id:"gpu-sharing-strategies-technical-deep-dive",level:2},{value:"Basic GPU Allocation",id:"basic-gpu-allocation",level:3},{value:"What is Time-Slicing?",id:"what-is-time-slicing",level:3},{value:"How to Deploy Time-Slicing with DRA",id:"how-to-deploy-time-slicing-with-dra",level:3},{value:"What is MPS?",id:"what-is-mps",level:3},{value:"How to Deploy MPS with DRA",id:"how-to-deploy-mps-with-dra",level:3},{value:"What is MIG?",id:"what-is-mig",level:3},{value:"How to Deploy MIG with DRA",id:"how-to-deploy-mig-with-dra",level:3},{value:"Strategy Selection Guide",id:"strategy-selection-guide",level:3},{value:"Cleanup",id:"cleanup",level:2},{value:"Removing DRA Components",id:"removing-dra-components",level:3},{value:"Conclusion",id:"conclusion",level:2}];function K(e){const n={a:"a",admonition:"admonition",blockquote:"blockquote",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",mermaid:"mermaid",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components},{Details:s}=n;return s||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"dynamic-resource-allocation-for-gpus-on-amazon-eks",children:"Dynamic Resource Allocation for GPUs on Amazon EKS"})}),"\n",(0,t.jsxs)(s,{children:[(0,t.jsx)("summary",{children:(0,t.jsx)("strong",{children:"\ud83d\ude80 TL;DR \u2013 Dynamic GPU Scheduling with DRA on EKS"})}),(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"DRA is the next-generation GPU scheduling approach in Kubernetes."})," Dynamic Resource Allocation (DRA) provides advanced GPU management capabilities beyond traditional device plugins. Here's what matters:"]}),(0,t.jsx)(n.h3,{id:"dra-advantages-over-traditional-gpu-scheduling",children:"DRA Advantages over Traditional GPU Scheduling"}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"\ud83c\udfaf Fine-grained resource control"})," \u2013 Request specific GPU memory amounts, not just whole devices"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"\ud83d\udd04 Per-workload sharing strategies"})," \u2013 Choose ",(0,t.jsx)(n.code,{children:"mps"}),", ",(0,t.jsx)(n.code,{children:"time-slicing"}),", ",(0,t.jsx)(n.code,{children:"mig"}),", or ",(0,t.jsx)(n.code,{children:"exclusive"})," per pod, not cluster-wide"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"\ud83e\udde0 Topology-aware scheduling"})," \u2013 Understands ",(0,t.jsx)(n.a,{href:"https://www.nvidia.com/en-us/data-center/nvlink/",children:"NVLink"}),", ",(0,t.jsx)(n.a,{href:"https://docs.nvidia.com/multi-node-nvlink-systems/imex-guide/overview.html",children:"IMEX"}),", and GPU interconnects for multi-GPU workloads"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"\u26a1 Advanced GPU features"})," \u2013 Required for ",(0,t.jsx)(n.a,{href:"https://aws.amazon.com/ec2/instance-types/p6/",children:"Amazon EC2 P6e-GB200 UltraServers"})," ",(0,t.jsx)(n.a,{href:"https://docs.nvidia.com/multi-node-nvlink-systems/imex-guide/overview.html",children:"IMEX"}),", Multi-Node ",(0,t.jsx)(n.a,{href:"https://www.nvidia.com/en-us/data-center/nvlink/",children:"NVLink"}),", and next-gen GPU capabilities"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"\ud83e\udd1d Coexistence-friendly"})," \u2013 Can run alongside traditional device plugins during transition"]}),"\n"]}),(0,t.jsx)(R.A,{type:"warning",title:"Amazon EC2 P6e-GB200 UltraServer Requirement",children:(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Traditional scheduling unsupported"})," \u2013 Amazon EC2 P6e-GB200 UltraServers ",(0,t.jsx)(n.strong,{children:"require DRA"})," and won't work with NVIDIA device plugin + kube-scheduler"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"DRA mandatory"})," \u2013 Multi-Node ",(0,t.jsx)(n.a,{href:"https://www.nvidia.com/en-us/data-center/nvlink/",children:"NVLink"})," and ",(0,t.jsx)(n.a,{href:"https://docs.nvidia.com/multi-node-nvlink-systems/imex-guide/overview.html",children:"IMEX"})," capabilities only available through DRA"]}),"\n"]})}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Key Implementation Details:"})}),(0,t.jsx)(N,{badges:[{type:"production",icon:"\u2638\ufe0f",title:"EKS Control Plane",value:"v1.33+",note:"DRA feature gates enabled"},{type:"production",icon:"\ud83d\udda5\ufe0f",title:"EKS Optimized NVIDIA AMI",value:"Latest AMI",note:"Pre-installed drivers"},{type:"production",icon:"\ud83d\udd17",title:"Managed Node Groups",value:"Full DRA Support",note:"Recommended approach"},{type:"info",icon:"\ud83d\udd27",title:"Self-Managed Nodegroups",value:"DRA Support",note:"Manual configuration"},{type:"production",icon:"\ud83d\udee0\ufe0f",title:"NVIDIA GPU Operator",value:"v25.3.0+",note:"Required for DRA"},{type:"production",icon:"\u26a1",title:"NVIDIA DRA Driver",value:"v25.3.0+",note:"Core DRA functionality"},{type:"warning",icon:"\ud83d\udea7",title:"Karpenter DRA Support",value:"In Development",note:"GitHub Issue #1231"},{type:"beta",icon:"\ud83d\udd2c",title:"DRA Status",value:"Beta (K8s v1.32+)",note:"Technology Preview"}]}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"EKS v1.33"})," \u2013 DRA feature gates enabled in EKS-optimized configurations"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"For detailed DRA implementation"})," \u2013 See ",(0,t.jsx)(n.a,{href:"https://kubernetes.io/docs/concepts/scheduling-eviction/dynamic-resource-allocation/",children:"Kubernetes DRA documentation"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Node provisioning compatibility:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Managed Node Groups"})," \u2013 Full DRA support \ud83c\udfaf"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Self-Managed Node Groups"})," \u2013 DRA support (requires manual configuration) \ud83d\udd27"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Karpenter"})," \u2013 DRA support in development (",(0,t.jsx)(n.a,{href:"https://github.com/kubernetes-sigs/karpenter/issues/1231",children:"Issue #1231"}),") \ud83c\udfd7\ufe0f"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Coexistence"})," \u2013 Traditional device plugin and DRA can run simultaneously"]}),"\n"]}),(0,t.jsx)(n.h3,{id:"why-managedself-managed-node-groups-vs-karpenter-for-dra",children:"Why Managed/Self-Managed Node Groups vs Karpenter for DRA?"}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Managed/Self-Managed Node Groups"})," \u2013 Full DRA support, optimized for Capacity Block Reservations"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Karpenter"})," \u2013 DRA support in development, dynamic scaling conflicts with reserved GPU capacity"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"EKS-optimized AMIs"})," \u2013 Come with pre-installed NVIDIA drivers"]}),"\n"]}),(0,t.jsx)(n.h3,{id:"can-i-use-both-traditional-gpu-allocation-and-dra-together",children:"Can I Use Both Traditional GPU Allocation and DRA Together?"}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Coexistence supported"})," \u2013 Both can run simultaneously on the same cluster"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"DRA is the future"})," \u2013 NVIDIA and Kubernetes moving exclusively to DRA"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Migration strategy"})," \u2013 Use DRA for new workloads, traditional for existing production"]}),"\n"]}),(0,t.jsx)(n.h3,{id:"production-readiness",children:"Production Readiness"}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Technology Preview"})," \u2013 GPU allocation and sharing features actively developed by NVIDIA"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Production Ready"})," \u2013 ComputeDomains for Multi-Node ",(0,t.jsx)(n.a,{href:"https://www.nvidia.com/en-us/data-center/nvlink/",children:"NVLink"})," fully supported"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Scheduling overhead"})," \u2013 Additional latency due to claim resolution process"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"General Availability"})," \u2013 Expected in Kubernetes v1.34 (2025)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Latest status updates"})," \u2013 Follow ",(0,t.jsx)(n.a,{href:"https://github.com/NVIDIA/k8s-dra-driver-gpu",children:"NVIDIA DRA Driver GitHub"})," for current development progress"]}),"\n"]}),(0,t.jsx)(R.A,{type:"tip",title:"Additional Resources",children:(0,t.jsxs)(n.p,{children:["For comprehensive guidance on AI/ML workloads on EKS, see the ",(0,t.jsx)(n.a,{href:"https://docs.aws.amazon.com/eks/latest/best-practices/aiml-compute.html#aiml-dra",children:"AWS EKS Best Practices for AI/ML Compute"}),"."]})})]}),"\n",(0,t.jsx)(I,{icon:"\ud83d\udcb8",title:"Enterprise GPU Utilization Crisis",statNumber:"60%",statLabel:"GPU capacity wasted",description:"Despite high demand, enterprise AI platforms consistently waste over half their GPU resources due to scheduling limitations. This represents millions in infrastructure costs.",type:"critical"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Even in high-demand AI clusters, GPU utilization frequently remains below 40%."})," This isn't a configuration issue \u2014 it's a fundamental limitation of how Kubernetes abstracts GPU resources. Organizations are paying premium prices for GPU instances while letting the majority of compute power sit idle."]}),"\n",(0,t.jsx)(_,{icon:"\ud83c\udf9b\ufe0f"}),"\n",(0,t.jsx)(n.h2,{id:"the-gpu-scheduling-challenge-in-kubernetes",children:"The GPU Scheduling Challenge in Kubernetes"}),"\n",(0,t.jsx)(n.h3,{id:"current-state-traditional-gpu-allocation",children:"Current State: Traditional GPU Allocation"}),"\n",(0,t.jsx)(n.p,{children:"Kubernetes has rapidly evolved into the de facto standard for orchestrating AI/ML workloads across enterprise environments, with Amazon EKS emerging as the leading platform for managing GPU-accelerated infrastructure at scale. Organizations are running everything from small inference services to massive distributed training jobs on EKS clusters, leveraging GPU instances like P4d, P5, and the latest P6 series to power their machine learning pipelines."}),"\n",(0,t.jsx)(n.p,{children:"However, despite Kubernetes' sophistication in managing containerized workloads, the traditional GPU scheduling model remains surprisingly primitive and creates significant operational challenges. The current approach treats GPUs as simple, atomic resources that can only be allocated in whole units, fundamentally mismatched with the diverse and evolving needs of modern AI workloads."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"How Traditional GPU Scheduling Works:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Pods request GPUs using simple integer values: ",(0,t.jsx)(n.code,{children:"nvidia.com/gpu: 1"})]}),"\n",(0,t.jsx)(n.li,{children:"Scheduler treats GPUs as opaque, indivisible resources"}),"\n",(0,t.jsx)(n.li,{children:"Each workload gets exclusive access to entire GPU devices"}),"\n",(0,t.jsx)(n.li,{children:"No awareness of actual resource requirements or GPU topology"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"The Problem with This Approach:"}),"\nModern AI workloads have diverse requirements that don't fit this binary model:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Small inference jobs"})," need only 2-4GB GPU memory but get allocated entire 80GB A100s"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Large training jobs"})," require coordinated multi-GPU communication via ",(0,t.jsx)(n.a,{href:"https://www.nvidia.com/en-us/data-center/nvlink/",children:"NVLink"})," or ",(0,t.jsx)(n.a,{href:"https://docs.nvidia.com/multi-node-nvlink-systems/imex-guide/overview.html",children:"IMEX"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Mixed workloads"})," could share GPUs efficiently but are forced into separate devices"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"the-gpu-utilization-crisis",children:"The GPU Utilization Crisis"}),"\n",(0,t.jsx)(R.A,{type:"warning",title:"Critical Inefficiency in Production",children:(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Even in high-demand clusters, GPU utilization frequently remains below 40%."})," This isn't a configuration issue: it's a fundamental limitation of how Kubernetes abstracts GPU resources."]})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Common symptoms of inefficient GPU allocation:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Queue starvation"})," - Small inference jobs wait behind long-running training tasks"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Resource fragmentation"})," - GPU memory is stranded in unusable chunks across nodes"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Topology blindness"})," - Multi-GPU jobs get suboptimal placement, degrading ",(0,t.jsx)(n.a,{href:"https://www.nvidia.com/en-us/data-center/nvlink/",children:"NVLink"})," performance"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cost explosion"})," - Organizations overprovision GPUs to work around scheduling inefficiencies"]}),"\n"]}),"\n",(0,t.jsx)(_,{icon:"\ud83d\udc8e"}),"\n",(0,t.jsx)(n.h2,{id:"enter-dynamic-resource-allocation-dra",children:"Enter Dynamic Resource Allocation (DRA)"}),"\n",(0,t.jsx)(n.h3,{id:"what-dra-changes",children:"What DRA Changes"}),"\n",(0,t.jsx)(n.p,{children:"Dynamic Resource Allocation fundamentally transforms GPU scheduling in Kubernetes from a rigid, device-centric model to a flexible, workload-aware approach:"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Traditional Approach:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"resources:\n  limits:\n    nvidia.com/gpu: 1  # Get entire GPU, no customization\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"DRA Approach:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"resourceClaims:\n- name: gpu-claim\n  source:\n    resourceClaimTemplateName: gpu-template  # Detailed requirements\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.em,{children:"See examples section below for ResourceClaimTemplate configurations."})}),"\n",(0,t.jsx)(R.A,{type:"warning",title:"Namespace Requirement",children:(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Critical:"})," ResourceClaims must exist in the same namespace as the Pods that reference them. Cross-namespace resource claims are not supported."]})}),"\n",(0,t.jsx)(n.h3,{id:"key-dra-innovations",children:"Key DRA Innovations"}),"\n",(0,t.jsxs)("div",{className:"dra-innovations-grid",children:[(0,t.jsxs)("div",{className:"innovation-card innovation-card--primary",children:[(0,t.jsxs)("div",{className:"innovation-card__header",children:[(0,t.jsx)("div",{className:"innovation-card__icon",children:"\ud83c\udfaf"}),(0,t.jsx)("h4",{children:"Fine-grained Resource Control"})]}),(0,t.jsxs)("div",{className:"innovation-card__content",children:[(0,t.jsxs)("ul",{className:"innovation-card__features",children:[(0,t.jsx)("li",{children:"Request specific GPU memory amounts (e.g., 16Gi out of 80Gi available)"}),(0,t.jsx)("li",{children:"Specify compute requirements independent of memory needs"}),(0,t.jsx)("li",{children:"Define topology constraints for multi-GPU workloads"})]}),(0,t.jsx)("div",{className:"innovation-card__note",children:(0,t.jsxs)(n.p,{children:[(0,t.jsx)("strong",{children:"Note:"})," ResourceClaims and Pods must be in the same namespace"]})})]})]}),(0,t.jsxs)("div",{className:"innovation-card innovation-card--secondary",children:[(0,t.jsxs)("div",{className:"innovation-card__header",children:[(0,t.jsx)("div",{className:"innovation-card__icon",children:"\ud83d\udd04"}),(0,t.jsx)("h4",{children:"Per-Workload Sharing Strategies"})]}),(0,t.jsx)("div",{className:"innovation-card__content",children:(0,t.jsxs)("div",{className:"strategy-grid",children:[(0,t.jsx)("div",{className:"strategy-item",children:(0,t.jsxs)(n.p,{children:[(0,t.jsx)("strong",{children:"MPS"})," - Concurrent small workloads with memory isolation"]})}),(0,t.jsx)("div",{className:"strategy-item",children:(0,t.jsxs)(n.p,{children:[(0,t.jsx)("strong",{children:"Time-slicing"})," - Workloads with different peak usage patterns"]})}),(0,t.jsx)("div",{className:"strategy-item",children:(0,t.jsxs)(n.p,{children:[(0,t.jsx)("strong",{children:"MIG"})," - Hardware-level isolation in multi-tenant environments"]})}),(0,t.jsx)("div",{className:"strategy-item",children:(0,t.jsxs)(n.p,{children:[(0,t.jsx)("strong",{children:"Exclusive"})," - Performance-critical training jobs"]})})]})})]}),(0,t.jsxs)("div",{className:"innovation-card innovation-card--success",children:[(0,t.jsxs)("div",{className:"innovation-card__header",children:[(0,t.jsx)("div",{className:"innovation-card__icon",children:"\ud83c\udf10"}),(0,t.jsx)("h4",{children:"Topology-Aware Scheduling"})]}),(0,t.jsx)("div",{className:"innovation-card__content",children:(0,t.jsxs)("ul",{className:"innovation-card__features",children:[(0,t.jsxs)("li",{children:["Understands ",(0,t.jsx)("a",{href:"https://www.nvidia.com/en-us/data-center/nvlink/",children:"NVLink"})," connections between GPUs"]}),(0,t.jsxs)("li",{children:["Leverages ",(0,t.jsx)("a",{href:"https://docs.nvidia.com/multi-node-nvlink-systems/imex-guide/overview.html",children:"IMEX"})," for ",(0,t.jsx)("a",{href:"https://aws.amazon.com/ec2/instance-types/p6/",children:"Amazon EC2 P6e-GB200 UltraServer"})," clusters"]}),(0,t.jsx)("li",{children:"Optimizes placement for distributed training workloads"})]})})]}),(0,t.jsxs)("div",{className:"innovation-card innovation-card--warning",children:[(0,t.jsxs)("div",{className:"innovation-card__header",children:[(0,t.jsx)("div",{className:"innovation-card__icon",children:"\ud83d\ude80"}),(0,t.jsx)("h4",{children:"Future-Proof Architecture"})]}),(0,t.jsx)("div",{className:"innovation-card__content",children:(0,t.jsxs)("ul",{className:"innovation-card__features",children:[(0,t.jsxs)("li",{children:["Required for next-generation systems like ",(0,t.jsx)("a",{href:"https://aws.amazon.com/ec2/instance-types/p6/",children:"Amazon EC2 P6e-GB200 UltraServers"})]}),(0,t.jsxs)("li",{children:["Enables advanced features like Multi-Node ",(0,t.jsx)("a",{href:"https://www.nvidia.com/en-us/data-center/nvlink/",children:"NVLink"})]}),(0,t.jsx)("li",{children:"Supports emerging GPU architectures and sharing technologies"})]})})]})]}),"\n",(0,t.jsx)("style",{jsx:!0,children:"\n.dra-innovations-grid {\ndisplay: grid;\ngrid-template-columns: repeat(auto-fit, minmax(320px, 1fr));\ngap: 1.5rem;\nmargin: 2rem 0;\n}\n\n.innovation-card {\nbackground: var(--ifm-background-surface-color);\nborder: 1px solid var(--ifm-color-emphasis-300);\nborder-radius: 12px;\npadding: 1.5rem;\ntransition: all 0.3s ease;\nbox-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);\n}\n\n.innovation-card:hover {\ntransform: translateY(-4px);\nbox-shadow: 0 8px 25px rgba(0, 0, 0, 0.15);\n}\n\n.innovation-card--primary {\nborder-left: 4px solid var(--ifm-color-primary);\n}\n\n.innovation-card--secondary {\nborder-left: 4px solid var(--ifm-color-secondary);\n}\n\n.innovation-card--success {\nborder-left: 4px solid var(--ifm-color-success);\n}\n\n.innovation-card--warning {\nborder-left: 4px solid var(--ifm-color-warning);\n}\n\n.innovation-card__header {\ndisplay: flex;\nalign-items: center;\nmargin-bottom: 1rem;\n}\n\n.innovation-card__icon {\nfont-size: 2rem;\nmargin-right: 0.75rem;\n}\n\n.innovation-card__header h4 {\nmargin: 0;\nfont-size: 1.25rem;\nfont-weight: 600;\n}\n\n.innovation-card__content {\ncolor: var(--ifm-color-content-secondary);\n}\n\n.innovation-card__features {\nmargin: 0;\npadding-left: 1rem;\n}\n\n.innovation-card__features li {\nmargin-bottom: 0.5rem;\n}\n\n.innovation-card__note {\nbackground: var(--ifm-color-warning-contrast-background);\nborder: 1px solid var(--ifm-color-warning-contrast-border);\nborder-radius: 6px;\npadding: 0.75rem;\nmargin-top: 1rem;\nfont-size: 0.875rem;\n}\n\n.strategy-grid {\ndisplay: grid;\ngrid-template-columns: 1fr 1fr;\ngap: 0.75rem;\n}\n\n.strategy-item {\nbackground: var(--ifm-color-emphasis-100);\npadding: 0.75rem;\nborder-radius: 6px;\nfont-size: 0.875rem;\nborder-left: 3px solid var(--ifm-color-secondary);\n}\n\n@media (max-width: 768px) {\n.dra-innovations-grid {\n  grid-template-columns: 1fr;\n}\n\n.strategy-grid {\n  grid-template-columns: 1fr;\n}\n}\n"}),"\n",(0,t.jsx)(n.h3,{id:"understanding-imex-computedomains-and-amazon-ec2-p6e-gb200-multi-node-scheduling",children:"Understanding IMEX, ComputeDomains, and Amazon EC2 P6e-GB200 Multi-Node Scheduling"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsxs)(n.strong,{children:[(0,t.jsx)(n.a,{href:"https://docs.nvidia.com/multi-node-nvlink-systems/imex-guide/overview.html",children:"IMEX"})," (NVIDIA Internode Memory Exchange/Management Service)"]})," is NVIDIA's orchestration service for GPU memory sharing across ",(0,t.jsx)(n.a,{href:"https://www.nvidia.com/en-us/data-center/nvlink/",children:"NVLink"})," multi-node deployments. In ",(0,t.jsx)(n.a,{href:"https://aws.amazon.com/ec2/instance-types/p6/",children:"Amazon EC2 P6e-GB200 UltraServer"})," configurations, IMEX coordinates memory export and import operations between nodes, enabling direct GPU-to-GPU memory access across multiple compute nodes for massive AI model training with billions of parameters."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"ComputeDomains"})," represent logical groupings of interconnected GPUs that can communicate efficiently through high-bandwidth connections like ",(0,t.jsx)(n.a,{href:"https://www.nvidia.com/en-us/data-center/nvlink/",children:"NVLink"})," or ",(0,t.jsx)(n.a,{href:"https://docs.nvidia.com/multi-node-nvlink-systems/imex-guide/overview.html",children:"IMEX"}),". DRA uses ComputeDomains to understand GPU topology and ensure workloads requiring multi-GPU coordination are scheduled on appropriately connected hardware."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Amazon EC2 P6e-GB200 Multi-Node Scheduling"})," leverages DRA's topology awareness to coordinate workloads across multiple superchip nodes. Traditional GPU scheduling cannot understand these complex interconnect relationships, making DRA essential for optimal placement of distributed training jobs on ",(0,t.jsx)(n.a,{href:"https://aws.amazon.com/ec2/instance-types/p6/",children:"Amazon EC2 P6e-GB200 UltraServer"})," systems where proper GPU topology selection directly impacts training performance."]}),"\n",(0,t.jsxs)(n.p,{children:["For detailed configuration examples and implementation guidance, see the ",(0,t.jsx)(n.a,{href:"https://docs.aws.amazon.com/eks/latest/best-practices/aiml-compute.html#aiml-dra",children:"AWS EKS AI/ML Best Practices documentation"}),"."]}),"\n",(0,t.jsx)(n.h2,{id:"implementation-considerations-for-eks",children:"Implementation Considerations for EKS"}),"\n",(0,t.jsx)(n.p,{children:"Now that we understand DRA's capabilities and advanced features like IMEX and ComputeDomains, let's explore the practical considerations for implementing DRA on Amazon EKS. The following sections address key decisions around node provisioning, migration strategies, and EKS-specific configurations that will determine your DRA deployment success."}),"\n",(0,t.jsx)(n.h3,{id:"managed-node-groups-vs-karpenter-for-p-series-gpu-instances-and-dra",children:"Managed Node Groups vs Karpenter for P-Series GPU Instances and DRA"}),"\n",(0,t.jsxs)(n.p,{children:["The choice between node provisioning methods for DRA isn't just about technical compatibility. It's fundamentally about how GPU capacity is purchased and utilized in enterprise AI workloads. ",(0,t.jsx)(n.strong,{children:"Managed and Self-Managed Node Groups are currently the recommended approach for DRA because they align with the economics and operational patterns of high-end GPU instances."})]}),"\n",(0,t.jsxs)(n.p,{children:["Here's why: The majority of large GPU instances (",(0,t.jsx)(n.a,{href:"https://aws.amazon.com/ec2/instance-types/p4/",children:"P4d"})," (A100), ",(0,t.jsx)(n.a,{href:"https://aws.amazon.com/ec2/instance-types/p5/",children:"P5"})," (H100), ",(0,t.jsx)(n.a,{href:"https://aws.amazon.com/ec2/instance-types/p6/",children:"P6 with B200"}),", and ",(0,t.jsx)(n.a,{href:"https://www.nvidia.com/en-us/data-center/gb200-nvl72/",children:"P6e with GB200"}),") are primarily available through AWS Capacity Block Reservations rather than on-demand pricing. ",(0,t.jsx)(n.strong,{children:"When organizations purchase Capacity Blocks, they commit to paying for every second of GPU time until the reservation expires, regardless of whether the GPUs are actively utilized."})," This creates a fundamental mismatch with Karpenter's core value proposition of dynamic scaling based on workload demand. Spinning nodes down during low-demand periods doesn't save money. It actually wastes the reserved capacity you're already paying for."]}),"\n",(0,t.jsxs)(n.p,{children:["Additionally, ",(0,t.jsx)(n.strong,{children:"Karpenter doesn't yet support DRA scheduling"})," (",(0,t.jsx)(n.a,{href:"https://github.com/kubernetes-sigs/karpenter/issues/1231",children:"Issue #1231"})," tracks active development), making it incompatible with production DRA workloads. While Karpenter excels at cost optimization through dynamic scaling for general compute workloads, ",(0,t.jsx)(n.strong,{children:'Capacity Block reservations require an "always-on" utilization strategy to maximize ROI'}),": exactly what Managed Node Groups provide with their static capacity model."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"The future picture is more optimistic:"})," Karpenter's roadmap includes static node features that would make it suitable for Capacity Block scenarios. The community is actively working on ",(0,t.jsx)(n.a,{href:"https://github.com/kubernetes-sigs/karpenter/issues/749",children:"manual node provisioning without workloads"})," and static provisioning capabilities through RFCs like ",(0,t.jsx)(n.a,{href:"https://github.com/kubernetes-sigs/karpenter/pull/2309",children:"static provisioning"})," and ",(0,t.jsx)(n.a,{href:"https://github.com/kubernetes-sigs/karpenter/pull/2397",children:"manual node provisioning"}),". Once DRA support is added alongside these static provisioning capabilities, Karpenter could become the preferred choice for DRA workloads with Capacity Block ML reserved instances. Until then, ",(0,t.jsx)(n.strong,{children:"Managed Node Groups with EKS-optimized AMIs (which come with pre-installed NVIDIA drivers) provide the most reliable foundation for DRA implementations."})]}),"\n",(0,t.jsx)(n.h3,{id:"dra-and-traditional-gpu-allocation-coexistence",children:"DRA and Traditional GPU Allocation Coexistence"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Yes, but with careful configuration to avoid conflicts."})," DRA and traditional GPU allocation can coexist on the same cluster, but this requires thoughtful setup to prevent resource double-allocation issues. NVIDIA's DRA driver is designed as an additional component alongside the GPU Operator, with selective enablement to avoid conflicts."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"The recommended approach for gradual migration:"})," Configure the NVIDIA DRA driver to enable only specific subsystems initially. For example, you can set ",(0,t.jsx)(n.code,{children:"resources.gpus.enabled=false"})," to use traditional device plugins for GPU allocation while enabling DRA's ComputeDomain subsystem for Multi-Node NVLink capabilities. This allows teams to gain operational experience with DRA's advanced features without risking established GPU allocation workflows."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Key considerations for coexistence:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Avoid same-device conflicts"}),": DRA and device plugins should not manage the same GPU devices simultaneously"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Selective component enablement"}),": Use NVIDIA DRA driver's modular design to enable features gradually"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Node selector management"}),": Configure node selectors carefully to prevent resource allocation conflicts"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Technology Preview status"}),": GPU allocation and sharing features are in Technology Preview (check ",(0,t.jsx)(n.a,{href:"https://github.com/NVIDIA/k8s-dra-driver-gpu",children:"NVIDIA DRA Driver GitHub"})," for updates)"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"For migration planning,"})," start with DRA's production-ready features like ComputeDomains for Multi-Node ",(0,t.jsx)(n.a,{href:"https://www.nvidia.com/en-us/data-center/nvlink/",children:"NVLink"}),", while keeping traditional device plugins for core GPU allocation. Once DRA's GPU allocation reaches full support, gradually migrate workloads starting with development and inference services before moving mission-critical training jobs. ",(0,t.jsx)(n.strong,{children:"NVIDIA and the Kubernetes community have designed DRA as the eventual replacement for device plugins"}),", but the transition requires careful orchestration to maintain cluster stability."]}),"\n",(0,t.jsx)(n.h3,{id:"visual-comparison-traditional-vs-dra",children:"Visual Comparison: Traditional vs DRA"}),"\n",(0,t.jsx)(n.p,{children:"The diagram below illustrates how DRA fundamentally changes the scheduling flow:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Traditional Model"}),": The pod directly requests an entire GPU via the node resource model. Scheduling and allocation are static, with no room for partial usage or workload intent."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"DRA Model"}),": Pods express intent via templates; claims are dynamically generated and resolved with the help of a DRA-aware scheduler and device driver. Multiple workloads can share GPUs safely and efficiently, maximizing utilization."]}),"\n"]}),"\n",(0,t.jsx)(n.mermaid,{value:'flowchart TB\n\n%% --- Traditional Device Plugin Model ---\nsubgraph Traditional ["\ud83d\udd34 Traditional Device Plugin"]\n    direction TB\n    T_Spacer[" "]\n    T_Pod["\ud83d\ude80 Pod<br/>resources: nvidia.com/gpu: 1"]\n    T_Scheduler["\u26a1 Scheduler<br/>Finds Node with GPU"]\n    T_Node["\ud83d\udda5\ufe0f Node<br/>with Device Plugin"]\n    T_GPU["\ud83c\udfaf GPU<br/>\ud83d\udc94 Static Configuration<br/>Cluster-wide Policies"]\n\n    T_Spacer ~~~ T_Pod\n    T_Pod --\x3e T_Scheduler\n    T_Scheduler --\x3e T_Node\n    T_Node --\x3e T_GPU\nend\n\n%% --- DRA Model ---\nsubgraph DRA ["\ud83d\udfe2 Dynamic Resource Allocation"]\n    direction TB\n\n    %% Spacer for title\n    D_Spacer[" "]\n\n    %% User Intent\n    D_Pod1["\ud83d\ude80 Pod A<br/>Inference Workload"]\n    D_Pod2["\ud83d\ude80 Pod B<br/>Training Workload"]\n\n    %% Templates and Claims\n    D_Template["\ud83d\udcc4 ResourceClaimTemplate<br/>Defines GPU Requirements"]\n    D_RC1["\ud83d\udccb ResourceClaim A<br/>Generated for Pod A"]\n    D_RC2["\ud83d\udccb ResourceClaim B<br/>Generated for Pod B"]\n\n    %% Core Components\n    D_DeviceClass["\ud83c\udff7\ufe0f DeviceClass<br/>gpu.nvidia.com"]\n    D_ResourceSlice["\ud83d\udcca ResourceSlice<br/>Advertises Available GPUs"]\n    D_Scheduler["\u26a1 DRA-Aware Scheduler<br/>Coordinates Allocation"]\n    D_Driver["\ud83c\udf9b\ufe0f NVIDIA DRA Driver<br/>Manages GPU Sharing"]\n    D_GPU["\ud83c\udfaf GPU<br/>\ud83d\udc9a Dynamic Per-Workload<br/>Flexible Sharing"]\n\n    %% Flow with spacer\n    D_Spacer ~~~ D_Pod1\n    D_Spacer ~~~ D_Pod2\n    D_Pod1 --\x3e D_Template\n    D_Pod2 --\x3e D_Template\n    D_Template --\x3e D_RC1\n    D_Template --\x3e D_RC2\n\n    D_DeviceClass --\x3e D_ResourceSlice\n    D_RC1 --\x3e D_Scheduler\n    D_RC2 --\x3e D_Scheduler\n    D_ResourceSlice --\x3e D_Scheduler\n\n    D_Scheduler --\x3e D_Driver\n    D_Driver --\x3e D_GPU\nend\n\n%% --- Styling ---\nstyle Traditional fill:#ffe6e6,stroke:#d32f2f,stroke-width:3px,font-size:18px,font-weight:bold\nstyle DRA fill:#e6ffe6,stroke:#2e7d32,stroke-width:3px,font-size:18px,font-weight:bold\n\n%% Hide spacer nodes\nstyle T_Spacer fill:transparent,stroke:transparent,color:transparent\nstyle D_Spacer fill:transparent,stroke:transparent,color:transparent\n\nstyle T_Pod fill:#ffcdd2,stroke:#c62828,color:#000\nstyle T_Scheduler fill:#ffab91,stroke:#d84315,color:#000\nstyle T_Node fill:#f8bbd9,stroke:#ad1457,color:#000\nstyle T_GPU fill:#e1bee7,stroke:#7b1fa2,color:#000\n\nstyle D_Pod1 fill:#c8e6c9,stroke:#388e3c,color:#000\nstyle D_Pod2 fill:#c8e6c9,stroke:#2e7d32,color:#000\nstyle D_Template fill:#a5d6a7,stroke:#2e7d32,color:#000\nstyle D_RC1 fill:#81c784,stroke:#388e3c,color:#000\nstyle D_RC2 fill:#81c784,stroke:#388e3c,color:#000\nstyle D_DeviceClass fill:#b39ddb,stroke:#5e35b1,color:#000\nstyle D_ResourceSlice fill:#90caf9,stroke:#1976d2,color:#000\nstyle D_Scheduler fill:#80deea,stroke:#00acc1,color:#000\nstyle D_Driver fill:#80cbc4,stroke:#00695c,color:#000\nstyle D_GPU fill:#a5d6a7,stroke:#2e7d32,color:#000'}),"\n",(0,t.jsx)(n.h3,{id:"technical-capabilities-comparison",children:"Technical Capabilities Comparison"}),"\n",(0,t.jsx)(T,{capabilities:[{name:"Resource Request Model",traditional:{status:"none",icon:"\u274c",description:"Simple integers",code:"nvidia.com/gpu: 1"},dra:{status:"full",icon:"\u2705",description:"Structured claims via",code:"ResourceClaimTemplate"}},{name:"GPU Memory Specification",traditional:{status:"none",icon:"\u274c",description:"All-or-nothing allocation"},dra:{status:"full",icon:"\u2705",description:"Memory-based constraints and selectors"}},{name:"Sharing Configuration",traditional:{status:"limited",icon:"\u26a0\ufe0f",description:"Static cluster-wide ConfigMaps"},dra:{status:"full",icon:"\u2705",description:"Per-workload sharing strategies"}},{name:"Multi-GPU Topology Awareness",traditional:{status:"none",icon:"\u274c",description:"No topology coordination"},dra:{status:"full",icon:"\u2705",description:"DeviceClass selectors for NVLink, IMEX"}},{name:"Runtime Reconfiguration",traditional:{status:"none",icon:"\u274c",description:"Requires pod deletion and redeployment"},dra:{status:"full",icon:"\u2705",description:"Dynamic reallocation without restarts"}},{name:"MIG Support",traditional:{status:"limited",icon:"\u26a0\ufe0f",description:"Limited - static partitions, manual setup"},dra:{status:"full",icon:"\u2705",description:"Full MIG profiles via dynamic claims"}}]}),"\n",(0,t.jsx)(_,{icon:"\u2699\ufe0f"}),"\n",(0,t.jsx)(n.h2,{id:"how-dra-actually-works-the-complete-technical-flow",children:"How DRA Actually Works: The Complete Technical Flow"}),"\n",(0,t.jsxs)(n.p,{children:["Dynamic Resource Allocation (DRA) extends Kubernetes scheduling with a modular, pluggable mechanism for handling GPU and other device resources. Rather than allocating integer units of opaque hardware, DRA introduces ",(0,t.jsx)(n.code,{children:"ResourceClaims"}),", ",(0,t.jsx)(n.code,{children:"ResourceClaimTemplates"}),", ",(0,t.jsx)(n.code,{children:"DeviceClasses"}),", and ",(0,t.jsx)(n.code,{children:"ResourceSlices"})," to express, match, and provision device requirements at runtime."]}),"\n",(0,t.jsx)(n.h3,{id:"step-by-step-dra-workflow",children:"Step-by-step DRA Workflow"}),"\n",(0,t.jsx)(n.p,{children:"DRA fundamentally changes how Kubernetes manages GPU resources through sophisticated orchestration:"}),"\n",(0,t.jsx)(n.h4,{id:"1-resource-discovery-and-advertisement",children:"1. Resource Discovery and Advertisement"}),"\n",(0,t.jsxs)(n.p,{children:["When NVIDIA DRA driver starts, it discovers available GPUs on each node and creates ",(0,t.jsx)(n.strong,{children:"ResourceSlices"})," that advertise device capabilities to the Kubernetes API server."]}),"\n",(0,t.jsx)(n.h4,{id:"2-deviceclass-registration",children:"2. DeviceClass Registration"}),"\n",(0,t.jsxs)(n.p,{children:["The driver registers one or more ",(0,t.jsx)(n.code,{children:"DeviceClass"})," objects to logically group GPU resources:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"gpu.nvidia.com"}),": Standard GPU resources"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"mig.nvidia.com"}),": Multi-Instance GPU partitions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"compute-domain.nvidia.com"}),": Cross-node GPU coordination"]}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"3-resource-claim-creation",children:"3. Resource Claim Creation"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"ResourceClaimTemplates"})," generate individual ",(0,t.jsx)(n.strong,{children:"ResourceClaims"})," for each pod, specifying:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Specific GPU memory requirements"}),"\n",(0,t.jsx)(n.li,{children:"Sharing strategy (MPS, time-slicing, exclusive)"}),"\n",(0,t.jsx)(n.li,{children:"Driver versions and compute capabilities"}),"\n",(0,t.jsx)(n.li,{children:"Topology constraints for multi-GPU workloads"}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"4-intelligent-scheduling",children:"4. Intelligent Scheduling"}),"\n",(0,t.jsxs)(n.p,{children:["The DRA-aware scheduler evaluates pending ",(0,t.jsx)(n.code,{children:"ResourceClaims"})," and queries available ",(0,t.jsx)(n.code,{children:"ResourceSlices"})," across nodes:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Matches device properties and constraints using CEL expressions"}),"\n",(0,t.jsx)(n.li,{children:"Ensures sharing strategy compatibility with other running pods"}),"\n",(0,t.jsx)(n.li,{children:"Selects optimal nodes considering topology, availability, and policy"}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"5-dynamic-allocation",children:"5. Dynamic Allocation"}),"\n",(0,t.jsx)(n.p,{children:"On the selected node, the DRA driver:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Sets up device access for the container (e.g., mounts MIG instance or configures MPS)"}),"\n",(0,t.jsx)(n.li,{children:"Allocates shared vs. exclusive access as per claim configuration"}),"\n",(0,t.jsx)(n.li,{children:"Isolates GPU slices securely between concurrent workloads"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"deploying-the-solution",children:"Deploying the Solution"}),"\n",(0,t.jsxs)(s,{children:[(0,t.jsx)("summary",{children:(0,t.jsx)("strong",{children:"\ud83d\udc47 In this example, you will provision JARK Cluster on Amazon EKS with DRA support"})}),(0,t.jsx)(L,{steps:[{title:"Prerequisites",description:"Install required tools and dependencies"},{title:"Deploy",description:"Configure and run JARK stack installation"},{title:"Verify",description:"Test your DRA deployment and validate functionality"}],currentStep:0}),(0,t.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),(0,t.jsx)(n.p,{children:"Ensure that you have installed the following tools on your machine:"}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"https://aws.amazon.com/cli/",children:"AWS CLI"})})," - AWS Command Line Interface"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"kubectl"})," - Kubernetes command-line tool"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"terraform"})," - Infrastructure as Code tool"]}),"\n"]}),(0,t.jsx)(n.h3,{id:"deploy",children:"Deploy"}),(0,t.jsx)(n.h4,{id:"1-clone-the-repository",children:"1. Clone the repository:"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",metastring:'title="Clone the repository"',children:"git clone https://github.com/awslabs/ai-on-eks.git\n"})}),(0,t.jsx)(n.admonition,{title:"Authentication Profile",type:"info",children:(0,t.jsxs)(n.p,{children:["If you are using a profile for authentication, set your ",(0,t.jsx)(n.code,{children:'export AWS_PROFILE="<PROFILE_name>"'})," to the desired profile name"]})}),(0,t.jsx)(n.h4,{id:"2-review-and-customize-configurations",children:"2. Review and customize configurations:"}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Check available addons in ",(0,t.jsx)(n.code,{children:"infra/base/terraform/variables.tf"})]}),"\n",(0,t.jsxs)(n.li,{children:["Modify addon settings in ",(0,t.jsx)(n.code,{children:"infra/jark-stack/terraform/blueprint.tfvars"})," as needed"]}),"\n",(0,t.jsxs)(n.li,{children:["Update the AWS region in ",(0,t.jsx)(n.code,{children:"blueprint.tfvars"})]}),"\n"]}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Enable DRA Components:"})}),(0,t.jsxs)(n.p,{children:["In the ",(0,t.jsx)(n.a,{href:"https://github.com/awslabs/ai-on-eks/blob/main/infra/jark-stack/terraform/blueprint.tfvars",children:(0,t.jsx)(n.code,{children:"blueprint.tfvars"})})," file, uncomment the following lines:"]}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-hcl",metastring:'title="blueprint.tfvars" showLineNumbers',children:"enable_nvidia_dra_driver         = true\nenable_nvidia_gpu_operator       = true\n"})}),(0,t.jsxs)(n.admonition,{title:"Automated Setup",type:"tip",children:[(0,t.jsx)(n.p,{children:"The NVIDIA GPU Operator includes all necessary components:"}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"NVIDIA Device Plugin"}),"\n",(0,t.jsx)(n.li,{children:"DCGM Exporter"}),"\n",(0,t.jsx)(n.li,{children:"MIG Manager"}),"\n",(0,t.jsx)(n.li,{children:"GPU Feature Discovery"}),"\n",(0,t.jsx)(n.li,{children:"Node Feature Discovery"}),"\n"]}),(0,t.jsx)(n.p,{children:"The NVIDIA DRA Driver is deployed as a separate Helm chart parallel to the GPU Operator."})]}),(0,t.jsx)(L,{steps:[{title:"Prerequisites",description:"Install required tools and dependencies"},{title:"Deploy",description:"Configure and run JARK stack installation"},{title:"Verify",description:"Test your DRA deployment and validate functionality"}],currentStep:1}),(0,t.jsx)(n.h4,{id:"3-navigate-to-the-deployment-directory-and-run-the-install-script",children:"3. Navigate to the deployment directory and run the install script:"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",metastring:'title="Deploy JARK Stack with DRA"',children:"cd ai-on-eks/infra/jark-stack && chmod +x install.sh\n./install.sh\n"})}),(0,t.jsx)(n.p,{children:"This script will automatically provision and configure the following components:"}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Amazon EKS Cluster with DRA (Dynamic Resource Allocation) feature gates enabled."}),"\n",(0,t.jsx)(n.li,{children:"Two GPU-managed node groups using Amazon Linux 2023 GPU AMIs:"}),"\n",(0,t.jsx)(n.li,{children:"G6 Node Group: Intended for testing MPS and time-slicing strategies."}),"\n",(0,t.jsx)(n.li,{children:"P4d(e) Node Group: Intended for testing MIG-based GPU partitioning."}),"\n"]}),(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsx)(n.p,{children:"\u26a0\ufe0f Both node groups are initialized with zero nodes to avoid unnecessary cost."}),"\n"]}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["To test MPS/time-slicing, manually update the ",(0,t.jsx)(n.code,{children:"g6"})," node group\u2019s ",(0,t.jsx)(n.code,{children:"min_size"})," and ",(0,t.jsx)(n.code,{children:"desired_size"})," via the EKS console."]}),"\n",(0,t.jsxs)(n.li,{children:["To test MIG, you need at least one ",(0,t.jsx)(n.code,{children:"p4d"})," or ",(0,t.jsx)(n.code,{children:"p4de"})," instance, which requires a Capacity Block Reservation (CBR). Edit the file: ",(0,t.jsx)(n.code,{children:"infra/base/terraform/eks.tf"}),". Set your actual ",(0,t.jsx)(n.code,{children:"capacity_reservation_id"})," and change the ",(0,t.jsx)(n.code,{children:"min_size"})," for the MIG node group to ",(0,t.jsx)(n.code,{children:"1"})]}),"\n"]}),(0,t.jsx)(L,{steps:[{title:"Prerequisites",description:"Install required tools and dependencies"},{title:"Deploy",description:"Configure and run JARK stack installation"},{title:"Verify",description:"Test your DRA deployment and validate functionality"}],currentStep:2}),(0,t.jsx)(n.h4,{id:"4-verify-deployment",children:"4. Verify Deployment"}),(0,t.jsx)(n.p,{children:"Follow these verification steps to ensure your DRA deployment is working correctly:"}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Step 1: Configure kubectl access"})}),(0,t.jsx)(n.p,{children:"Update your local kubeconfig to access the Kubernetes cluster:"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"aws eks update-kubeconfig --name jark-stack  # Replace with your EKS cluster name\n"})}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Step 2: Verify worker nodes"})}),(0,t.jsx)(n.p,{children:"First, let's verify that worker nodes are running in the cluster:"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"kubectl get nodes\n"})}),(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Expected output:"})," You should see two x86 instances from the core node group, plus any GPU instances (g6, p4d, etc.) that you manually scaled up via the EKS console."]}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Step 3: Verify DRA components"})}),(0,t.jsx)(n.p,{children:"Run this command to verify all deployments, including the NVIDIA GPU Operator and NVIDIA DRA Driver:"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"kubectl get deployments -A\n"})}),(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Expected output:"})," All pods should be in ",(0,t.jsx)(n.code,{children:"Running"})," state before proceeding to test the examples below."]}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Instance compatibility for testing:"})}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Time-slicing and MPS"}),": Any G5 or G6 instance"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"MIG partitioning"}),": P-series instances (P4d or higher)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"IMEX use cases"}),": P6e-GB200 UltraServers"]}),"\n"]}),(0,t.jsx)(n.p,{children:"Once all components are running, you can start testing the various DRA examples mentioned in the following sections."})]}),"\n",(0,t.jsx)(n.h3,{id:"component-architecture",children:"Component Architecture"}),"\n",(0,t.jsx)(n.mermaid,{value:'graph TB\n    subgraph "JARK Stack Components"\n        A[EKS Cluster v1.33+]\n        B[NVIDIA GPU Operator v25.3.0+]\n        C[NVIDIA DRA Driver v25.3.0+]\n        D[Managed Node Groups / Self-Managed Node Groups with GPU Instances]\n        E[NVIDIA Device Plugin]\n        F[DCGM Exporter]\n        G[MIG Manager]\n        H[Node Feature Discovery]\n        I[GPU Feature Discovery]\n    end\n\n    A --\x3e D\n    D --\x3e B\n    D --\x3e C\n    B --\x3e E\n    B --\x3e F\n    B --\x3e G\n    B --\x3e H\n    B --\x3e I\n\n    style A fill:#e8f5e8\n    style B fill:#f3e5f5\n    style C fill:#e1f5fe\n    style D fill:#fff3e0\n    style E fill:#ffecb3\n    style F fill:#f1f8e9\n    style G fill:#fce4ec\n    style H fill:#e3f2fd\n    style I fill:#e1f5fe'}),"\n",(0,t.jsx)(n.admonition,{title:"NVIDIA Tools",type:"info",children:(0,t.jsx)(n.p,{children:"The NVIDIA DRA Driver runs as an independent Helm chart parallel to the NVIDIA GPU Operator, not as part of it. Both components work together to provide comprehensive GPU management capabilities."})}),"\n",(0,t.jsx)(_,{icon:"\ud83c\udfb2"}),"\n",(0,t.jsx)(n.h2,{id:"gpu-sharing-strategies-technical-deep-dive",children:"GPU Sharing Strategies: Technical Deep Dive"}),"\n",(0,t.jsx)(n.p,{children:"Understanding GPU sharing technologies is crucial for optimizing resource utilization. Each strategy provides different benefits and addresses specific use cases."}),"\n",(0,t.jsxs)("div",{className:"tabs-container",children:[(0,t.jsxs)(k,{groupId:"sharing-strategies",children:[(0,t.jsxs)(D,{value:"basic",label:"\ud83d\udc8e Basic Allocation",default:!0,children:[(0,t.jsx)(n.h3,{id:"basic-gpu-allocation",children:"Basic GPU Allocation"}),(0,t.jsx)(n.p,{children:"Standard GPU allocation without sharing - each workload gets exclusive access to a complete GPU. This is the traditional model that provides maximum performance isolation."}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"How to Deploy Basic Allocation:"})}),(0,t.jsxs)(k,{groupId:"basic-config",children:[(0,t.jsx)(D,{value:"template",label:"ResourceClaimTemplate",children:(0,t.jsx)(G.A,{language:"yaml",title:"basic-gpu-claim-template.yaml",showLineNumbers:!0,children:i(37981).A})}),(0,t.jsx)(D,{value:"pod",label:"Basic Pod",children:(0,t.jsx)(G.A,{language:"yaml",title:"basic-gpu-pod.yaml",showLineNumbers:!0,children:i(20245).A})})]}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Deploy the Example:"})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",metastring:'title="Deploy Basic GPU Allocation"',children:"kubectl apply -f basic-gpu-claim-template.yaml\nkubectl apply -f basic-gpu-pod.yaml\nkubectl get pods -n gpu-test1 -w\n"})}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Best For:"})}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Large model training requiring full GPU resources"}),"\n",(0,t.jsx)(n.li,{children:"Workloads that fully utilize GPU compute and memory"}),"\n",(0,t.jsx)(n.li,{children:"Applications requiring maximum performance isolation"}),"\n",(0,t.jsx)(n.li,{children:"Legacy applications not designed for GPU sharing"}),"\n"]})]}),(0,t.jsxs)(D,{value:"timeslicing",label:"\u231b Time-Slicing",children:[(0,t.jsx)(n.h3,{id:"what-is-time-slicing",children:"What is Time-Slicing?"}),(0,t.jsx)(n.p,{children:"Time-slicing is a GPU sharing mechanism where multiple workloads take turns using the GPU, with each getting exclusive access during their allocated time slice. This approach is similar to CPU time-sharing but applied to GPU resources."}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Technical Implementation:"})}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"The GPU scheduler allocates specific time windows (typically 1-10ms) to each workload"}),"\n",(0,t.jsx)(n.li,{children:"During a workload's time slice, it has complete access to GPU compute and memory"}),"\n",(0,t.jsx)(n.li,{children:"Context switching occurs between time slices, saving and restoring GPU state"}),"\n",(0,t.jsx)(n.li,{children:"No memory isolation between workloads - they share the same GPU memory space"}),"\n"]}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Key Characteristics:"})}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Temporal Isolation"}),": Workloads are isolated in time but not in memory"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Full GPU Access"}),": Each workload gets complete GPU resources during its slice"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Context Switching Overhead"}),": Small performance penalty for switching between workloads"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Flexible Allocation"}),": Time slice duration can be adjusted based on workload requirements"]}),"\n"]}),(0,t.jsx)(n.h3,{id:"how-to-deploy-time-slicing-with-dra",children:"How to Deploy Time-Slicing with DRA"}),(0,t.jsxs)(k,{groupId:"timeslicing-config",children:[(0,t.jsx)(D,{value:"template",label:"ResourceClaimTemplate",children:(0,t.jsx)(G.A,{language:"yaml",title:"timeslicing-claim-template.yaml",showLineNumbers:!0,children:i(25204).A})}),(0,t.jsx)(D,{value:"pod",label:"Pod Configuration",children:(0,t.jsx)(G.A,{language:"yaml",title:"timeslicing-pod.yaml",showLineNumbers:!0,children:i(21438).A})})]}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Deploy the Example:"})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",metastring:'title="Deploy Time-Slicing GPU Sharing"',children:"kubectl apply -f timeslicing-claim-template.yaml\nkubectl apply -f timeslicing-pod.yaml\nkubectl get pods -n timeslicing-gpu -w\n"})}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Best For:"})}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Inference workloads with sporadic GPU usage"}),"\n",(0,t.jsx)(n.li,{children:"Development and testing environments"}),"\n",(0,t.jsx)(n.li,{children:"Workloads with different peak usage times"}),"\n",(0,t.jsx)(n.li,{children:"Applications that don't require memory isolation"}),"\n"]}),(0,t.jsx)(n.admonition,{title:"Time-Slicing Limitations",type:"caution",children:(0,t.jsx)(n.p,{children:"No memory or fault isolation between workloads. One workload can affect others through memory exhaustion or GPU errors."})})]}),(0,t.jsxs)(D,{value:"mps",label:"\ud83c\udf0a Multi-Process Service (MPS)",children:[(0,t.jsx)(n.h3,{id:"what-is-mps",children:"What is MPS?"}),(0,t.jsx)(n.p,{children:"NVIDIA Multi-Process Service (MPS) is a GPU sharing technology that allows multiple CUDA applications to run concurrently on the same GPU by creating a daemon that manages GPU access and enables spatial sharing of GPU resources."}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Technical Implementation:"})}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"MPS daemon acts as a proxy between CUDA applications and the GPU driver"}),"\n",(0,t.jsx)(n.li,{children:"Each process gets dedicated GPU memory allocation"}),"\n",(0,t.jsx)(n.li,{children:"Compute kernels from different processes can execute simultaneously when resources allow"}),"\n",(0,t.jsx)(n.li,{children:"Memory isolation is maintained between processes"}),"\n",(0,t.jsx)(n.li,{children:"Hardware scheduling enables true parallel execution"}),"\n"]}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Key Characteristics:"})}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Spatial Isolation"}),": GPU compute units can be shared simultaneously"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Memory Isolation"}),": Each process has dedicated memory space"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Concurrent Execution"}),": Multiple kernels can run in parallel"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Lower Latency"}),": Reduced context switching compared to time-slicing"]}),"\n"]}),(0,t.jsx)(n.h3,{id:"how-to-deploy-mps-with-dra",children:"How to Deploy MPS with DRA"}),(0,t.jsxs)(k,{groupId:"mps-config",children:[(0,t.jsx)(D,{value:"template",label:"ResourceClaimTemplate",children:(0,t.jsx)(G.A,{language:"yaml",title:"mps-claim-template.yaml",showLineNumbers:!0,children:i(85596).A})}),(0,t.jsx)(D,{value:"pod",label:"Multi-Container Pod",children:(0,t.jsx)(G.A,{language:"yaml",title:"mps-pod.yaml",showLineNumbers:!0,children:i(10278).A})})]}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Deploy the Example:"})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",metastring:'title="Deploy MPS GPU Sharing"',children:"kubectl apply -f mps-claim-template.yaml\nkubectl apply -f mps-pod.yaml\nkubectl get pods -n mps-gpu -w\n"})}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Best For:"})}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Multiple small inference workloads"}),"\n",(0,t.jsx)(n.li,{children:"Concurrent model serving scenarios"}),"\n",(0,t.jsx)(n.li,{children:"Workloads using less than 50% of GPU compute"}),"\n",(0,t.jsx)(n.li,{children:"Applications requiring memory isolation"}),"\n"]}),(0,t.jsx)(n.admonition,{title:"MPS Performance Benefits",type:"tip",children:(0,t.jsx)(n.p,{children:"MPS eliminates context switching overhead and enables true parallelism. Ideal for workloads using less than 50% of GPU compute capacity."})})]}),(0,t.jsxs)(D,{value:"mig",label:"\ud83c\udfd7\ufe0f Multi-Instance GPU (MIG)",children:[(0,t.jsx)(n.h3,{id:"what-is-mig",children:"What is MIG?"}),(0,t.jsx)(n.p,{children:"Multi-Instance GPU (MIG) is a hardware-level GPU partitioning technology available on NVIDIA A100, H100, and newer GPUs that creates smaller, isolated GPU instances with dedicated compute units, memory, and memory bandwidth."}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Technical Implementation:"})}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Hardware-level partitioning creates separate GPU instances"}),"\n",(0,t.jsx)(n.li,{children:"Each MIG instance has dedicated streaming multiprocessors (SMs)"}),"\n",(0,t.jsx)(n.li,{children:"Memory and memory bandwidth are physically partitioned"}),"\n",(0,t.jsx)(n.li,{children:"Complete fault isolation between instances"}),"\n",(0,t.jsx)(n.li,{children:"Independent scheduling and execution contexts"}),"\n"]}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Key Characteristics:"})}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Hardware Isolation"}),": Physical separation of compute and memory resources"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Fault Isolation"}),": Issues in one instance don't affect others"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Predictable Performance"}),": Guaranteed resources for each instance"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Fixed Partitioning"}),": Predefined MIG profiles (1g.5gb, 2g.10gb, etc.)"]}),"\n"]}),(0,t.jsx)(n.h3,{id:"how-to-deploy-mig-with-dra",children:"How to Deploy MIG with DRA"}),(0,t.jsxs)(k,{groupId:"mig-config",children:[(0,t.jsx)(D,{value:"template",label:"ResourceClaimTemplate",children:(0,t.jsx)(G.A,{language:"yaml",title:"mig-claim-template.yaml",showLineNumbers:!0,children:i(74864).A})}),(0,t.jsx)(D,{value:"pod",label:"MIG Pod",children:(0,t.jsx)(G.A,{language:"yaml",title:"mig-pod.yaml",showLineNumbers:!0,children:i(47338).A})})]}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Deploy the Example:"})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",metastring:'title="Deploy MIG GPU Partitioning"',children:"kubectl apply -f mig-claim-template.yaml\nkubectl apply -f mig-pod.yaml\nkubectl get pods -n mig-gpu -w\n"})}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Best For:"})}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Multi-tenant environments requiring strict isolation"}),"\n",(0,t.jsx)(n.li,{children:"Predictable performance requirements"}),"\n",(0,t.jsx)(n.li,{children:"Production workloads requiring guaranteed resources"}),"\n",(0,t.jsx)(n.li,{children:"Compliance scenarios requiring hardware-level isolation"}),"\n"]}),(0,t.jsx)(n.admonition,{title:"MIG Requirements",type:"warning",children:(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Hardware-level partitioning creates isolated GPU instances"}),"\n",(0,t.jsx)(n.li,{children:"Each MIG instance has dedicated compute units and memory"}),"\n",(0,t.jsx)(n.li,{children:"Complete fault isolation between instances"}),"\n",(0,t.jsx)(n.li,{children:"Requires GPU Operator with MIG Manager for dynamic reconfiguration"}),"\n"]})})]})]}),(0,t.jsx)(n.h3,{id:"strategy-selection-guide",children:"Strategy Selection Guide"}),(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Workload Type"}),(0,t.jsx)(n.th,{children:"Recommended Strategy"}),(0,t.jsx)(n.th,{children:"Key Benefit"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Small Inference Jobs"})}),(0,t.jsx)(n.td,{children:"Time-slicing or MPS"}),(0,t.jsx)(n.td,{children:"Higher GPU utilization"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Concurrent Small Models"})}),(0,t.jsx)(n.td,{children:"MPS"}),(0,t.jsx)(n.td,{children:"True parallelism"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Production Multi-tenant"})}),(0,t.jsx)(n.td,{children:"MIG"}),(0,t.jsx)(n.td,{children:"Hardware isolation"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Large Model Training"})}),(0,t.jsx)(n.td,{children:"Basic Allocation"}),(0,t.jsx)(n.td,{children:"Maximum performance"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Development/Testing"})}),(0,t.jsx)(n.td,{children:"Time-slicing"}),(0,t.jsx)(n.td,{children:"Flexibility and simplicity"})]})]})]})]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"cleanup",children:"Cleanup"}),"\n",(0,t.jsx)(n.h3,{id:"removing-dra-components",children:"Removing DRA Components"}),"\n",(0,t.jsxs)(k,{groupId:"cleanup-steps",children:[(0,t.jsxs)(D,{value:"workloads",label:"1\ufe0f\u20e3 Clean Up DRA Examples",children:[(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Remove all DRA example workloads:"})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",metastring:'title="Clean up DRA workloads" showLineNumbers',children:"# Delete all pods first to ensure proper cleanup\nkubectl delete pod inference-pod-1 -n timeslicing-gpu --ignore-not-found\nkubectl delete pod training-pod-2 -n timeslicing-gpu --ignore-not-found\nkubectl delete pod mps-workload -n mps-gpu --ignore-not-found\nkubectl delete pod mig-workload -n mig-gpu --ignore-not-found\nkubectl delete pod basic-gpu-pod -n gpu-test1 --ignore-not-found\n\n# Delete ResourceClaimTemplates\nkubectl delete resourceclaimtemplate timeslicing-gpu-template -n timeslicing-gpu --ignore-not-found\nkubectl delete resourceclaimtemplate mps-gpu-template -n mps-gpu --ignore-not-found\nkubectl delete resourceclaimtemplate mig-gpu-template -n mig-gpu --ignore-not-found\nkubectl delete resourceclaimtemplate basic-gpu-template -n gpu-test1 --ignore-not-found\n\n# Delete any remaining ResourceClaims\nkubectl delete resourceclaims --all --all-namespaces --ignore-not-found\n\n# Delete ConfigMaps (contain scripts)\nkubectl delete configmap timeslicing-scripts-configmap -n timeslicing-gpu --ignore-not-found\n\n# Finally delete namespaces\nkubectl delete namespace timeslicing-gpu --ignore-not-found\nkubectl delete namespace mps-gpu --ignore-not-found\nkubectl delete namespace mig-gpu --ignore-not-found\nkubectl delete namespace gpu-test1 --ignore-not-found\n\n# Verify cleanup\nkubectl get resourceclaims --all-namespaces\nkubectl get resourceclaimtemplates --all-namespaces\n"})})]}),(0,t.jsxs)(D,{value:"jark-cleanup",label:"2\ufe0f\u20e3 JARK Stack Cleanup",children:[(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"For JARK-deployed clusters, use the automated cleanup:"})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",metastring:'title="JARK Stack Complete Cleanup"',children:"# Navigate to JARK directory\ncd ai-on-eks/infra/jark-stack/terraform/_LOCAL\n\n# Run the cleanup script\nchmod +x cleanup.sh\n./cleanup.sh\n\n# Alternative: Manual terraform destroy\n# terraform destroy -var-file=terraform/blueprint.tfvars -auto-approve\n"})}),(0,t.jsx)(n.admonition,{title:"Complete Infrastructure Removal",type:"warning",children:(0,t.jsx)(n.p,{children:"This will remove the entire EKS cluster and all associated resources. Ensure you have backed up any important data before proceeding."})})]})]}),"\n",(0,t.jsxs)(s,{children:[(0,t.jsx)("summary",{children:(0,t.jsx)("strong",{children:"\ud83d\udd27 Troubleshooting Common Issues"})}),(0,t.jsxs)(k,{groupId:"troubleshooting",children:[(0,t.jsxs)(D,{value:"pods-stuck",label:"\ud83d\udd0d Pods Stuck in Pending",children:[(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Issue:"})," Pods with ResourceClaims stuck in Pending state"]}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Diagnosis:"})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Check ResourceClaim status\nkubectl get resourceclaims --all-namespaces -o wide\n\n# Check DRA driver logs\nkubectl logs -n gpu-operator -l app=nvidia-dra-driver --tail=100\n\n# Verify DeviceClasses exist\nkubectl get deviceclasses\n"})}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Resolution:"})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Restart DRA driver pods\nkubectl delete pods -n gpu-operator -l app=nvidia-dra-driver\n\n# Check node GPU availability\nkubectl describe nodes | grep -A 10 "Allocatable"\n'})})]}),(0,t.jsxs)(D,{value:"sharing-conflicts",label:"\u26a0\ufe0f GPU Sharing Conflicts",children:[(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Issue:"})," Incompatible sharing strategies on same GPU"]}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Diagnosis:"})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Check ResourceSlice allocation\nkubectl get resourceslices -o yaml\n\n# Verify current allocations\nkubectl get resourceclaims --all-namespaces -o jsonpath='{range .items[*]}{.metadata.namespace}/{.metadata.name}: {.status.allocation.deviceResults[*].device}{\"\\n\"}{end}'\n"})}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Resolution:"})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Remove conflicting ResourceClaims\nkubectl delete resourceclaim <conflicting-claim> -n <namespace>\n\n# Wait for resource cleanup\nkubectl wait --for=delete resourceclaim <claim-name> -n <namespace> --timeout=60s\n"})})]}),(0,t.jsxs)(D,{value:"performance",label:"\ud83d\udcca Performance Issues",children:[(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Issue:"})," Suboptimal GPU utilization or performance"]}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Monitoring:"})}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Check GPU utilization\nkubectl exec -it <gpu-pod> -n <namespace> -- nvidia-smi\n\n# Monitor ResourceClaim allocation\nkubectl get events --field-selector reason=ResourceClaimAllocated --sort-by='.lastTimestamp'\n\n# Check sharing strategy effectiveness\nkubectl logs <workload-pod> -n <namespace> | grep -i gpu\n"})}),(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Optimization:"})}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Review sharing strategy selection (MPS vs time-slicing vs exclusive)"}),"\n",(0,t.jsx)(n.li,{children:"Validate workload resource requirements match allocation"}),"\n",(0,t.jsx)(n.li,{children:"Consider MIG partitioning for predictable isolation"}),"\n"]})]})]})]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,t.jsx)(n.p,{children:"Dynamic Resource Allocation represents a fundamental shift from rigid GPU allocation to intelligent, workload-aware resource management. By leveraging structured ResourceClaims and vendor-specific drivers, DRA unlocks the GPU utilization rates necessary for cost-effective AI/ML operations at enterprise scale."}),"\n",(0,t.jsx)(n.admonition,{title:"\ud83d\ude80 Ready to Transform Your GPU Infrastructure?",type:"tip",children:(0,t.jsx)(n.p,{children:"With the simplified JARK-based deployment approach, organizations can implement production-grade DRA capabilities in three steps, transforming their GPU infrastructure from a static resource pool into a dynamic, intelligent platform optimized for modern AI workloads."})}),"\n",(0,t.jsx)(n.p,{children:"The combination of EKS's managed infrastructure, NVIDIA's driver ecosystem, and Kubernetes' declarative model creates a powerful foundation for next-generation AI workloads - from small inference jobs to multi-node distributed training on GB200 superchips."})]})}function F(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(K,{...e})}):K(e)}},85596:(e,n,i)=>{i.d(n,{A:()=>s});const s='apiVersion: v1\nkind: Namespace\nmetadata:\n  name: mps-gpu\n---\napiVersion: resource.k8s.io/v1beta1\nkind: ResourceClaimTemplate\nmetadata:\n  name: mps-gpu-template\n  namespace: mps-gpu\nspec:\n  spec:\n    devices:\n      requests:\n      - name: shared-gpu\n        deviceClassName: gpu.nvidia.com\n      config:\n      - requests: ["shared-gpu"]\n        opaque:\n          driver: gpu.nvidia.com\n          parameters:\n            apiVersion: resource.nvidia.com/v1beta1\n            kind: GpuConfig\n            sharing:\n              strategy: MPS\n'}}]);