"use strict";(self.webpackChunkdoeks_website=self.webpackChunkdoeks_website||[]).push([[8867],{19750:(e,n,s)=>{s.d(n,{A:()=>r});const r=s.p+"assets/images/ray_on_kubernetes-5882d71319bfc6b44d7da61e25a0c122.webp"},28453:(e,n,s)=>{s.d(n,{R:()=>a,x:()=>o});var r=s(96540);const i={},t=r.createContext(i);function a(e){const n=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),r.createElement(t.Provider,{value:n},e.children)}},39918:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"infra/ai-ml/jark","title":"JARK on EKS","description":"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn\'t working, it\u2019s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren\'t initializing, check the logs for Karpenter or Node groups to resolve the issue.","source":"@site/docs/infra/ai-ml/jark.md","sourceDirName":"infra/ai-ml","slug":"/infra/ai-ml/jark","permalink":"/ai-on-eks/docs/infra/ai-ml/jark","draft":false,"unlisted":false,"editUrl":"https://github.com/awslabs/ai-on-eks/blob/main/website/docs/infra/ai-ml/jark.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_label":"JARK on EKS","sidebar_position":1},"sidebar":"infra","previous":{"title":"Introduction","permalink":"/ai-on-eks/docs/infra/ai-ml/"},"next":{"title":"AIBrix on EKS","permalink":"/ai-on-eks/docs/infra/ai-ml/aibrix"}}');var i=s(74848),t=s(28453),a=s(42450);const o={sidebar_label:"JARK on EKS",sidebar_position:1},l="JARK on EKS",c={},d=[{value:"What is JARK?",id:"what-is-jark",level:3},{value:"Key Features and Benefits",id:"key-features-and-benefits",level:3},{value:"Why Use JARK?",id:"why-use-jark",level:3},{value:"Ray on Kubernetes",id:"ray-on-kubernetes",level:3},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Deploy",id:"deploy",level:3}];function h(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",span:"span",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"jark-on-eks",children:"JARK on EKS"})}),"\n",(0,i.jsx)(n.admonition,{type:"warning",children:(0,i.jsx)(n.p,{children:"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn't working, it\u2019s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren't initializing, check the logs for Karpenter or Node groups to resolve the issue."})}),"\n",(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsxs)(n.p,{children:["These instructions only deploy the JARK cluster as a base. If you are looking to deploy specific models for inference or training, please refer to this ",(0,i.jsx)(n.a,{href:"https://awslabs.github.io/ai-on-eks/docs/blueprints",children:"AI"})," page for end-to-end instructions."]})}),"\n",(0,i.jsx)(n.h3,{id:"what-is-jark",children:"What is JARK?"}),"\n",(0,i.jsxs)(n.p,{children:["JARK is a powerful stack composed of ",(0,i.jsx)(n.a,{href:"https://jupyter.org/hub",children:"JupyterHub"}),", ",(0,i.jsx)(n.a,{href:"https://github.com/argoproj/argo-workflows",children:"Argo Workflows"}),", ",(0,i.jsx)(n.a,{href:"https://github.com/ray-project/ray",children:"Ray"}),", and ",(0,i.jsx)(n.a,{href:"https://kubernetes.io/",children:"Kubernetes"}),", designed to streamline the deployment and management of Generative AI models on Amazon EKS. This stack brings together some of the most effective tools in the AI and Kubernetes ecosystem, offering a robust solution for training, fine-tuning, and inference large AI models."]}),"\n",(0,i.jsxs)(n.p,{children:["JARK comes enabled with ",(0,i.jsx)(n.a,{href:"https://github.com/awslabs/ai-ml-observability-reference-architecture",children:"AI/ML Observability"}),". Please see the ",(0,i.jsx)(n.a,{href:"https://awslabs.github.io/ai-on-eks/docs/bestpractices/observability",children:"observability"})," section for details on the observability architecture."]}),"\n",(0,i.jsx)(n.h3,{id:"key-features-and-benefits",children:"Key Features and Benefits"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"https://jupyter.org/hub",children:"JupyterHub"}),": Provides a collaborative environment for running notebooks, crucial for model development and prompt engineering."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"https://github.com/argoproj/argo-workflows",children:"Argo Workflows"}),": Automates the entire AI model pipeline\u2014from data preparation to model deployment\u2014ensuring a consistent and efficient process."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"https://github.com/ray-project/ray",children:"Ray"}),": Scales AI model training and inference across multiple nodes, making it easier to handle large datasets and reduce training time."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"https://kubernetes.io/",children:"Kubernetes"}),": Powers the stack by providing the necessary orchestration to run, scale, and manage containerized AI models with high availability and resource efficiency."]}),"\n",(0,i.jsx)(n.h3,{id:"why-use-jark",children:"Why Use JARK?"}),"\n",(0,i.jsx)(n.p,{children:"The JARK stack is ideal for teams and organizations looking to simplify the complex process of deploying and managing AI models. Whether you're working on cutting-edge generative models or scaling existing AI workloads, JARK on Amazon EKS offers the flexibility, scalability, and control you need to succeed."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"alt text",src:s(76066).A+"",width:"879",height:"454"})}),"\n",(0,i.jsx)(n.h3,{id:"ray-on-kubernetes",children:"Ray on Kubernetes"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"https://www.ray.io/",children:"Ray"})," is an open-source framework for building scalable and distributed applications. It is designed to make it easy to write parallel and distributed Python applications by providing a simple and intuitive API for distributed computing. It has a growing community of users and contributors, and is actively maintained and developed by the Ray team at Anyscale, Inc."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"RayCluster",src:s(96388).A+"",width:"773",height:"388"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsxs)(n.em,{children:["Source: ",(0,i.jsx)(n.a,{href:"https://docs.ray.io/en/latest/cluster/key-concepts.html",children:"https://docs.ray.io/en/latest/cluster/key-concepts.html"})]})}),"\n",(0,i.jsxs)(n.p,{children:["To deploy Ray in production across multiple machines users must first deploy ",(0,i.jsx)(n.a,{href:"https://docs.ray.io/en/latest/cluster/getting-started.html",children:(0,i.jsx)(n.strong,{children:"Ray Cluster"})}),". A Ray Cluster consists of head nodes and worker nodes which can be autoscaled using the built-in ",(0,i.jsx)(n.strong,{children:"Ray Autoscaler"}),"."]}),"\n",(0,i.jsxs)(n.p,{children:["Deploying Ray Cluster on Kubernetes including on Amazon EKS is supported via the ",(0,i.jsx)(n.a,{href:"https://ray-project.github.io/kuberay/",children:(0,i.jsx)(n.strong,{children:"KubeRay Operator"})}),". The operator provides a Kubernetes-native way to manage Ray clusters. The installation of KubeRay Operator involves deploying the operator and the CRDs for ",(0,i.jsx)(n.code,{children:"RayCluster"}),", ",(0,i.jsx)(n.code,{children:"RayJob"})," and ",(0,i.jsx)(n.code,{children:"RayService"})," as documented ",(0,i.jsx)(n.a,{href:"https://ray-project.github.io/kuberay/deploy/helm/",children:"here"}),"."]}),"\n",(0,i.jsx)(n.p,{children:"Deploying Ray on Kubernetes can provide several benefits:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Scalability"}),": Kubernetes allows you to scale your Ray cluster up or down based on your workload requirements, making it easy to manage large-scale distributed applications."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Fault tolerance"}),": Kubernetes provides built-in mechanisms for handling node failures and ensuring high availability of your Ray cluster."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Resource allocation"}),": With Kubernetes, you can easily allocate and manage resources for your Ray workloads, ensuring that they have access to the necessary resources for optimal performance."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Portability"}),": By deploying Ray on Kubernetes, you can run your workloads across multiple clouds and on-premises data centers, making it easy to move your applications as needed."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Monitoring"}),": Kubernetes provides rich monitoring capabilities, including metrics and logging, making it easy to troubleshoot issues and optimize performance."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Overall, deploying Ray on Kubernetes can simplify the deployment and management of distributed applications, making it a popular choice for many organizations that need to run large-scale machine learning workloads."}),"\n",(0,i.jsxs)(n.p,{children:["Before moving forward with the deployment please make sure you have read the pertinent sections of the official ",(0,i.jsx)(n.a,{href:"https://docs.ray.io/en/latest/cluster/kubernetes/index.html",children:"documentation"}),"."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"RayonK8s",src:s(19750).A+"",width:"1708",height:"686"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsxs)(n.em,{children:["Source: ",(0,i.jsx)(n.a,{href:"https://docs.ray.io/en/latest/cluster/kubernetes/index.html",children:"https://docs.ray.io/en/latest/cluster/kubernetes/index.html"})]})}),"\n",(0,i.jsxs)(a.A,{header:(0,i.jsx)(n.h2,{children:(0,i.jsx)(n.span,{children:"Deploying the Solution"})}),children:[(0,i.jsxs)(n.p,{children:["In this ",(0,i.jsx)(n.a,{href:"https://github.com/awslabs/ai-on-eks/tree/main/infra/jark-stack/terraform",children:"example"}),", you will provision JARK Cluster on Amazon EKS."]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"JARK",src:s(72957).A+"",width:"879",height:"512"})}),(0,i.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),(0,i.jsx)(n.p,{children:"Ensure that you have installed the following tools on your machine."}),(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html",children:"aws cli"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://Kubernetes.io/docs/tasks/tools/",children:"kubectl"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://learn.hashicorp.com/tutorials/terraform/install-cli",children:"terraform"})}),"\n"]}),(0,i.jsx)(n.h3,{id:"deploy",children:"Deploy"}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"1. Clone the repository:"})}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"git clone https://github.com/awslabs/ai-on-eks.git\n"})}),(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsxs)(n.p,{children:["If you are using profile for authentication\nset your ",(0,i.jsx)(n.code,{children:'export AWS_PROFILE="<PROFILE_name>"'})," to the desired profile name"]})}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"2. Review and customize configurations:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Check available addons in ",(0,i.jsx)(n.code,{children:"infra/base/terraform/variables.tf"})]}),"\n",(0,i.jsxs)(n.li,{children:["Modify addon settings in ",(0,i.jsx)(n.code,{children:"infra/jark-stack/terraform/blueprint.tfvars"})," as needed"]}),"\n",(0,i.jsxs)(n.li,{children:["Update the AWS region in ",(0,i.jsx)(n.code,{children:"blueprint.tfvars"})]}),"\n"]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"3. Navigate to the deployment directory and run the install script:"})}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"cd ai-on-eks/infra/jark-stack && chmod +x install.sh\n./install.sh\n"})})]}),"\n",(0,i.jsxs)(a.A,{header:(0,i.jsx)(n.h3,{children:(0,i.jsx)(n.span,{children:"Verify Deployment"})}),children:[(0,i.jsx)(n.p,{children:"Update local kubeconfig so we can access kubernetes cluster"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"aws eks update-kubeconfig --name jark-stack #or whatever you used for EKS cluster name\n"})}),(0,i.jsx)(n.p,{children:"First, lets verify that we have worker nodes running in the cluster."}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl get nodes\n"})}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"NAME                             STATUS   ROLES    AGE     VERSION\nip-100-64-218-158.ec2.internal   Ready    <none>   3h13m   v1.32.3-eks-473151a\nip-100-64-39-78.ec2.internal     Ready    <none>   3h13m   v1.32.3-eks-473151a\n"})}),(0,i.jsx)(n.p,{children:"Next, lets verify all the pods are running."}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"kubectl get deployments -A\n"})}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"NAMESPACE              NAME                                                 READY   UP-TO-DATE   AVAILABLE   AGE\namazon-cloudwatch      amazon-cloudwatch-observability-controller-manager   1/1     1            1           3h3m\nargo-events            argo-events-controller-manager                       1/1     1            1           3h2m\nargo-events            events-webhook                                       1/1     1            1           3h2m\nargo-workflows         argo-workflows-server                                1/1     1            1           3h2m\nargo-workflows         argo-workflows-workflow-controller                   1/1     1            1           3h2m\nargocd                 argocd-applicationset-controller                     1/1     1            1           3h2m\nargocd                 argocd-dex-server                                    1/1     1            1           3h2m\nargocd                 argocd-notifications-controller                      1/1     1            1           3h2m\nargocd                 argocd-redis                                         1/1     1            1           3h2m\nargocd                 argocd-repo-server                                   1/1     1            1           3h2m\nargocd                 argocd-server                                        1/1     1            1           3h2m\ningress-nginx          ingress-nginx-controller                             1/1     1            1           3h1m\njupyterhub             hub                                                  1/1     1            1           3h1m\njupyterhub             proxy                                                1/1     1            1           3h1m\njupyterhub             user-scheduler                                       2/2     2            2           3h1m\nkarpenter              karpenter                                            2/2     2            2           3h1m\nkube-system            aws-load-balancer-controller                         2/2     2            2           3h1m\nkube-system            coredns                                              2/2     2            2           3h8m\nkube-system            ebs-csi-controller                                   2/2     2            2           3h4m\nkube-system            efs-csi-controller                                   2/2     2            2           3h2m\nkube-system            k8s-neuron-scheduler                                 1/1     1            1           3h1m\nkube-system            metrics-server                                       2/2     2            2           3h4m\nkube-system            my-scheduler                                         1/1     1            1           3h1m\nkuberay-operator       kuberay-operator                                     1/1     1            1           3h1m\nmonitoring             fluent-operator                                      1/1     1            1           178m\nmonitoring             kube-prometheus-stack-grafana                        1/1     1            1           178m\nmonitoring             kube-prometheus-stack-kube-state-metrics             1/1     1            1           178m\nmonitoring             kube-prometheus-stack-operator                       1/1     1            1           178m\nmonitoring             opencost                                             1/1     1            1           178m\nmonitoring             opensearch-dashboards                                2/2     2            2           177m\nmonitoring             opensearch-operator-controller-manager               1/1     1            1           178m\nnvidia-device-plugin   nvidia-device-plugin-node-feature-discovery-master   1/1     1            1           23m\n"})}),(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsxs)(n.p,{children:["Please refer to ",(0,i.jsx)(n.a,{href:"https://awslabs.github.io/ai-on-eks/docs/blueprints",children:"AI"})," page for deploying AI models on EKS."]})})]}),"\n",(0,i.jsxs)(a.A,{header:(0,i.jsx)(n.h3,{children:(0,i.jsx)(n.span,{children:"Clean Up"})}),children:[(0,i.jsx)(n.admonition,{type:"caution",children:(0,i.jsx)(n.p,{children:"To avoid unwanted charges to your AWS account, delete all the AWS resources created during this deployment."})}),(0,i.jsxs)(n.p,{children:["This script will cleanup the environment using ",(0,i.jsx)(n.code,{children:"-target"})," option to ensure all the resources are deleted in correct order."]}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"cd ai-on-eks/infra/jark-stack/terraform && chmod +x cleanup.sh\n"})})]})]})}function u(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}},42450:(e,n,s)=>{s.d(n,{A:()=>m});var r=s(96540),i=s(5556),t=s.n(i),a=s(34164);const o="collapsibleContent_q3kw",l="header_QCEw",c="icon_PckA",d="content_qLC1",h="expanded_iGsi";var u=s(74848);function p({children:e,header:n}){const[s,i]=(0,r.useState)(!1);return(0,u.jsxs)("div",{className:o,children:[(0,u.jsxs)("div",{className:(0,a.A)(l,{[h]:s}),onClick:()=>{i(!s)},children:[n,(0,u.jsx)("span",{className:(0,a.A)(c,{[h]:s}),children:s?"\ud83d\udc47":"\ud83d\udc48"})]}),s&&(0,u.jsx)("div",{className:d,children:e})]})}p.propTypes={children:t().node.isRequired,header:t().node.isRequired};const m=p},72957:(e,n,s)=>{s.d(n,{A:()=>r});const r=s.p+"assets/images/jark-stack-b9f7aeb34ed82f80ca2efb0b564fcae8.png"},76066:(e,n,s)=>{s.d(n,{A:()=>r});const r=s.p+"assets/images/jark-5d33b50b8af6ba3123cc219092abd953.png"},96388:(e,n,s)=>{s.d(n,{A:()=>r});const r=s.p+"assets/images/ray-cluster-8928896ee110a20b193fa95f297970a5.svg"}}]);