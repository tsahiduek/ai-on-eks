<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-blueprints/inference/GPUs/nvidia-nim-operator" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.8.1">
<title data-rh="true">NVIDIA NIM Operator on EKS | AI on EKS</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://awslabs.github.io/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-nim-operator"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="NVIDIA NIM Operator on EKS | AI on EKS"><meta data-rh="true" name="description" content="What is NVIDIA NIM?"><meta data-rh="true" property="og:description" content="What is NVIDIA NIM?"><link data-rh="true" rel="icon" href="/ai-on-eks/img/header-icon.png"><link data-rh="true" rel="canonical" href="https://awslabs.github.io/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-nim-operator"><link data-rh="true" rel="alternate" href="https://awslabs.github.io/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-nim-operator" hreflang="en"><link data-rh="true" rel="alternate" href="https://awslabs.github.io/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-nim-operator" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Inference on EKS","item":"https://awslabs.github.io/ai-on-eks/docs/category/inference-on-eks"},{"@type":"ListItem","position":2,"name":"GPU Inference on EKS","item":"https://awslabs.github.io/ai-on-eks/docs/category/gpu-inference-on-eks"},{"@type":"ListItem","position":3,"name":"NVIDIA NIM Operator on EKS","item":"https://awslabs.github.io/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-nim-operator"}]}</script><link rel="stylesheet" href="/ai-on-eks/assets/css/styles.f2fe9425.css">
<script src="/ai-on-eks/assets/js/runtime~main.d157f65d.js" defer="defer"></script>
<script src="/ai-on-eks/assets/js/main.6c2f188d.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/ai-on-eks/"><div class="navbar__logo"><img src="/ai-on-eks/img/header-icon.png" alt="AIoEKS Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/ai-on-eks/img/header-icon.png" alt="AIoEKS Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div></a><a class="navbar__item navbar__link" href="/ai-on-eks/docs/infra/ai-ml">Infrastructure</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/ai-on-eks/docs/blueprints">Blueprints</a><a class="navbar__item navbar__link" href="/ai-on-eks/docs/resources/intro">Resources</a><a class="navbar__item navbar__link" href="/ai-on-eks/docs/guidance">Guidance</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/awslabs/ai-on-eks" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><div class="navbar__search"><span aria-label="expand searchbar" role="button" class="search-icon" tabindex="0"></span><input id="search_input_react" type="search" placeholder="Loading..." aria-label="Search" class="navbar__search-input search-bar" disabled=""></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai-on-eks/docs/blueprints">Overview</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/ai-on-eks/docs/category/training-on-eks">Training on EKS</a><button aria-label="Expand sidebar category &#x27;Training on EKS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/ai-on-eks/docs/category/inference-on-eks">Inference on EKS</a><button aria-label="Collapse sidebar category &#x27;Inference on EKS&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" tabindex="0" href="/ai-on-eks/docs/category/gpu-inference-on-eks">GPU Inference on EKS</a><button aria-label="Collapse sidebar category &#x27;GPU Inference on EKS&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/docs/blueprints/inference/GPUs/vLLM-rayserve">RayServe with vLLM</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/docs/blueprints/inference/GPUs/vLLM-NVIDIATritonServer">NVIDIA Triton Server with vLLM</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/docs/blueprints/inference/GPUs/stablediffusion-gpus">Stable Diffusion on GPU</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-nim-llama3">NVIDIA NIM LLM on Amazon EKS</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-nim-operator">NVIDIA NIM Operator on EKS</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/docs/blueprints/inference/GPUs/ray-vllm-deepseek">DeepSeek-R1 on EKS</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-dynamo">NVIDIA Dynamo on Amazon EKS</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/docs/blueprints/inference/GPUs/aibrix-deepseek-distill">AIBrix on EKS</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai-on-eks/docs/category/neuron-inference-on-eks">Neuron Inference on EKS</a><button aria-label="Expand sidebar category &#x27;Neuron Inference on EKS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/docs/blueprints/inference">Overview</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/docs/blueprints/inference/inference-charts">Inference Charts</a></li></ul></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/ai-on-eks/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai-on-eks/docs/category/inference-on-eks"><span>Inference on EKS</span></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai-on-eks/docs/category/gpu-inference-on-eks"><span>GPU Inference on EKS</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">NVIDIA NIM Operator on EKS</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>NVIDIA NIM Operator on Amazon EKS</h1></header>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-nvidia-nim"><a href="https://docs.nvidia.com/nim/large-language-models/latest/introduction.html" target="_blank" rel="noopener noreferrer">What is NVIDIA NIM?</a><a href="#what-is-nvidia-nim" class="hash-link" aria-label="Direct link to what-is-nvidia-nim" title="Direct link to what-is-nvidia-nim">​</a></h2>
<p><strong>NVIDIA NIM</strong> (<a href="https://docs.nvidia.com/nim/large-language-models/latest/introduction.html" target="_blank" rel="noopener noreferrer">NVIDIA Inference Microservices</a>) is a set of containerized microservices that make it easier to deploy and host large language models (LLMs) and other AI models in your own environment. NIM provides standard APIs (similar to OpenAI or other AI services) for developers to build applications like chatbots and AI assistants, while leveraging NVIDIA’s GPU acceleration for high-performance inference. In essence, NIM abstracts away the complexities of model runtime and optimization, offering a fast path to inference with optimized backends (e.g., TensorRT-LLM, FasterTransformer, etc.) under the hood.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="nvidia-nim-operator-for-kubernetes"><a href="https://docs.nvidia.com/nim-operator/latest/index.html#" target="_blank" rel="noopener noreferrer">NVIDIA NIM Operator for Kubernetes</a><a href="#nvidia-nim-operator-for-kubernetes" class="hash-link" aria-label="Direct link to nvidia-nim-operator-for-kubernetes" title="Direct link to nvidia-nim-operator-for-kubernetes">​</a></h2>
<p>The <strong>NVIDIA NIM Operator</strong> is a Kubernetes operator that automates the deployment, scaling, and management of NVIDIA NIM microservices on a Kubernetes cluster.</p>
<p><img decoding="async" loading="lazy" alt="NVIDIA NIM Operator Architecture" src="/ai-on-eks/assets/images/NIMOperatorArchitecture-547ebef43bd769516d44bc9f1bf45b18.png" width="1920" height="1080" class="img_ev3q"></p>
<p>Instead of manually pulling containers, provisioning GPU nodes, or writing YAML for every model, the NIM Operator introduces three primary <a href="https://docs.nvidia.com/nim-operator/latest/crds.html" target="_blank" rel="noopener noreferrer">Custom Resource Definitions (CRDs)</a>:</p>
<ul>
<li><a href="https://docs.nvidia.com/nim-operator/latest/cache.html" target="_blank" rel="noopener noreferrer"><code>NIMCache</code></a></li>
<li><a href="https://docs.nvidia.com/nim-operator/latest/service.html" target="_blank" rel="noopener noreferrer"><code>NIMService</code></a></li>
<li>[<code>NIMPipeline</code>] (<a href="https://docs.nvidia.com/nim-operator/latest/pipelines.html" target="_blank" rel="noopener noreferrer">https://docs.nvidia.com/nim-operator/latest/pipelines.html</a>)</li>
</ul>
<p>These CRDs allow you to declaratively define model deployments using native Kubernetes syntax.</p>
<p>The Operator handles:</p>
<ul>
<li>Pulling the model image from NVIDIA GPU Cloud (NGC)</li>
<li>Caching model weights and optimized runtime profiles</li>
<li>Launching model-serving pods with GPU allocation</li>
<li>Exposing inference endpoints via Kubernetes Services</li>
<li>Integrating with autoscaling (e.g., HPA + Karpenter)</li>
<li>Chaining multiple models together into inference pipelines using NIMPipeline</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="nimcache--model-caching-for-faster-load-times"><a href="https://docs.nvidia.com/nim-operator/latest/cache.html" target="_blank" rel="noopener noreferrer">NIMCache – Model Caching for Faster Load Times</a><a href="#nimcache--model-caching-for-faster-load-times" class="hash-link" aria-label="Direct link to nimcache--model-caching-for-faster-load-times" title="Direct link to nimcache--model-caching-for-faster-load-times">​</a></h3>
<p>A <code>NIMCache</code> (<code>nimcaches.apps.nvidia.com</code>) is a custom resource that pre-downloads and stores a model’s weights, tokenizer, and runtime-optimized engine files (such as TensorRT-LLM profiles) into a shared persistent volume.</p>
<p>This ensures:</p>
<ul>
<li><strong>Faster cold start times</strong>: no repeated downloads from NGC</li>
<li><strong>Storage reuse across nodes and replicas</strong></li>
<li><strong>Centralized, shared model store</strong> (typically on EFS or FSx for Lustre in EKS)</li>
</ul>
<p>Model profiles are optimized for specific GPUs (e.g., A10G, L4) and precisions (e.g., FP16). When NIMCache is created, the Operator discovers available model profiles and selects the best one for your cluster.</p>
<blockquote>
<p>📌 Tip: Using <code>NIMCache</code> is highly recommended for production, especially when running multiple replicas or restarting models frequently.</p>
</blockquote>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="nimservice--deploying-and-managing-the-model-server"><a href="https://docs.nvidia.com/nim-operator/latest/service.html" target="_blank" rel="noopener noreferrer">NIMService – Deploying and Managing the Model Server</a><a href="#nimservice--deploying-and-managing-the-model-server" class="hash-link" aria-label="Direct link to nimservice--deploying-and-managing-the-model-server" title="Direct link to nimservice--deploying-and-managing-the-model-server">​</a></h3>
<p>A <code>NIMService</code> (<code>nimservices.apps.nvidia.com</code>) represents a running instance of a NIM model server on your cluster. It specifies the container image, GPU resources, number of replicas, and optionally the name of a <code>NIMCache</code>.</p>
<p>Key benefits:</p>
<ul>
<li><strong>Declarative model deployment</strong> using Kubernetes YAML</li>
<li><strong>Automatic node scheduling</strong> for GPU nodes</li>
<li><strong>Shared cache support</strong> using <code>NIMCache</code></li>
<li><strong>Autoscaling</strong> via HPA or external triggers</li>
<li><strong>ClusterIP or Ingress support</strong> to expose APIs</li>
</ul>
<p>For example, deploying the Meta Llama 3.1 8B Instruct model involves creating:</p>
<ul>
<li>A <code>NIMCache</code> to store the model (optional but recommended)</li>
<li>A <code>NIMService</code> pointing to the cached model and allocating GPUs</li>
</ul>
<p>If <code>NIMCache</code> is not used, the model will be downloaded each time a pod starts, which may increase startup latency.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="nimpipeline"><a href="https://docs.nvidia.com/nim-operator/latest/pipeline.html" target="_blank" rel="noopener noreferrer">NIMPipeline</a><a href="#nimpipeline" class="hash-link" aria-label="Direct link to nimpipeline" title="Direct link to nimpipeline">​</a></h3>
<p><code>NIMPipeline</code> is another CRD that can group multiple <code>NIMService</code> resources into an ordered inference pipeline. This is useful for multi-model workflows like:</p>
<ul>
<li>Retrieval-Augmented Generation (RAG)</li>
<li>Embeddings + LLM chaining</li>
<li>Preprocessing + classification pipelines</li>
</ul>
<blockquote>
<p>In this tutorial, we focus on a single model deployment using <code>NIMCache</code> and <code>NIMService</code>.</p>
</blockquote>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="overview-of-this-deployment-pattern-on-amazon-eks">Overview of this deployment pattern on Amazon EKS<a href="#overview-of-this-deployment-pattern-on-amazon-eks" class="hash-link" aria-label="Direct link to Overview of this deployment pattern on Amazon EKS" title="Direct link to Overview of this deployment pattern on Amazon EKS">​</a></h2>
<p>This deployment blueprint demonstrates how to run the <strong>Meta Llama 3.1 8B Instruct</strong> model on <strong>Amazon EKS</strong> using the <strong>NVIDIA NIM Operator</strong> with multi-GPU support and optimized model caching for fast startup times.</p>
<p><img decoding="async" loading="lazy" alt="NVIDIA NIM Operator Architecture" src="/ai-on-eks/assets/images/NIMOperatoronEKS-25e8b19c29b3881fa3f9b58036135179.png" width="1920" height="1080" class="img_ev3q"></p>
<p>The model is served using:</p>
<ul>
<li><strong>G5 instances (g5.12xlarge)</strong>: These instances come with <strong>4 NVIDIA A10G GPUs</strong></li>
<li><strong>Tensor Parallelism (TP)</strong>: Set to <code>2</code>, meaning the model will run in parallel across <strong>2 GPUs</strong></li>
<li><strong>Persistent Shared Cache</strong>: Backed by Amazon <strong>EFS</strong> to speed up model startup by reusing previously generated engine files</li>
</ul>
<p>By combining these components, the model is deployed as a scalable Kubernetes workload that supports:</p>
<ul>
<li>Efficient GPU scheduling with <a href="https://karpenter.sh/" target="_blank" rel="noopener noreferrer">Karpenter</a></li>
<li>Fast model load using the <a href="https://docs.nvidia.com/nim-operator/latest/cache.html" target="_blank" rel="noopener noreferrer"><code>NIMCache</code></a></li>
<li>Scalable serving endpoint via <a href="https://docs.nvidia.com/nim-operator/latest/service.html" target="_blank" rel="noopener noreferrer"><code>NIMService</code></a></li>
</ul>
<blockquote>
<p>📌 Note: You can modify the <code>tensorParallelism</code> setting or select a different instance type (e.g., G6 with L4 GPUs) based on your performance and cost requirements.</p>
</blockquote>
<div class="theme-admonition theme-admonition-warning admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>warning</div><div class="admonitionContent_BuS1"><p>Note: Before implementing NVIDIA NIM, please be aware it is part of <a href="https://www.nvidia.com/en-us/data-center/products/ai-enterprise/" target="_blank" rel="noopener noreferrer">NVIDIA AI Enterprise</a>, which may introduce potential cost and licensing for production use.</p><p>For evaluation, NVIDIA also offers a free evaluation license to try NVIDIA AI Enterprise for 90 days, and you can <a href="https://enterpriseproductregistration.nvidia.com/?LicType=EVAL&amp;ProductFamily=NVAIEnterprise" target="_blank" rel="noopener noreferrer">register</a> it with your corporate email.</p></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="deploying-the-solution">Deploying the Solution<a href="#deploying-the-solution" class="hash-link" aria-label="Direct link to Deploying the Solution" title="Direct link to Deploying the Solution">​</a></h2>
<p>In this tutorial, the entire AWS infrastructure is provisioned using <strong>Terraform</strong>, including:</p>
<ul>
<li>Amazon VPC with public and private subnets</li>
<li>Amazon EKS cluster</li>
<li>GPU nodepools using <strong>Karpenter</strong></li>
<li>Addons such as:<!-- -->
<ul>
<li>NVIDIA device plugin</li>
<li>EFS CSI driver</li>
<li><strong>NVIDIA NIM Operator</strong></li>
</ul>
</li>
</ul>
<p>As a demonstration, the <strong>Meta Llama-3.1 8B Instruct</strong> model will be deployed using a <code>NIMService</code>, optionally backed by a <code>NIMCache</code> for improved cold start performance.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="prerequisites">Prerequisites<a href="#prerequisites" class="hash-link" aria-label="Direct link to Prerequisites" title="Direct link to Prerequisites">​</a></h3>
<p>Before getting started with NVIDIA NIM, ensure you have the following:</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Click to expand the NVIDIA NIM account setup details</summary><div><div class="collapsibleContent_i85q"><p><strong>NVIDIA AI Enterprise Account</strong></p><ul>
<li>Register for an NVIDIA AI Enterprise account. If you don&#x27;t have one, you can sign up for a trial account using this <a href="https://enterpriseproductregistration.nvidia.com/?LicType=EVAL&amp;ProductFamily=NVAIEnterprise" target="_blank" rel="noopener noreferrer">link</a>.</li>
</ul><p><strong>NGC API Key</strong></p><ol>
<li>
<p>Log in to your NVIDIA AI Enterprise account</p>
</li>
<li>
<p>Navigate to the NGC (NVIDIA GPU Cloud) <a href="https://org.ngc.nvidia.com/" target="_blank" rel="noopener noreferrer">portal</a></p>
</li>
<li>
<p>Generate a personal API key:</p>
<ul>
<li>Go to your account settings or navigate directly to: <a href="https://org.ngc.nvidia.com/setup/personal-keys" target="_blank" rel="noopener noreferrer">https://org.ngc.nvidia.com/setup/personal-keys</a></li>
<li>Click on &quot;Generate Personal Key&quot;</li>
<li>Ensure that at least &quot;NGC Catalog&quot; is selected from the &quot;Services Included&quot; dropdown</li>
<li>Copy and securely store your API key, the key should have a prefix with <code>nvapi-</code></li>
</ul>
<p><img decoding="async" loading="lazy" alt="NGC API KEY" src="/ai-on-eks/assets/images/nim-ngc-api-key-8f10f8d047f3721dbeddd112c6b69a81.png" width="2822" height="1524" class="img_ev3q"></p>
</li>
</ol><p><strong>Validate NGC API Key and Test Image Pull</strong></p><p>To ensure your API key is valid and working correctly:</p><ol>
<li>Set up your NGC API key as an environment variable:</li>
</ol><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token builtin class-name">export</span><span class="token plain"> </span><span class="token assign-left variable" style="color:#36acaa">NGC_API_KEY</span><span class="token operator" style="color:#393A34">=</span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">your_api_key_here</span><span class="token operator" style="color:#393A34">&gt;</span><br></span></code></pre></div></div><ol start="2">
<li>Authenticate Docker with the NVIDIA Container Registry:</li>
</ol><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token builtin class-name">echo</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;</span><span class="token string variable" style="color:#36acaa">$NGC_API_KEY</span><span class="token string" style="color:#e3116c">&quot;</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">|</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">docker</span><span class="token plain"> login nvcr.io </span><span class="token parameter variable" style="color:#36acaa">--username</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&#x27;$oauthtoken&#x27;</span><span class="token plain"> --password-stdin</span><br></span></code></pre></div></div><ol start="3">
<li>Test pulling an image from NGC:</li>
</ol><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token function" style="color:#d73a49">docker</span><span class="token plain"> pull nvcr.io/nim/meta/llama3-8b-instruct:latest</span><br></span></code></pre></div></div><p>You do not have to wait for it to complete, just to make sure the API key is valid to pull the image.</p></div></div></details>
<p>The following are required to run this tutorial</p>
<ul>
<li>An active AWS account with admin equivalent permissions</li>
<li><a href="https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html" target="_blank" rel="noopener noreferrer">aws cli</a></li>
<li><a href="https://Kubernetes.io/docs/tasks/tools/" target="_blank" rel="noopener noreferrer">kubectl</a></li>
<li><a href="https://developer.hashicorp.com/terraform/tutorials/aws-get-started/install-cli" target="_blank" rel="noopener noreferrer">Terraform</a></li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="deploy">Deploy<a href="#deploy" class="hash-link" aria-label="Direct link to Deploy" title="Direct link to Deploy">​</a></h3>
<p>Clone the <a href="https://github.com/awslabs/ai-on-eks" target="_blank" rel="noopener noreferrer">ai-on-eks</a> repository that contains the Terraform code for this deployment pattern:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token function" style="color:#d73a49">git</span><span class="token plain"> clone https://github.com/awslabs/ai-on-eks.git</span><br></span></code></pre></div></div>
<p>Navigate to the NVIDIA NIM deployment directory and run the install script to deploy the infrastructure:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token builtin class-name">cd</span><span class="token plain"> ai-on-eks/infra/nvidia-nim</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">./install.sh</span><br></span></code></pre></div></div>
<p>This deployment will take approximately <code>~20 minute</code> to complete.</p>
<p>Once the installation finishes, you may find the <code>configure_kubectl</code> command from the output. Run the following to configure EKS cluster access</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># Creates k8s config file to authenticate with EKS</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">aws eks </span><span class="token parameter variable" style="color:#36acaa">--region</span><span class="token plain"> us-west-2 update-kubeconfig </span><span class="token parameter variable" style="color:#36acaa">--name</span><span class="token plain"> nvidia-nim-eks</span><br></span></code></pre></div></div>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Verify the deployments - Click to expand the deployment details</summary><div><div class="collapsibleContent_i85q"><p>$ kubectl get all -n nim-operator</p><div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl get all -n nim-operator</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">NAME                                                 READY   STATUS    RESTARTS   AGE</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">pod/nim-operator-k8s-nim-operator-6fdffdf97f-56fxc   1/1     Running   0          26h</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">NAME                                       TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">service/k8s-nim-operator-metrics-service   ClusterIP   172.20.148.6   &lt;none&gt;        8080/TCP   26h</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">NAME                                            READY   UP-TO-DATE   AVAILABLE   AGE</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">deployment.apps/nim-operator-k8s-nim-operator   1/1     1            1           26h</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">NAME                                                       DESIRED   CURRENT   READY   AGE</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">replicaset.apps/nim-operator-k8s-nim-operator-6fdffdf97f   1         1         1       26h</span><br></span></code></pre></div></div><p>$ kubectl get crds | grep nim</p><div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">nimcaches.apps.nvidia.com                    2025-03-27T17:39:00Z</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">nimpipelines.apps.nvidia.com                 2025-03-27T17:39:00Z</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">nimservices.apps.nvidia.com                  2025-03-27T17:39:01Z</span><br></span></code></pre></div></div><p>$ kubectl get crds | grep nemo</p><div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">nemocustomizers.apps.nvidia.com              2025-03-27T17:38:59Z</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">nemodatastores.apps.nvidia.com               2025-03-27T17:38:59Z</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">nemoentitystores.apps.nvidia.com             2025-03-27T17:38:59Z</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">nemoevaluators.apps.nvidia.com               2025-03-27T17:39:00Z</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">nemoguardrails.apps.nvidia.com               2025-03-27T17:39:00Z</span><br></span></code></pre></div></div><p>To list Karpenter autoscaling Nodepools</p><p>$ kubectl get nodepools</p><div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">NAME                NODECLASS           NODES   READY   AGE</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">g5-gpu-karpenter    g5-gpu-karpenter    1       True    47h</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">g6-gpu-karpenter    g6-gpu-karpenter    0       True    7h56m</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">inferentia-inf2     inferentia-inf2     0       False   47h</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">trainium-trn1       trainium-trn1       0       False   47h</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">x86-cpu-karpenter   x86-cpu-karpenter   0       True    47h</span><br></span></code></pre></div></div></div></div></details>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="deploy-llama-31-8b-instruct-with-nim-operator">Deploy llama-3.1-8b-instruct with NIM Operator<a href="#deploy-llama-31-8b-instruct-with-nim-operator" class="hash-link" aria-label="Direct link to Deploy llama-3.1-8b-instruct with NIM Operator" title="Direct link to Deploy llama-3.1-8b-instruct with NIM Operator">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="step-1-create-secrets-for-authentication">Step 1: Create Secrets for Authentication<a href="#step-1-create-secrets-for-authentication" class="hash-link" aria-label="Direct link to Step 1: Create Secrets for Authentication" title="Direct link to Step 1: Create Secrets for Authentication">​</a></h4>
<p>To access the NVIDIA container registry and model artifacts, you&#x27;ll need to provide your NGC API key. This script creates two Kubernetes secrets: <code>ngc-secret</code> for Docker image pulls and <code>ngc-api-secret</code> for model authorization.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token builtin class-name">cd</span><span class="token plain"> blueprints/inference/gpu/nvidia-nim-operator-llama3-8b</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token assign-left variable" style="color:#36acaa">NGC_API_KEY</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&quot;your-real-ngc-key&quot;</span><span class="token plain"> ./deploy-nim-auth.sh</span><br></span></code></pre></div></div>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="step-2-cache-the-model-to-efs-using-nimcache-crd">Step 2: Cache the Model to EFS using NIMCache CRD<a href="#step-2-cache-the-model-to-efs-using-nimcache-crd" class="hash-link" aria-label="Direct link to Step 2: Cache the Model to EFS using NIMCache CRD" title="Direct link to Step 2: Cache the Model to EFS using NIMCache CRD">​</a></h4>
<p>The <code>NIMCache</code> custom resource will pull the model and cache optimized engine profiles to EFS. This dramatically reduces startup time when launching the model later via <code>NIMService</code>.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token builtin class-name">cd</span><span class="token plain"> blueprints/inference/gpu/nvidia-nim-operator-llama3-8b</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kubectl apply </span><span class="token parameter variable" style="color:#36acaa">-f</span><span class="token plain"> nim-cache-llama3-8b-instruct.yaml</span><br></span></code></pre></div></div>
<p>Check status:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl get nimcaches.apps.nvidia.com </span><span class="token parameter variable" style="color:#36acaa">-n</span><span class="token plain"> nim-service</span><br></span></code></pre></div></div>
<p>Expected output:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">NAME                      STATUS   PVC                           AGE</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">meta-llama3-8b-instruct   Ready    meta-llama3-8b-instruct-pvc   21h</span><br></span></code></pre></div></div>
<p>Display cached model profiles:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl get nimcaches.apps.nvidia.com </span><span class="token parameter variable" style="color:#36acaa">-n</span><span class="token plain"> nim-service </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  meta-llama3-8b-instruct </span><span class="token parameter variable" style="color:#36acaa">-o</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">jsonpath</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&quot;{.status.profiles}&quot;</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">|</span><span class="token plain"> jq </span><span class="token builtin class-name">.</span><br></span></code></pre></div></div>
<p>Sample output:</p>
<div class="language-json codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-json codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token punctuation" style="color:#393A34">[</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token property" style="color:#36acaa">&quot;config&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token property" style="color:#36acaa">&quot;feat_lora&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;false&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token property" style="color:#36acaa">&quot;gpu&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;A10G&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token property" style="color:#36acaa">&quot;llm_engine&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;tensorrt_llm&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token property" style="color:#36acaa">&quot;precision&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;fp16&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token property" style="color:#36acaa">&quot;profile&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;throughput&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token property" style="color:#36acaa">&quot;tp&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;2&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token punctuation" style="color:#393A34">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">]</span><br></span></code></pre></div></div>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="step-3-deploy-the-model-using-nimservice-crd">Step 3: Deploy the Model using NIMService CRD<a href="#step-3-deploy-the-model-using-nimservice-crd" class="hash-link" aria-label="Direct link to Step 3: Deploy the Model using NIMService CRD" title="Direct link to Step 3: Deploy the Model using NIMService CRD">​</a></h4>
<p>Now launch the model service using the cached engine profiles.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token builtin class-name">cd</span><span class="token plain"> blueprints/inference/gpu/nvidia-nim-operator-llama3-8b</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kubectl apply </span><span class="token parameter variable" style="color:#36acaa">-f</span><span class="token plain"> nim-service-llama3-8b-instruct.yaml</span><br></span></code></pre></div></div>
<p>Check the deployed resources:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl get all </span><span class="token parameter variable" style="color:#36acaa">-n</span><span class="token plain"> nim-service</span><br></span></code></pre></div></div>
<p>Exepcted Output:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">NAME                                           READY   STATUS    RESTARTS   AGE</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">pod/meta-llama3-8b-instruct-6cdf47d6f6-hlbnf   1/1     Running   0          6h35m</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">NAME                              TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)    AGE</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">service/meta-llama3-8b-instruct   ClusterIP   172.20.85.8   &lt;none&gt;        8000/TCP   6h35m</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">NAME                                      READY   UP-TO-DATE   AVAILABLE   AGE</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">deployment.apps/meta-llama3-8b-instruct   1/1     1            1           6h35m</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">NAME                                                 DESIRED   CURRENT   READY   AGE</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">replicaset.apps/meta-llama3-8b-instruct-6cdf47d6f6   1         1         1       6h35m</span><br></span></code></pre></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="-model-startup-timeline">🚀 Model Startup Timeline<a href="#-model-startup-timeline" class="hash-link" aria-label="Direct link to 🚀 Model Startup Timeline" title="Direct link to 🚀 Model Startup Timeline">​</a></h3>
<p>The following sample is captured from the <code>pod/meta-llama3-8b-instruct-6cdf47d6f6-hlbnf</code> log</p>
<table><thead><tr><th>Step</th><th>Timestamp</th><th>Description</th></tr></thead><tbody><tr><td>Start</td><td>~20:00:50</td><td>Pod starts, NIM container logs begin</td></tr><tr><td>Profile Match</td><td>20:00:50.100</td><td>Detects and selects cached profile (tp=2)</td></tr><tr><td>Workspace Ready</td><td>20:00:50.132</td><td>Model workspace initialized via EFS in 0.126s</td></tr><tr><td>TensorRT Init</td><td>20:00:51.168</td><td>TensorRT-LLM engine begins setup</td></tr><tr><td>Engine Ready</td><td>20:01:06</td><td>Engine loaded and profiles activated (~16.6 GiB across 2 GPUs)</td></tr><tr><td>API Server Ready</td><td>20:02:11.036</td><td>FastAPI + Uvicorn starts</td></tr><tr><td>Health Check OK</td><td>20:02:18.781</td><td><code>/v1/health/ready</code> endpoint returns 200 OK</td></tr></tbody></table>
<blockquote>
<p>⚡ <strong>Startup time (cold boot to ready): ~81 seconds</strong> thanks to cached engine on EFS.</p>
</blockquote>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="test-the-model-with-a-prompt">Test the Model with a Prompt<a href="#test-the-model-with-a-prompt" class="hash-link" aria-label="Direct link to Test the Model with a Prompt" title="Direct link to Test the Model with a Prompt">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="step-1-port-forward-the-model-service">Step 1: Port Forward the Model Service<a href="#step-1-port-forward-the-model-service" class="hash-link" aria-label="Direct link to Step 1: Port Forward the Model Service" title="Direct link to Step 1: Port Forward the Model Service">​</a></h4>
<p>Expose the model locally using port forwarding:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl port-forward </span><span class="token parameter variable" style="color:#36acaa">-n</span><span class="token plain"> nim-service service/meta-llama3-8b-instruct </span><span class="token number" style="color:#36acaa">8001</span><span class="token plain">:8000</span><br></span></code></pre></div></div>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="step-2-send-a-sample-prompt-using-curl">Step 2: Send a Sample Prompt Using curl<a href="#step-2-send-a-sample-prompt-using-curl" class="hash-link" aria-label="Direct link to Step 2: Send a Sample Prompt Using curl" title="Direct link to Step 2: Send a Sample Prompt Using curl">​</a></h4>
<p>Run the following command to test the model with a chat prompt:</p>
<div class="language-sh codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-sh codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token function" style="color:#d73a49">curl</span><span class="token plain"> </span><span class="token parameter variable" style="color:#36acaa">-X</span><span class="token plain"> POST </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  http://localhost:8001/v1/chat/completions </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token parameter variable" style="color:#36acaa">-H</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&#x27;Accept: application/json&#x27;</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token parameter variable" style="color:#36acaa">-H</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&#x27;Content-Type: application/json&#x27;</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token parameter variable" style="color:#36acaa">-d</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&#x27;{</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">    &quot;model&quot;: &quot;meta/llama-3.1-8b-instruct&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">    &quot;messages&quot;: [</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">      {</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">        &quot;role&quot;: &quot;user&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">        &quot;content&quot;: &quot;What should I do for a 4 day vacation at Cape Hatteras National Seashore?&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">      }</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">    ],</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">    &quot;top_p&quot;: 1,</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">    &quot;n&quot;: 1,</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">    &quot;max_tokens&quot;: 1024,</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">    &quot;stream&quot;: false,</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">    &quot;frequency_penalty&quot;: 0.0,</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">    &quot;stop&quot;: [&quot;STOP&quot;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">  }&#x27;</span><br></span></code></pre></div></div>
<p><strong>Sample Response (Shortened):</strong></p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">{&quot;id&quot;:&quot;chat-061a9dba9179437fa24cab7f7c767f19&quot;,&quot;object&quot;:&quot;chat.completion&quot;,&quot;created&quot;:1743215809,&quot;model&quot;:&quot;meta/llama-3.1-8b-instruct&quot;,&quot;choices&quot;:[{&quot;index&quot;:0,&quot;message&quot;:{&quot;role&quot;:&quot;assistant&quot;,&quot;content&quot;:&quot;Cape Hatteras National Seashore is a beautiful coastal destination with a rich history, pristine beaches,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">...</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">exploration of the area&#x27;s natural beauty and history. Feel free to modify it to suit your interests and preferences. Safe travels!&quot;},&quot;logprobs&quot;:null,&quot;finish_reason&quot;:&quot;stop&quot;,&quot;stop_reason&quot;:null}],&quot;usage&quot;:{&quot;prompt_tokens&quot;:30,&quot;total_tokens&quot;:773,&quot;completion_tokens&quot;:743},&quot;prompt_logprobs&quot;:null}%</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span></code></pre></div></div>
<blockquote>
<p>🧠 The model is now running with Tensor Parallelism = 2 across two A10G GPUs, each utilizing approximately 21.4 GiB of memory. Thanks to NIMCache backed by EFS, the model loaded quickly and is ready for low-latency inference.</p>
</blockquote>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="open-webui-deployment">Open WebUI Deployment<a href="#open-webui-deployment" class="hash-link" aria-label="Direct link to Open WebUI Deployment" title="Direct link to Open WebUI Deployment">​</a></h2>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_BuS1"><p><a href="https://github.com/open-webui/open-webui" target="_blank" rel="noopener noreferrer">Open WebUI</a> is compatible only with models that work with the OpenAI API server and Ollama.</p></div></div>
<p><strong>1. Deploy the WebUI</strong></p>
<p>Deploy the <a href="https://github.com/open-webui/open-webui" target="_blank" rel="noopener noreferrer">Open WebUI</a> by running the following command:</p>
<div class="language-sh codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-sh codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl apply </span><span class="token parameter variable" style="color:#36acaa">-f</span><span class="token plain"> ai-on-eks/blueprints/inference/gpu/nvidia-nim-operator-llama3-8b/openai-webui-deployment.yaml</span><br></span></code></pre></div></div>
<p><strong>2. Port Forward to Access WebUI</strong></p>
<p>Use kubectl port-forward to access the WebUI locally:</p>
<div class="language-sh codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-sh codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl port-forward svc/open-webui </span><span class="token number" style="color:#36acaa">8081</span><span class="token plain">:80 </span><span class="token parameter variable" style="color:#36acaa">-n</span><span class="token plain"> openai-webui</span><br></span></code></pre></div></div>
<p><strong>3. Access the WebUI</strong></p>
<p>Open your browser and go to <a href="http://localhost:8081" target="_blank" rel="noopener noreferrer">http://localhost:8081</a></p>
<p><strong>4. Sign Up</strong></p>
<p>Sign up using your name, email, and a dummy password.</p>
<p><strong>5. Start a New Chat</strong></p>
<p>Click on New Chat and select the model from the dropdown menu, as shown in the screenshot below:</p>
<p><img decoding="async" loading="lazy" alt="alt text" src="/ai-on-eks/assets/images/openweb-ui-nim-1-7fddca9b300b28af7260b0bc12b69ff0.png" width="2076" height="936" class="img_ev3q"></p>
<p><strong>6. Enter Test Prompt</strong></p>
<p>Enter your prompt, and you will see the streaming results, as shown below:</p>
<p><img decoding="async" loading="lazy" alt="alt text" src="/ai-on-eks/assets/images/openweb-ui-nim-2-e9a836cbafc53c50b8437d766b3adbd4.png" width="2738" height="1382" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="performance-testing-with-nvidia-genai-perf-tool">Performance Testing with NVIDIA GenAI-Perf Tool<a href="#performance-testing-with-nvidia-genai-perf-tool" class="hash-link" aria-label="Direct link to Performance Testing with NVIDIA GenAI-Perf Tool" title="Direct link to Performance Testing with NVIDIA GenAI-Perf Tool">​</a></h2>
<p><a href="https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/client/src/c%2B%2B/perf_analyzer/genai-perf/README.html" target="_blank" rel="noopener noreferrer">GenAI-Perf</a> is a command line tool for measuring the throughput and latency of generative AI models as served through an inference server.</p>
<p>GenAI-Perf can be used as standard tool to benchmark with other models deployed with inference server. But this tool requires a GPU. To make it easier, we provide you a pre-configured manifest <code>genaiperf-deploy.yaml</code> to run the tool.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token builtin class-name">cd</span><span class="token plain"> ai-on-eks/blueprints/inference/gpu/nvidia-nim-operator-llama3-8b</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kubectl apply </span><span class="token parameter variable" style="color:#36acaa">-f</span><span class="token plain"> genaiperf-deploy.yaml</span><br></span></code></pre></div></div>
<p>Once the pod is ready with running status <code>1/1</code>, can execute into the pod.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token builtin class-name">export</span><span class="token plain"> </span><span class="token assign-left variable" style="color:#36acaa">POD_NAME</span><span class="token operator" style="color:#393A34">=</span><span class="token variable" style="color:#36acaa">$(</span><span class="token variable" style="color:#36acaa">kubectl get po </span><span class="token variable parameter variable" style="color:#36acaa">-l</span><span class="token variable" style="color:#36acaa"> </span><span class="token variable assign-left variable" style="color:#36acaa">app</span><span class="token variable operator" style="color:#393A34">=</span><span class="token variable" style="color:#36acaa">genai-perf </span><span class="token variable parameter variable" style="color:#36acaa">-ojsonpath</span><span class="token variable operator" style="color:#393A34">=</span><span class="token variable string" style="color:#e3116c">&#x27;{.items[0].metadata.name}&#x27;</span><span class="token variable" style="color:#36acaa">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kubectl </span><span class="token builtin class-name">exec</span><span class="token plain"> </span><span class="token parameter variable" style="color:#36acaa">-it</span><span class="token plain"> </span><span class="token variable" style="color:#36acaa">$POD_NAME</span><span class="token plain"> -- </span><span class="token function" style="color:#d73a49">bash</span><br></span></code></pre></div></div>
<p>Run the testing to the deployed NIM Llama3 model</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">genai-perf profile </span><span class="token parameter variable" style="color:#36acaa">-m</span><span class="token plain"> meta/llama-3.1-8b-instruct </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token parameter variable" style="color:#36acaa">--url</span><span class="token plain"> meta-llama3-8b-instruct.nim-service:8000 </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --service-kind openai </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --endpoint-type chat </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --num-prompts </span><span class="token number" style="color:#36acaa">100</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --synthetic-input-tokens-mean </span><span class="token number" style="color:#36acaa">200</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --synthetic-input-tokens-stddev </span><span class="token number" style="color:#36acaa">0</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --output-tokens-mean </span><span class="token number" style="color:#36acaa">100</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --output-tokens-stddev </span><span class="token number" style="color:#36acaa">0</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token parameter variable" style="color:#36acaa">--concurrency</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">20</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token parameter variable" style="color:#36acaa">--streaming</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token parameter variable" style="color:#36acaa">--tokenizer</span><span class="token plain"> hf-internal-testing/llama-tokenizer</span><br></span></code></pre></div></div>
<p>You should see similar output like the following</p>
<p><img decoding="async" loading="lazy" alt="NIM Operator genai-perf result" src="/ai-on-eks/assets/images/nim-operator-genaiperf-63f2fb8f12dfd067b016c6c11d271765.png" width="1688" height="404" class="img_ev3q"></p>
<p>You should be able to see the <a href="https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/perf_analyzer/genai-perf/README.html#metrics" target="_blank" rel="noopener noreferrer">metrics</a> that genai-perf collects, including Request latency, Out token throughput, Request throughput.</p>
<p>To understand the command line options, please refer to <a href="https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/perf_analyzer/genai-perf/README.html#command-line-options" target="_blank" rel="noopener noreferrer">this documentation</a>.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="grafana-dashboard">Grafana Dashboard<a href="#grafana-dashboard" class="hash-link" aria-label="Direct link to Grafana Dashboard" title="Direct link to Grafana Dashboard">​</a></h3>
<p>NVIDIA provided a Grafana <a href="https://docs.nvidia.com/nim/large-language-models/latest/_downloads/66e67782ce543dcccec574b1483f0ea0/nim-dashboard-example.json" target="_blank" rel="noopener noreferrer">dashboard</a> to better visualize NIM status. In the Grafana dashboard, it contains several important metrics:</p>
<ul>
<li><strong>Time to First Token (TTFT)</strong>: The latency between the initial inference request to the model and the return of the first token.</li>
<li><strong>Inter-Token Latency (ITL)</strong>: The latency between each token after the first.</li>
<li><strong>Total Throughput</strong>: The total number of tokens generated per second by the NIM.</li>
</ul>
<p>You can find more metrics description from this <a href="https://docs.nvidia.com/nim/large-language-models/latest/observability.html" target="_blank" rel="noopener noreferrer">document</a>.</p>
<p><img decoding="async" loading="lazy" alt="NVIDIA LLM Server" src="/ai-on-eks/assets/images/nim-dashboard-c8dad192e95da70bd2474db5a466b448.png" width="3300" height="1730" class="img_ev3q"></p>
<p>You can monitor metrics such as Time-to-First-Token, Inter-Token-Latency, KV Cache Utilization metrics.</p>
<p><img decoding="async" loading="lazy" alt="NVIDIA NIM Metrics" src="/ai-on-eks/assets/images/nim-dashboard-2-7fa24a28624902d2a665f515ab2b4fd1.png" width="3300" height="1730" class="img_ev3q"></p>
<p>To view the Grafana dashboard to monitor these metrics, follow the steps below:</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Click to expand details</summary><div><div class="collapsibleContent_i85q"><p><strong>1. Retrieve the Grafana password.</strong></p><p>The password is saved in the AWS Secret Manager. Below Terraform command will show you the secret name.</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">terraform output grafana_secret_name</span><br></span></code></pre></div></div><p>Then use the output secret name to run below command,</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">aws secretsmanager get-secret-value --secret-id </span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">grafana_secret_name_output</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> </span><span class="token parameter variable" style="color:#36acaa">--region</span><span class="token plain"> </span><span class="token variable" style="color:#36acaa">$AWS_REGION</span><span class="token plain"> </span><span class="token parameter variable" style="color:#36acaa">--query</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;SecretString&quot;</span><span class="token plain"> </span><span class="token parameter variable" style="color:#36acaa">--output</span><span class="token plain"> text</span><br></span></code></pre></div></div><p><strong>2. Expose the Grafana Service</strong></p><p>Use port-forward to expose the Grafana service.</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl port-forward svc/kube-prometheus-stack-grafana </span><span class="token number" style="color:#36acaa">3000</span><span class="token plain">:80 </span><span class="token parameter variable" style="color:#36acaa">-n</span><span class="token plain"> monitoring</span><br></span></code></pre></div></div><p><strong>3. Login to Grafana:</strong></p><ul>
<li>Open your web browser and navigate to <a href="http://localhost:3000" target="_blank" rel="noopener noreferrer">http://localhost:3000</a>.</li>
<li>Login with the username <code>admin</code> and the password retrieved from AWS Secrets Manager.</li>
</ul><p><strong>4. Open the NIM Monitoring Dashboard:</strong></p><ul>
<li>Once logged in, click &quot;Dashboards&quot; on the left sidebar and search &quot;nim&quot;</li>
<li>You can find the Dashboard <code>NVIDIA NIM Monitoring</code> from the list</li>
<li>Click and entering to the dashboard.</li>
</ul><p>You should now see the metrics displayed on the Grafana dashboard, allowing you to monitor the performance your NVIDIA NIM service deployment.</p></div></div></details>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</div><div class="admonitionContent_BuS1"><p>As of writing this guide, NVIDIA also provides an example Grafana dashboard. You can check it from <a href="https://docs.nvidia.com/nim/large-language-models/latest/observability.html#grafana" target="_blank" rel="noopener noreferrer">here</a>.</p></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>This blueprint showcases how to deploy and scale <strong>large language models like Meta’s Llama 3.1 8B Instruct</strong> efficiently on <strong>Amazon EKS</strong> using the <strong>NVIDIA NIM Operator</strong>.</p>
<p>By combining <strong>OpenAI-compatible APIs</strong> with <strong>GPU-accelerated inference</strong>, <strong>declarative Kubernetes CRDs</strong> (<code>NIMCache</code>, <code>NIMService</code>), and <strong>fast model startup via EFS-based caching</strong>, you get a streamlined, production-grade model deployment experience.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="key-benefits">Key Benefits:<a href="#key-benefits" class="hash-link" aria-label="Direct link to Key Benefits:" title="Direct link to Key Benefits:">​</a></h3>
<ul>
<li><strong>Faster cold starts</strong> through shared, persistent model cache</li>
<li><strong>Declarative and repeatable deployments</strong> with CRDs</li>
<li><strong>Dynamic GPU autoscaling</strong> powered by Karpenter</li>
<li><strong>One-click infrastructure provisioning</strong> using Terraform</li>
</ul>
<p>In just <strong>~20 minutes</strong>, you can go from zero to a <strong>scalable LLM service on Kubernetes</strong> — ready to serve real-world prompts with low latency and high efficiency.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="cleanup">Cleanup<a href="#cleanup" class="hash-link" aria-label="Direct link to Cleanup" title="Direct link to Cleanup">​</a></h2>
<p>To tear down the deployed model and associated infrastructure:</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-1-delete-model-resources">Step 1: Delete Model Resources<a href="#step-1-delete-model-resources" class="hash-link" aria-label="Direct link to Step 1: Delete Model Resources" title="Direct link to Step 1: Delete Model Resources">​</a></h3>
<p>Delete the deployed <code>NIMService</code> and <code>NIMCache</code> objects from your cluster:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token builtin class-name">cd</span><span class="token plain"> blueprints/inference/gpu/nvidia-nim-operator-llama3-8b</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kubectl delete </span><span class="token parameter variable" style="color:#36acaa">-f</span><span class="token plain"> nim-service-llama3-8b-instruct.yaml</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kubectl delete </span><span class="token parameter variable" style="color:#36acaa">-f</span><span class="token plain"> nim-cache-llama3-8b-instruct.yaml</span><br></span></code></pre></div></div>
<p><strong>Verify deletion:</strong></p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl get nimservices.apps.nvidia.com -n nim-service</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kubectl get nimcaches.apps.nvidia.com -n nim-service</span><br></span></code></pre></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-2-destroy-aws-infrastructure">Step 2: Destroy AWS Infrastructure<a href="#step-2-destroy-aws-infrastructure" class="hash-link" aria-label="Direct link to Step 2: Destroy AWS Infrastructure" title="Direct link to Step 2: Destroy AWS Infrastructure">​</a></h3>
<p>Navigate back to the root Terraform module and run the cleanup script. This will destroy all AWS resources created for this blueprint, including the VPC, EKS cluster, EFS, and node groups:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token builtin class-name">cd</span><span class="token plain"> ai-on-eks/infra/nvidia-nim</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">./cleanup.sh</span><br></span></code></pre></div></div></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/awslabs/ai-on-eks/blob/main/website/docs/blueprints/inference/GPUs/nvidia-nim-operator.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-nim-llama3"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">NVIDIA NIM LLM on Amazon EKS</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ai-on-eks/docs/blueprints/inference/GPUs/ray-vllm-deepseek"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">DeepSeek-R1 on EKS</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#what-is-nvidia-nim" class="table-of-contents__link toc-highlight">What is NVIDIA NIM?</a></li><li><a href="#nvidia-nim-operator-for-kubernetes" class="table-of-contents__link toc-highlight">NVIDIA NIM Operator for Kubernetes</a><ul><li><a href="#nimcache--model-caching-for-faster-load-times" class="table-of-contents__link toc-highlight">NIMCache – Model Caching for Faster Load Times</a></li><li><a href="#nimservice--deploying-and-managing-the-model-server" class="table-of-contents__link toc-highlight">NIMService – Deploying and Managing the Model Server</a></li><li><a href="#nimpipeline" class="table-of-contents__link toc-highlight">NIMPipeline</a></li></ul></li><li><a href="#overview-of-this-deployment-pattern-on-amazon-eks" class="table-of-contents__link toc-highlight">Overview of this deployment pattern on Amazon EKS</a></li><li><a href="#deploying-the-solution" class="table-of-contents__link toc-highlight">Deploying the Solution</a><ul><li><a href="#prerequisites" class="table-of-contents__link toc-highlight">Prerequisites</a></li><li><a href="#deploy" class="table-of-contents__link toc-highlight">Deploy</a></li><li><a href="#deploy-llama-31-8b-instruct-with-nim-operator" class="table-of-contents__link toc-highlight">Deploy llama-3.1-8b-instruct with NIM Operator</a></li><li><a href="#-model-startup-timeline" class="table-of-contents__link toc-highlight">🚀 Model Startup Timeline</a></li><li><a href="#test-the-model-with-a-prompt" class="table-of-contents__link toc-highlight">Test the Model with a Prompt</a></li></ul></li><li><a href="#open-webui-deployment" class="table-of-contents__link toc-highlight">Open WebUI Deployment</a></li><li><a href="#performance-testing-with-nvidia-genai-perf-tool" class="table-of-contents__link toc-highlight">Performance Testing with NVIDIA GenAI-Perf Tool</a><ul><li><a href="#grafana-dashboard" class="table-of-contents__link toc-highlight">Grafana Dashboard</a></li></ul></li><li><a href="#conclusion" class="table-of-contents__link toc-highlight">Conclusion</a><ul><li><a href="#key-benefits" class="table-of-contents__link toc-highlight">Key Benefits:</a></li></ul></li><li><a href="#cleanup" class="table-of-contents__link toc-highlight">Cleanup</a><ul><li><a href="#step-1-delete-model-resources" class="table-of-contents__link toc-highlight">Step 1: Delete Model Resources</a></li><li><a href="#step-2-destroy-aws-infrastructure" class="table-of-contents__link toc-highlight">Step 2: Destroy AWS Infrastructure</a></li></ul></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Get Involved</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/awslabs/ai-on-eks" target="_blank" rel="noopener noreferrer" class="footer__link-item">Github<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Built with ❤️ at AWS  <br> © 2025 Amazon.com, Inc. or its affiliates. All Rights Reserved</div></div></div></footer><script defer="" src="https://static.cloudflareinsights.com/beacon.min.js" data-cf-beacon="{&quot;token&quot;: &quot;7fbc7ab02fae4767b1af2588eba0cdf2&quot;}"></script></div>
</body>
</html>