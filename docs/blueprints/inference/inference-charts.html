<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-blueprints/inference/inference-charts" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.8.1">
<title data-rh="true">AI on EKS Inference Charts | AI on EKS</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://awslabs.github.io/ai-on-eks/docs/blueprints/inference/inference-charts"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="AI on EKS Inference Charts | AI on EKS"><meta data-rh="true" name="description" content="The AI on EKS Inference Charts provide a streamlined Helm-based approach to deploy AI/ML inference workloads on both GPU"><meta data-rh="true" property="og:description" content="The AI on EKS Inference Charts provide a streamlined Helm-based approach to deploy AI/ML inference workloads on both GPU"><link data-rh="true" rel="icon" href="/ai-on-eks/img/header-icon.png"><link data-rh="true" rel="canonical" href="https://awslabs.github.io/ai-on-eks/docs/blueprints/inference/inference-charts"><link data-rh="true" rel="alternate" href="https://awslabs.github.io/ai-on-eks/docs/blueprints/inference/inference-charts" hreflang="en"><link data-rh="true" rel="alternate" href="https://awslabs.github.io/ai-on-eks/docs/blueprints/inference/inference-charts" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Inference on EKS","item":"https://awslabs.github.io/ai-on-eks/docs/category/inference-on-eks"},{"@type":"ListItem","position":2,"name":"Inference Charts","item":"https://awslabs.github.io/ai-on-eks/docs/blueprints/inference/inference-charts"}]}</script><link rel="stylesheet" href="/ai-on-eks/assets/css/styles.f2fe9425.css">
<script src="/ai-on-eks/assets/js/runtime~main.d157f65d.js" defer="defer"></script>
<script src="/ai-on-eks/assets/js/main.6c2f188d.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/ai-on-eks/"><div class="navbar__logo"><img src="/ai-on-eks/img/header-icon.png" alt="AIoEKS Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/ai-on-eks/img/header-icon.png" alt="AIoEKS Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div></a><a class="navbar__item navbar__link" href="/ai-on-eks/docs/infra/ai-ml">Infrastructure</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/ai-on-eks/docs/blueprints">Blueprints</a><a class="navbar__item navbar__link" href="/ai-on-eks/docs/resources/intro">Resources</a><a class="navbar__item navbar__link" href="/ai-on-eks/docs/guidance">Guidance</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/awslabs/ai-on-eks" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><div class="navbar__search"><span aria-label="expand searchbar" role="button" class="search-icon" tabindex="0"></span><input id="search_input_react" type="search" placeholder="Loading..." aria-label="Search" class="navbar__search-input search-bar" disabled=""></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai-on-eks/docs/blueprints">Overview</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/ai-on-eks/docs/category/training-on-eks">Training on EKS</a><button aria-label="Expand sidebar category &#x27;Training on EKS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/ai-on-eks/docs/category/inference-on-eks">Inference on EKS</a><button aria-label="Collapse sidebar category &#x27;Inference on EKS&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai-on-eks/docs/category/gpu-inference-on-eks">GPU Inference on EKS</a><button aria-label="Expand sidebar category &#x27;GPU Inference on EKS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai-on-eks/docs/category/neuron-inference-on-eks">Neuron Inference on EKS</a><button aria-label="Expand sidebar category &#x27;Neuron Inference on EKS&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-on-eks/docs/blueprints/inference">Overview</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ai-on-eks/docs/blueprints/inference/inference-charts">Inference Charts</a></li></ul></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/ai-on-eks/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/ai-on-eks/docs/category/inference-on-eks"><span>Inference on EKS</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Inference Charts</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>AI on EKS Inference Charts</h1></header>
<p>The AI on EKS Inference Charts provide a streamlined Helm-based approach to deploy AI/ML inference workloads on both GPU
and AWS Neuron (Inferentia/Trainium) hardware. This chart supports multiple deployment configurations and comes with
pre-configured values for popular models.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="overview">Overview<a href="#overview" class="hash-link" aria-label="Direct link to Overview" title="Direct link to Overview">​</a></h2>
<p>The inference charts support the following deployment types:</p>
<ul>
<li><strong>GPU-based VLLM deployments</strong> - Single-node VLLM inference</li>
<li><strong>GPU-based Ray-VLLM deployments</strong> - Distributed VLLM inference with Ray</li>
<li><strong>Neuron-based VLLM deployments</strong> - VLLM inference on AWS Inferentia chips</li>
<li><strong>Neuron-based Ray-VLLM deployments</strong> - Distributed VLLM inference with Ray on Inferentia</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="prerequisites">Prerequisites<a href="#prerequisites" class="hash-link" aria-label="Direct link to Prerequisites" title="Direct link to Prerequisites">​</a></h2>
<p>Before deploying the inference charts, ensure you have:</p>
<ul>
<li>Amazon EKS cluster with GPU or AWS Neuron nodes (<a href="/ai-on-eks/docs/infra/ai-ml/jark">JARK-stack</a> for a quick start)</li>
<li>Helm 3.0+</li>
<li>For GPU deployments: NVIDIA device plugin installed</li>
<li>For Neuron deployments: AWS Neuron device plugin installed</li>
<li>Hugging Face Hub token (stored as a Kubernetes secret)</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="quick-start">Quick Start<a href="#quick-start" class="hash-link" aria-label="Direct link to Quick Start" title="Direct link to Quick Start">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="1-create-hugging-face-token-secret">1. Create Hugging Face Token Secret<a href="#1-create-hugging-face-token-secret" class="hash-link" aria-label="Direct link to 1. Create Hugging Face Token Secret" title="Direct link to 1. Create Hugging Face Token Secret">​</a></h3>
<p>Create a Kubernetes secret with your <a href="https://huggingface.co/docs/hub/en/security-tokens" target="_blank" rel="noopener noreferrer">Hugging Face token</a>:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl create secret generic hf-token --from-literal</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">token</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">your_huggingface_token</span><br></span></code></pre></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="2-deploy-a-pre-configured-model">2. Deploy a Pre-configured Model<a href="#2-deploy-a-pre-configured-model" class="hash-link" aria-label="Direct link to 2. Deploy a Pre-configured Model" title="Direct link to 2. Deploy a Pre-configured Model">​</a></h3>
<p>Choose from the available pre-configured models and deploy:</p>
<div class="theme-admonition theme-admonition-warning admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>warning</div><div class="admonitionContent_BuS1"><p>These deployments will need GPU/Neuron resources which need to
be <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-resource-limits.html" target="_blank" rel="noopener noreferrer">enabled</a> and cost more than CPU only
instances.</p></div></div>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># Deploy Llama 3.2 1B on GPU with VLLM</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">helm </span><span class="token function" style="color:#d73a49">install</span><span class="token plain"> llama-inference ./blueprints/inference/inference-charts </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token parameter variable" style="color:#36acaa">--values</span><span class="token plain"> ./blueprints/inference/inference-charts/values-llama-32-1b-vllm.yaml</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Deploy DeepSeek R1 Distill on GPU with Ray-VLLM</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">helm </span><span class="token function" style="color:#d73a49">install</span><span class="token plain"> deepseek-inference ./blueprints/inference/inference-charts </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token parameter variable" style="color:#36acaa">--values</span><span class="token plain"> ./blueprints/inference/inference-charts/values-deepseek-r1-distill-llama-8b-ray-vllm-gpu.yaml</span><br></span></code></pre></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="supported-models">Supported Models<a href="#supported-models" class="hash-link" aria-label="Direct link to Supported Models" title="Direct link to Supported Models">​</a></h2>
<p>The inference charts include pre-configured values files for the following models:</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="gpu-models">GPU Models<a href="#gpu-models" class="hash-link" aria-label="Direct link to GPU Models" title="Direct link to GPU Models">​</a></h3>
<table><thead><tr><th>Model</th><th>Size</th><th>Framework</th><th>Values File</th></tr></thead><tbody><tr><td><strong>DeepSeek R1 Distill Llama</strong></td><td>8B</td><td>Ray-VLLM</td><td><code>values-deepseek-r1-distill-llama-8b-ray-vllm-gpu.yaml</code></td></tr><tr><td><strong>Llama 3.2</strong></td><td>1B</td><td>VLLM</td><td><code>values-llama-32-1b-vllm.yaml</code></td></tr><tr><td><strong>Llama 3.2</strong></td><td>1B</td><td>Ray-VLLM</td><td><code>values-llama-32-1b-ray-vllm.yaml</code></td></tr><tr><td><strong>Llama 4 Scout</strong></td><td>17B</td><td>VLLM</td><td><code>values-llama-4-scout-17b-vllm.yaml</code></td></tr><tr><td><strong>Mistral Small</strong></td><td>24B</td><td>Ray-VLLM</td><td><code>values-mistral-small-24b-ray-vllm.yaml</code></td></tr></tbody></table>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="neuron-models-aws-inferentiatrainium">Neuron Models (AWS Inferentia/Trainium)<a href="#neuron-models-aws-inferentiatrainium" class="hash-link" aria-label="Direct link to Neuron Models (AWS Inferentia/Trainium)" title="Direct link to Neuron Models (AWS Inferentia/Trainium)">​</a></h3>
<table><thead><tr><th>Model</th><th>Size</th><th>Framework</th><th>Values File</th></tr></thead><tbody><tr><td><strong>DeepSeek R1 Distill Llama</strong></td><td>8B</td><td>VLLM</td><td><code>values-deepseek-r1-distill-llama-8b-vllm-neuron.yaml</code></td></tr><tr><td><strong>Llama 2</strong></td><td>13B</td><td>Ray-VLLM</td><td><code>values-llama-2-13b-ray-vllm-neuron.yaml</code></td></tr><tr><td><strong>Llama 3</strong></td><td>70B</td><td>Ray-VLLM</td><td><code>values-llama-3-70b-ray-vllm-neuron.yaml</code></td></tr><tr><td><strong>Llama 3.1</strong></td><td>8B</td><td>VLLM</td><td><code>values-llama-31-8b-vllm-neuron.yaml</code></td></tr><tr><td><strong>Llama 3.1</strong></td><td>8B</td><td>Ray-VLLM</td><td><code>values-llama-31-8b-ray-vllm-neuron.yaml</code></td></tr></tbody></table>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="deployment-examples">Deployment Examples<a href="#deployment-examples" class="hash-link" aria-label="Direct link to Deployment Examples" title="Direct link to Deployment Examples">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="gpu-deployments">GPU Deployments<a href="#gpu-deployments" class="hash-link" aria-label="Direct link to GPU Deployments" title="Direct link to GPU Deployments">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="deploy-llama-32-1b-with-vllm">Deploy Llama 3.2 1B with VLLM<a href="#deploy-llama-32-1b-with-vllm" class="hash-link" aria-label="Direct link to Deploy Llama 3.2 1B with VLLM" title="Direct link to Deploy Llama 3.2 1B with VLLM">​</a></h4>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">helm </span><span class="token function" style="color:#d73a49">install</span><span class="token plain"> llama32-vllm ./blueprints/inference/inference-charts </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token parameter variable" style="color:#36acaa">--values</span><span class="token plain"> ./blueprints/inference/inference-charts/values-llama-32-1b-vllm.yaml</span><br></span></code></pre></div></div>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="deploy-deepseek-r1-distill-with-ray-vllm">Deploy DeepSeek R1 Distill with Ray-VLLM<a href="#deploy-deepseek-r1-distill-with-ray-vllm" class="hash-link" aria-label="Direct link to Deploy DeepSeek R1 Distill with Ray-VLLM" title="Direct link to Deploy DeepSeek R1 Distill with Ray-VLLM">​</a></h4>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">helm </span><span class="token function" style="color:#d73a49">install</span><span class="token plain"> deepseek-ray ./blueprints/inference/inference-charts </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token parameter variable" style="color:#36acaa">--values</span><span class="token plain"> ./blueprints/inference/inference-charts/values-deepseek-r1-distill-llama-8b-ray-vllm-gpu.yaml</span><br></span></code></pre></div></div>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="deploy-mistral-small-24b-with-ray-vllm">Deploy Mistral Small 24B with Ray-VLLM<a href="#deploy-mistral-small-24b-with-ray-vllm" class="hash-link" aria-label="Direct link to Deploy Mistral Small 24B with Ray-VLLM" title="Direct link to Deploy Mistral Small 24B with Ray-VLLM">​</a></h4>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">helm </span><span class="token function" style="color:#d73a49">install</span><span class="token plain"> mistral-ray ./blueprints/inference/inference-charts </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token parameter variable" style="color:#36acaa">--values</span><span class="token plain"> ./blueprints/inference/inference-charts/values-mistral-small-24b-ray-vllm.yaml</span><br></span></code></pre></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="neuron-deployments">Neuron Deployments<a href="#neuron-deployments" class="hash-link" aria-label="Direct link to Neuron Deployments" title="Direct link to Neuron Deployments">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="deploy-llama-31-8b-with-vllm-on-inferentia">Deploy Llama 3.1 8B with VLLM on Inferentia<a href="#deploy-llama-31-8b-with-vllm-on-inferentia" class="hash-link" aria-label="Direct link to Deploy Llama 3.1 8B with VLLM on Inferentia" title="Direct link to Deploy Llama 3.1 8B with VLLM on Inferentia">​</a></h4>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">helm </span><span class="token function" style="color:#d73a49">install</span><span class="token plain"> llama31-neuron ./blueprints/inference/inference-charts </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token parameter variable" style="color:#36acaa">--values</span><span class="token plain"> ./blueprints/inference/inference-charts/values-llama-31-8b-vllm-neuron.yaml</span><br></span></code></pre></div></div>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="deploy-llama-3-70b-with-ray-vllm-on-inferentia">Deploy Llama 3 70B with Ray-VLLM on Inferentia<a href="#deploy-llama-3-70b-with-ray-vllm-on-inferentia" class="hash-link" aria-label="Direct link to Deploy Llama 3 70B with Ray-VLLM on Inferentia" title="Direct link to Deploy Llama 3 70B with Ray-VLLM on Inferentia">​</a></h4>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">helm </span><span class="token function" style="color:#d73a49">install</span><span class="token plain"> llama3-70b-neuron ./blueprints/inference/inference-charts </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token parameter variable" style="color:#36acaa">--values</span><span class="token plain"> ./blueprints/inference/inference-charts/values-llama-3-70b-ray-vllm-neuron.yaml</span><br></span></code></pre></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="configuration-options">Configuration Options<a href="#configuration-options" class="hash-link" aria-label="Direct link to Configuration Options" title="Direct link to Configuration Options">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="key-parameters">Key Parameters<a href="#key-parameters" class="hash-link" aria-label="Direct link to Key Parameters" title="Direct link to Key Parameters">​</a></h3>
<p>The chart provides extensive configuration options. Here are the most important parameters:</p>
<table><thead><tr><th>Parameter</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>inference.accelerator</code></td><td>Accelerator type (<code>gpu</code> or <code>neuron</code>)</td><td><code>gpu</code></td></tr><tr><td><code>inference.framework</code></td><td>Framework type (<code>vllm</code> or <code>ray-vllm</code>)</td><td><code>vllm</code></td></tr><tr><td><code>inference.serviceName</code></td><td>Name of the inference service</td><td><code>inference</code></td></tr><tr><td><code>inference.modelServer.deployment.replicas</code></td><td>Number of replicas</td><td><code>1</code></td></tr><tr><td><code>modelParameters.modelId</code></td><td>Model ID from Hugging Face Hub</td><td><code>NousResearch/Llama-3.2-1B</code></td></tr><tr><td><code>modelParameters.gpuMemoryUtilization</code></td><td>GPU memory utilization</td><td><code>0.8</code></td></tr><tr><td><code>modelParameters.maxModelLen</code></td><td>Maximum model sequence length</td><td><code>8192</code></td></tr><tr><td><code>modelParameters.tensorParallelSize</code></td><td>Tensor parallel size</td><td><code>1</code></td></tr><tr><td><code>service.type</code></td><td>Service type</td><td><code>ClusterIP</code></td></tr><tr><td><code>service.port</code></td><td>Service port</td><td><code>8000</code></td></tr></tbody></table>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="custom-deployment">Custom Deployment<a href="#custom-deployment" class="hash-link" aria-label="Direct link to Custom Deployment" title="Direct link to Custom Deployment">​</a></h3>
<p>Create your own values file for custom configurations:</p>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token key atrule" style="color:#00a4db">inference</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">accelerator</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> gpu  </span><span class="token comment" style="color:#999988;font-style:italic"># or neuron</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">framework</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> vllm   </span><span class="token comment" style="color:#999988;font-style:italic"># or ray-vllm</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">serviceName</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> custom</span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain">inference</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">modelServer</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token key atrule" style="color:#00a4db">deployment</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token key atrule" style="color:#00a4db">replicas</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">2</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token key atrule" style="color:#00a4db">resources</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token key atrule" style="color:#00a4db">gpu</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          </span><span class="token key atrule" style="color:#00a4db">requests</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token key atrule" style="color:#00a4db">nvidia.com/gpu</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          </span><span class="token key atrule" style="color:#00a4db">limits</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token key atrule" style="color:#00a4db">nvidia.com/gpu</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token key atrule" style="color:#00a4db">modelParameters</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">modelId</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;your-custom-model-id&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">gpuMemoryUtilization</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;0.9&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">maxModelLen</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;4096&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token key atrule" style="color:#00a4db">tensorParallelSize</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;1&quot;</span><br></span></code></pre></div></div>
<p>Deploy with custom values:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">helm </span><span class="token function" style="color:#d73a49">install</span><span class="token plain"> custom-inference ./blueprints/inference/inference-charts </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token parameter variable" style="color:#36acaa">--values</span><span class="token plain"> custom-values.yaml</span><br></span></code></pre></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="api-endpoints">API Endpoints<a href="#api-endpoints" class="hash-link" aria-label="Direct link to API Endpoints" title="Direct link to API Endpoints">​</a></h2>
<p>Once deployed, the service exposes OpenAI-compatible API endpoints:</p>
<ul>
<li><strong><code>/v1/models</code></strong> - List available models</li>
<li><strong><code>/v1/completions</code></strong> - Text completion API</li>
<li><strong><code>/v1/chat/completions</code></strong> - Chat completion API</li>
<li><strong><code>/metrics</code></strong> - Prometheus metrics endpoint</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="example-api-usage">Example API Usage<a href="#example-api-usage" class="hash-link" aria-label="Direct link to Example API Usage" title="Direct link to Example API Usage">​</a></h3>
<p>Note: These deployments do not create an ingress, you will need to <code>kubectl port-forward</code> to test from your machine,
eg (for deepseek):</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl get svc </span><span class="token operator" style="color:#393A34">|</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">grep</span><span class="token plain"> deepseek</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Note the service name for deepseek, in this case deepseekr1-dis-lllama-8b-ray-vllm-gpu-ray-vllm</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kubectl port-forward svc/deepseekr1-dis-llama-8b-ray-vllm-gpu-ray-vllm </span><span class="token number" style="color:#36acaa">8000</span><br></span></code></pre></div></div>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># List models</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token function" style="color:#d73a49">curl</span><span class="token plain"> http://localhost:8000/v1/models</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Chat completion</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token function" style="color:#d73a49">curl</span><span class="token plain"> </span><span class="token parameter variable" style="color:#36acaa">-X</span><span class="token plain"> POST http://localhost:8000/v1/chat/completions </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token parameter variable" style="color:#36acaa">-H</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;Content-Type: application/json&quot;</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token parameter variable" style="color:#36acaa">-d</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&#x27;{</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">    &quot;model&quot;: &quot;your-model-name&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">    &quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hello!&quot;}],</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">    &quot;max_tokens&quot;: 100</span><br></span><span class="token-line" style="color:#393A34"><span class="token string" style="color:#e3116c">  }&#x27;</span><br></span></code></pre></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="monitoring-and-observability">Monitoring and Observability<a href="#monitoring-and-observability" class="hash-link" aria-label="Direct link to Monitoring and Observability" title="Direct link to Monitoring and Observability">​</a></h2>
<p>The charts include built-in observability features:</p>
<ul>
<li><strong>Fluent Bit</strong> for log collection</li>
<li><strong>Prometheus metrics</strong> for monitoring</li>
<li><strong>Grafana dashboards</strong> for visualizations</li>
</ul>
<p>Access metrics at the <code>/metrics</code> endpoint of your deployed service.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="troubleshooting">Troubleshooting<a href="#troubleshooting" class="hash-link" aria-label="Direct link to Troubleshooting" title="Direct link to Troubleshooting">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="common-issues">Common Issues<a href="#common-issues" class="hash-link" aria-label="Direct link to Common Issues" title="Direct link to Common Issues">​</a></h3>
<ol>
<li>
<p><strong>Pod stuck in Pending state</strong></p>
<ul>
<li>Check if GPU/Neuron nodes are available</li>
<li>Verify resource requests match available hardware</li>
</ul>
</li>
<li>
<p><strong>Model download failures</strong></p>
<ul>
<li>Ensure Hugging Face token is correctly configured</li>
<li>Check network connectivity to Hugging Face Hub</li>
</ul>
</li>
<li>
<p><strong>Out of memory errors</strong></p>
<ul>
<li>Adjust <code>gpuMemoryUtilization</code> parameter</li>
<li>Consider using tensor parallelism for larger models</li>
</ul>
</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="logs">Logs<a href="#logs" class="hash-link" aria-label="Direct link to Logs" title="Direct link to Logs">​</a></h3>
<p>Check deployment logs:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl logs </span><span class="token parameter variable" style="color:#36acaa">-l</span><span class="token plain"> </span><span class="token assign-left variable" style="color:#36acaa">app</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">inference-server</span><br></span></code></pre></div></div>
<p>For Ray deployments, check Ray cluster status:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl </span><span class="token builtin class-name">exec</span><span class="token plain"> </span><span class="token parameter variable" style="color:#36acaa">-it</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain">ray-head-pod</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> -- ray status</span><br></span></code></pre></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="next-steps">Next Steps<a href="#next-steps" class="hash-link" aria-label="Direct link to Next Steps" title="Direct link to Next Steps">​</a></h2>
<ul>
<li>Explore <a href="/ai-on-eks/docs/category/gpu-inference-on-eks">GPU-specific configurations</a> for GPU deployments</li>
<li>Learn about <a href="/ai-on-eks/docs/category/neuron-inference-on-eks">Neuron-specific configurations</a> for Inferentia deployments</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/awslabs/ai-on-eks/blob/main/website/docs/blueprints/inference/inference-charts.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/ai-on-eks/docs/blueprints/inference"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Overview</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#overview" class="table-of-contents__link toc-highlight">Overview</a></li><li><a href="#prerequisites" class="table-of-contents__link toc-highlight">Prerequisites</a></li><li><a href="#quick-start" class="table-of-contents__link toc-highlight">Quick Start</a><ul><li><a href="#1-create-hugging-face-token-secret" class="table-of-contents__link toc-highlight">1. Create Hugging Face Token Secret</a></li><li><a href="#2-deploy-a-pre-configured-model" class="table-of-contents__link toc-highlight">2. Deploy a Pre-configured Model</a></li></ul></li><li><a href="#supported-models" class="table-of-contents__link toc-highlight">Supported Models</a><ul><li><a href="#gpu-models" class="table-of-contents__link toc-highlight">GPU Models</a></li><li><a href="#neuron-models-aws-inferentiatrainium" class="table-of-contents__link toc-highlight">Neuron Models (AWS Inferentia/Trainium)</a></li></ul></li><li><a href="#deployment-examples" class="table-of-contents__link toc-highlight">Deployment Examples</a><ul><li><a href="#gpu-deployments" class="table-of-contents__link toc-highlight">GPU Deployments</a></li><li><a href="#neuron-deployments" class="table-of-contents__link toc-highlight">Neuron Deployments</a></li></ul></li><li><a href="#configuration-options" class="table-of-contents__link toc-highlight">Configuration Options</a><ul><li><a href="#key-parameters" class="table-of-contents__link toc-highlight">Key Parameters</a></li><li><a href="#custom-deployment" class="table-of-contents__link toc-highlight">Custom Deployment</a></li></ul></li><li><a href="#api-endpoints" class="table-of-contents__link toc-highlight">API Endpoints</a><ul><li><a href="#example-api-usage" class="table-of-contents__link toc-highlight">Example API Usage</a></li></ul></li><li><a href="#monitoring-and-observability" class="table-of-contents__link toc-highlight">Monitoring and Observability</a></li><li><a href="#troubleshooting" class="table-of-contents__link toc-highlight">Troubleshooting</a><ul><li><a href="#common-issues" class="table-of-contents__link toc-highlight">Common Issues</a></li><li><a href="#logs" class="table-of-contents__link toc-highlight">Logs</a></li></ul></li><li><a href="#next-steps" class="table-of-contents__link toc-highlight">Next Steps</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Get Involved</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/awslabs/ai-on-eks" target="_blank" rel="noopener noreferrer" class="footer__link-item">Github<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Built with ❤️ at AWS  <br> © 2025 Amazon.com, Inc. or its affiliates. All Rights Reserved</div></div></div></footer><script defer="" src="https://static.cloudflareinsights.com/beacon.min.js" data-cf-beacon="{&quot;token&quot;: &quot;7fbc7ab02fae4767b1af2588eba0cdf2&quot;}"></script></div>
</body>
</html>