{"searchDocs":[{"title":"AI on EKS","type":0,"sectionRef":"#","url":"/ai-on-eks/docs/blueprints","content":"","keywords":"","version":"Next"},{"title":"Base Infrastructure‚Äã","type":1,"pageTitle":"AI on EKS","url":"/ai-on-eks/docs/blueprints#base-infrastructure","content":" At its core, AIoEKS is a set of modules that can be composed to create the environment in which you want to work. We have some blueprints that will enable you to quickly start using the environment for either experimentation, training or inference. We intend to add more as they come up, but we also enable you to compose the environment however you would like. This allows you to build the environment you want to use on EKS.  If you're ready to deploy your EKS cluster, check out the infrastructure section.  ","version":"Next","tagName":"h2"},{"title":"Training‚Äã","type":1,"pageTitle":"AI on EKS","url":"/ai-on-eks/docs/blueprints#training","content":" Training in the generative AI world involves teaching a model to understand and generate human-like text by learning from vast amounts of data. This process is crucial as it forms the foundation upon which models like BERT-Large and Llama2 are built, enabling them to perform a wide range of tasks such as text summarization, translation, and more. Effective training ensures that the model can understand complex patterns and generate coherent, contextually appropriate responses.  Our platform supports various ML frameworks including PyTorch, TensorFlow, TensorRT, and vLLM. You can use JupyterHub for interactive and collaborative model development, where you can perform data analysis, build models, and run experiments. The training phase also integrates with Kubeflow and Ray, providing robust solutions for managing complex machine learning workflows, from data preprocessing to training and evaluation.  Are you ready to dive into the world of LLMs and train models for your specific needs? Discover our comprehensive Training resources to get started.  ","version":"Next","tagName":"h2"},{"title":"Inference‚Äã","type":1,"pageTitle":"AI on EKS","url":"/ai-on-eks/docs/blueprints#inference","content":" Inference is the process of using a trained model to make predictions or generate outputs based on new input data. In the context of generative AI, inference enables models to perform tasks such as text generation, translation, and summarization in real-time. Building a scalable inference platform is essential to handle the high demand and ensure low latency, which is critical for applications requiring real-time responses.  Unlock the potential of LLMs for powerful inference tasks. Our Inference resources will guide you through deploying LLMs effectively. Utilize deployment tools such as RayServe, NVIDIA Triton Inference Server, and KServe to ensure high-performance model serving. We also offer optimization techniques with AWS Neuron for Inferentia and NVIDIA GPUs to accelerate inference. This section includes step-by-step instructions on setting up inference endpoints, scaling deployments to handle varying loads, and monitoring performance to maintain reliability and efficiency.  ","version":"Next","tagName":"h2"},{"title":"Storage and Data Management‚Äã","type":1,"pageTitle":"AI on EKS","url":"/ai-on-eks/docs/blueprints#storage-and-data-management","content":" Efficient data storage and management are fundamental to successful AI/ML operations. Our platform integrates with AWS storage solutions such as S3, EBS, EFS, and FSx to ensure scalable and reliable data handling. Utilize MLflow for model registry and versioning, and manage container images with Amazon ECR. This ensures a seamless workflow from model development to deployment, with robust data management practices to support your ML lifecycle.  Whether you're an experienced practitioner or new to the field, our AI on EKS capabilities empower you to harness the latest advancements in language modeling. Dive into each section to begin your journey, and explore how you can leverage these tools and frameworks to build, fine-tune, and deploy powerful AI models on Amazon EKS. ","version":"Next","tagName":"h2"},{"title":"AIBrix","type":0,"sectionRef":"#","url":"/ai-on-eks/docs/blueprints/inference/GPUs/aibrix-deepseek-distill","content":"","keywords":"","version":"Next"},{"title":"Features‚Äã","type":1,"pageTitle":"AIBrix","url":"/ai-on-eks/docs/blueprints/inference/GPUs/aibrix-deepseek-distill#features","content":" LLM Gateway and Routing: Efficiently manage and direct traffic across multiple models and replicas.High-Density LoRA Management: Streamlined support for lightweight, low-rank adaptations of models.Distributed Inference: Scalable architecture to handle large workloads across multiple nodes.LLM App-Tailored Autoscaler: Dynamically scale inference resources based on real-time demand.Unified AI Runtime: A versatile sidecar enabling metric standardization, model downloading, and management.Heterogeneous-GPU Inference: Cost-effective SLO-driven LLM inference using heterogeneous GPUs.GPU Hardware Failure Detection: Proactive detection of GPU hardware issues.  Deploying the Solution üëà  ","version":"Next","tagName":"h3"},{"title":"Checking AIBrix Installation‚Äã","type":1,"pageTitle":"AIBrix","url":"/ai-on-eks/docs/blueprints/inference/GPUs/aibrix-deepseek-distill#checking-aibrix-installation","content":" Please run the below commands to check the AIBrix installation  kubectl get pods -n aibrix-system   Wait till all the pods are in Running status.  Running a model on AiBrix system‚Äã  We will now run Deepseek-Distill-llama-8b model using AIBrix on EKS.  Please run the below command.  kubectl apply -f blueprints/inference/aibrix/deepseek-distill.yaml   This will deploy the model on deepseek-aibrix namespace. Wait for few minutes and run  kubectl get pods -n deepseek-aibrix   Wait for the pod to be in running state.  Accessing the model using gateway‚Äã  Gateway is designed to serve LLM requests and provides features such as dynamic model &amp; LoRA adapter discovery, user configuration for request count &amp; token usage budgeting, streaming and advanced routing strategies such as prefix-cache aware, heterogeneous GPU hardware. To access the model using Gateway, Please run the below command  kubectl -n envoy-gateway-system port-forward service/envoy-aibrix-system-aibrix-eg-903790dc 8888:80 &amp;   Once the port-forward is running, you can test the model by sending a request to the Gateway.  ENDPOINT=&quot;localhost:8888&quot; curl -v http://${ENDPOINT}/v1/completions \\ -H &quot;Content-Type: application/json&quot; \\ -d '{ &quot;model&quot;: &quot;deepseek-r1-distill-llama-8b&quot;, &quot;prompt&quot;: &quot;San Francisco is a&quot;, &quot;max_tokens&quot;: 128, &quot;temperature&quot;: 0 }'   Cleanup üëà  caution To avoid unwanted charges to your AWS account, delete all the AWS resources created during this deployment ","version":"Next","tagName":"h3"},{"title":"Inference on EKS","type":0,"sectionRef":"#","url":"/ai-on-eks/docs/blueprints/inference","content":"","keywords":"","version":"Next"},{"title":"Quick Start Options‚Äã","type":1,"pageTitle":"Inference on EKS","url":"/ai-on-eks/docs/blueprints/inference#quick-start-options","content":" ","version":"Next","tagName":"h2"},{"title":"üöÄ Inference Charts (Recommended)‚Äã","type":1,"pageTitle":"Inference on EKS","url":"/ai-on-eks/docs/blueprints/inference#-inference-charts-recommended","content":" Get started quickly with our pre-configured Helm charts that support multiple models and deployment patterns:  Inference Charts - Streamlined Helm-based deployments with pre-configured values for popular modelsSupports both GPU and Neuron hardwareIncludes VLLM and Ray-VLLM frameworksPre-configured for 10+ popular models including Llama, DeepSeek, and Mistral  ","version":"Next","tagName":"h3"},{"title":"Hardware-Specific Guides‚Äã","type":1,"pageTitle":"Inference on EKS","url":"/ai-on-eks/docs/blueprints/inference#hardware-specific-guides","content":" ","version":"Next","tagName":"h2"},{"title":"GPU Deployments‚Äã","type":1,"pageTitle":"Inference on EKS","url":"/ai-on-eks/docs/blueprints/inference#gpu-deployments","content":" Explore GPU-specific inference solutions:  DeepSeek-R1 with Ray and vLLMNVIDIA NIM with Llama3NVIDIA NIM OperatorvLLM with NVIDIA Triton ServervLLM with Ray ServeStable Diffusion on GPUsAIBrix with DeepSeek  ","version":"Next","tagName":"h3"},{"title":"Neuron Deployments (AWS Inferentia)‚Äã","type":1,"pageTitle":"Inference on EKS","url":"/ai-on-eks/docs/blueprints/inference#neuron-deployments-aws-inferentia","content":" Leverage AWS Inferentia chips for cost-effective inference:  Llama2 on Inferentia2Llama3 on Inferentia2Mistral 7B on Inferentia2Ray Serve High AvailabilityvLLM with Ray on Inferentia2Stable Diffusion on Inferentia2  ","version":"Next","tagName":"h3"},{"title":"Architecture Overview‚Äã","type":1,"pageTitle":"Inference on EKS","url":"/ai-on-eks/docs/blueprints/inference#architecture-overview","content":" AI on EKS inference solutions support multiple deployment patterns:  Single-node inference with vLLMDistributed inference with Ray-vLLMProduction-ready deployments with load balancingAuto-scaling capabilitiesObservability and monitoring integration  ","version":"Next","tagName":"h2"},{"title":"Choosing the Right Approach‚Äã","type":1,"pageTitle":"Inference on EKS","url":"/ai-on-eks/docs/blueprints/inference#choosing-the-right-approach","content":" Use Case\tRecommended Solution\tBenefitsQuick prototyping\tInference Charts\tPre-configured, fast deployment GPU\tGPU-specific guides\tGPU-based inference Neuron\tNeuron guides\tInferentia-based inference  ","version":"Next","tagName":"h2"},{"title":"Next Steps‚Äã","type":1,"pageTitle":"Inference on EKS","url":"/ai-on-eks/docs/blueprints/inference#next-steps","content":" Start with Inference Charts for the fastest path to deploymentExplore hardware-specific guides for optimized configurationsSet up monitoring and observability for production workloads ","version":"Next","tagName":"h2"},{"title":"NVIDIA Dynamo on Amazon EKS","type":0,"sectionRef":"#","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-dynamo","content":"","keywords":"","version":"Next"},{"title":"Quick Start‚Äã","type":1,"pageTitle":"NVIDIA Dynamo on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-dynamo#quick-start","content":" Want to get started immediately? Here's the minimal command sequence:  # 1. Clone and navigate git clone https://github.com/awslabs/ai-on-eks.git &amp;&amp; cd ai-on-eks/infra/nvidia-dynamo # 2. Deploy everything (15-30 minutes) ./install.sh # 3. Build base image and deploy inference cd ../../blueprints/inference/nvidia-dynamo source dynamo_env.sh ./build-base-image.sh vllm --push ./deploy.sh # 4. Test your deployment ./test.sh   Prerequisites: AWS CLI, kubectl, docker, terraform, earthly, python3.10+, git (detailed setup below)    ","version":"Next","tagName":"h2"},{"title":"What is NVIDIA Dynamo?‚Äã","type":1,"pageTitle":"NVIDIA Dynamo on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-dynamo#what-is-nvidia-dynamo","content":" NVIDIA Dynamo is an open-source inference framework designed to optimize performance and scalability for large language models (LLMs) and generative AI applications.  ","version":"Next","tagName":"h2"},{"title":"What is an Inference Graph?‚Äã","type":1,"pageTitle":"NVIDIA Dynamo on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-dynamo#what-is-an-inference-graph","content":" An inference graph is a computational workflow that defines how AI models process data through interconnected nodes, enabling complex multi-step AI operations like:  LLM chains: Sequential processing through multiple language modelsMultimodal processing: Combining text, image, and audio processingCustom inference pipelines: Tailored workflows for specific AI applicationsDisaggregated serving: Separating prefill and decode phases for optimal resource utilization  ","version":"Next","tagName":"h3"},{"title":"Overview‚Äã","type":1,"pageTitle":"NVIDIA Dynamo on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-dynamo#overview","content":" This blueprint uses the official NVIDIA Dynamo Helm charts from the dynamo source repository, with additional shell scripts and Terraform automation to simplify the deployment process on Amazon EKS.  ","version":"Next","tagName":"h2"},{"title":"Deployment Approach‚Äã","type":1,"pageTitle":"NVIDIA Dynamo on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-dynamo#deployment-approach","content":" Why This Setup Process?While this implementation involves multiple steps, it provides several advantages over a simple Helm-only deployment:  Complete Infrastructure: Automatically provisions VPC, EKS cluster, ECR repositories, and monitoring stackProduction Ready: Includes enterprise-grade security, monitoring, and scalability featuresAWS Integration: Leverages EKS autoscaling, EFA networking, and AWS servicesCustomizable: Allows fine-tuning of GPU node pools, networking, and resource allocationReproducible: Infrastructure as Code ensures consistent deployments across environments  For Simpler Deployments: If you already have an EKS cluster and prefer a minimal setup, you can use the Dynamo Helm charts directly from the source repository. This blueprint provides the full production-ready experience.  As LLMs and generative AI applications become increasingly prevalent, the demand for efficient, scalable, and low-latency inference solutions has grown. Traditional inference systems often struggle to meet these demands, especially in distributed, multi-node environments. NVIDIA Dynamo addresses these challenges by offering innovative solutions to optimize performance and scalability with support for AWS services such as Amazon S3, Elastic Fabric Adapter (EFA), and Amazon EKS.  ","version":"Next","tagName":"h3"},{"title":"Key Features‚Äã","type":1,"pageTitle":"NVIDIA Dynamo on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-dynamo#key-features","content":" Performance Optimizations:  Disaggregated Serving: Separates prefill and decode phases across different GPUs for optimal resource utilizationDynamic GPU Scheduling: Intelligent resource allocation based on real-time demand through the NVIDIA Dynamo PlannerSmart Request Routing: Minimizes KV cache recomputation by routing requests to workers with relevant cached dataAccelerated Data Transfer: Low-latency communication via NVIDIA NIXL libraryEfficient KV Cache Management: Intelligent offloading across memory hierarchies with the KV Cache Block Manager  Infrastructure Ready:  Inference Engine Agnostic: Supports TensorRT-LLM, vLLM, SGLang, and other runtimesModular Design: Pick and choose components that fit your existing AI stackEnterprise Grade: Complete monitoring, logging, and security integrationAmazon EKS Optimized: Leverages EKS autoscaling, GPU support, and AWS services  ","version":"Next","tagName":"h3"},{"title":"Architecture‚Äã","type":1,"pageTitle":"NVIDIA Dynamo on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-dynamo#architecture","content":" The deployment uses Amazon EKS with the following components:    Key Components:  VPC and Networking: Standard VPC with EFA support for low-latency inter-node communicationEKS Cluster: Managed Kubernetes with GPU-enabled node groups using KarpenterDynamo Platform: Operator, API Store, and supporting services (NATS, PostgreSQL, MinIO)Monitoring Stack: Prometheus, Grafana, and AI/ML observabilityStorage: Amazon EFS for shared model storage and caching  ","version":"Next","tagName":"h2"},{"title":"Prerequisites‚Äã","type":1,"pageTitle":"NVIDIA Dynamo on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-dynamo#prerequisites","content":" System Requirements: Ubuntu 22.04 or 24.04 (NVIDIA Dynamo officially supports only these versions)  Install the following tools on your setup host (recommended: EC2 instance t3.xlarge or higher with EKS and ECR permissions):  AWS CLI: Configured with appropriate permissions (installation guide)kubectl: Kubernetes command-line tool (installation guide)helm: Kubernetes package manager (installation guide)terraform: Infrastructure as code tool (installation guide)docker: With buildx and user needs docker permissions (installation guide)earthly: Multi-platform build automation tool used by NVIDIA Dynamo for reproducible container builds (installation guide)Python 3.10+: With pip and venv (installation guide)git: Version control (installation guide)EKS Cluster: Version 1.33 (tested and supported)  Deploying the Solution üëà  ","version":"Next","tagName":"h2"},{"title":"Test and Validate‚Äã","type":1,"pageTitle":"NVIDIA Dynamo on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-dynamo#test-and-validate","content":" ","version":"Next","tagName":"h2"},{"title":"Automated Testing‚Äã","type":1,"pageTitle":"NVIDIA Dynamo on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-dynamo#automated-testing","content":" Use the built-in test script to validate your deployment:  ./test.sh   This script:  Starts port forwarding to the frontend serviceTests health check, metrics, and /v1/models endpointsRuns sample inference requests to verify functionality  ","version":"Next","tagName":"h3"},{"title":"Manual Testing‚Äã","type":1,"pageTitle":"NVIDIA Dynamo on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-dynamo#manual-testing","content":" Access your deployment directly:  kubectl port-forward svc/&lt;frontend-service&gt; 3000:3000 -n dynamo-cloud &amp; curl -X POST http://localhost:3000/v1/chat/completions \\ -H &quot;Content-Type: application/json&quot; \\ -d '{ &quot;model&quot;: &quot;deepseek-ai/DeepSeek-R1-Distill-Llama-8B&quot;, &quot;messages&quot;: [ {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Explain what a Q-Bit is in quantum computing.&quot;} ], &quot;max_tokens&quot;: 2000, &quot;temperature&quot;: 0.7, &quot;stream&quot;: false }'   Expected Output:  { &quot;id&quot;: &quot;1918b11a-6d98-4891-bc84-08f99de70fd0&quot;, &quot;choices&quot;: [ { &quot;index&quot;: 0, &quot;message&quot;: { &quot;content&quot;: &quot;A Q-bit, or qubit, is the basic unit of quantum information...&quot;, &quot;role&quot;: &quot;assistant&quot; }, &quot;finish_reason&quot;: &quot;stop&quot; } ], &quot;created&quot;: 1752018267, &quot;model&quot;: &quot;deepseek-ai/DeepSeek-R1-Distill-Llama-8B&quot;, &quot;object&quot;: &quot;chat.completion&quot; }   ","version":"Next","tagName":"h3"},{"title":"Monitor and Observe‚Äã","type":1,"pageTitle":"NVIDIA Dynamo on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-dynamo#monitor-and-observe","content":" ","version":"Next","tagName":"h2"},{"title":"Grafana Dashboard‚Äã","type":1,"pageTitle":"NVIDIA Dynamo on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-dynamo#grafana-dashboard","content":" Access Grafana for visualization (default port 3000):  kubectl port-forward -n kube-prometheus-stack svc/kube-prometheus-stack-grafana 3000:80   ","version":"Next","tagName":"h3"},{"title":"Prometheus Metrics‚Äã","type":1,"pageTitle":"NVIDIA Dynamo on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-dynamo#prometheus-metrics","content":" Access Prometheus for metrics collection (port 9090):  kubectl port-forward -n kube-prometheus-stack svc/prometheus 9090:80   ","version":"Next","tagName":"h3"},{"title":"Automatic Monitoring‚Äã","type":1,"pageTitle":"NVIDIA Dynamo on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-dynamo#automatic-monitoring","content":" The deployment automatically creates:  Service: Exposes inference graphs for API calls and metricsServiceMonitor: Configures Prometheus to scrape metricsDashboards: Pre-configured Grafana dashboards for inference monitoring  ","version":"Next","tagName":"h3"},{"title":"Advanced Configuration‚Äã","type":1,"pageTitle":"NVIDIA Dynamo on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-dynamo#advanced-configuration","content":" ","version":"Next","tagName":"h2"},{"title":"ECR Authentication‚Äã","type":1,"pageTitle":"NVIDIA Dynamo on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-dynamo#ecr-authentication","content":" The deployment uses IRSA (IAM Roles for Service Accounts) for secure ECR access:  Primary Method: IRSA eliminates credential rotationFallback Method: ECR token refresh CronJob (legacy mode)Security: Service accounts automatically authenticate to ECRNo Secrets: No long-lived credentials stored in Kubernetes  Note: AWS Pod Identity is available as an alternative to IRSA (GA since April 2024), but IRSA remains the recommended approach due to its maturity, wide adoption, and proven reliability in production environments.  ","version":"Next","tagName":"h3"},{"title":"Custom Model Deployment‚Äã","type":1,"pageTitle":"NVIDIA Dynamo on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-dynamo#custom-model-deployment","content":" To deploy custom models, modify the configuration files in dynamo/examples/llm/configs/:  Choose Architecture: Select based on model size and requirementsUpdate Configuration: Edit the appropriate YAML fileSet Model Parameters: Update model and served_model_name fieldsConfigure Resources: Adjust GPU allocation and memory settings  Example for DeepSeek-R1 70B model:  Common: model: deepseek-ai/DeepSeek-R1-Distill-Llama-70B max-model-len: 32768 tensor-parallel-size: 4 Frontend: served_model_name: deepseek-ai/DeepSeek-R1-Distill-Llama-70B VllmWorker: ServiceArgs: resources: gpu: '4'   ","version":"Next","tagName":"h3"},{"title":"Karpenter Node Pools‚Äã","type":1,"pageTitle":"NVIDIA Dynamo on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-dynamo#karpenter-node-pools","content":" The deployment can optionally use custom Karpenter node pools optimized for NVIDIA Dynamo:  C7i CPU Pools: For general compute and BuildKit (newer than base M5 instances)G6 GPU Pools: For inference workloads with NVIDIA L4 GPUsHigher Priority: Weight 100 vs base addons weight 50 for priority schedulingBuildKit Support: User namespace configuration for container buildsEFA Support: Low-latency networking for multi-node setups  Note: Custom node pools are disabled by default. The base infrastructure provides existing Karpenter node pools (G6 GPU, G5 GPU, M5 CPU) that work well for most Dynamo workloads. Enable custom pools only if you need BuildKit support or higher scheduling priority.  ","version":"Next","tagName":"h3"},{"title":"Configuration Options‚Äã","type":1,"pageTitle":"NVIDIA Dynamo on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-dynamo#configuration-options","content":" Modify terraform/blueprint.tfvars for customization:  # Enable custom node pools (optional - disabled by default) enable_custom_karpenter_nodepools = true # Choose AMI (AL2023 recommended) use_bottlerocket = false # Resource limits karpenter_cpu_limits = 10000 karpenter_memory_limits = 10000   ","version":"Next","tagName":"h3"},{"title":"Troubleshooting‚Äã","type":1,"pageTitle":"NVIDIA Dynamo on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-dynamo#troubleshooting","content":" ","version":"Next","tagName":"h2"},{"title":"Common Issues‚Äã","type":1,"pageTitle":"NVIDIA Dynamo on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-dynamo#common-issues","content":" GPU Nodes Not Available: Check Karpenter logs and instance availabilityImage Pull Errors: Verify ECR repositories and image push successPod Failures: Check resource limits and cluster capacityDeployment Timeouts: Ensure base images are built and available  ","version":"Next","tagName":"h3"},{"title":"Debug Commands‚Äã","type":1,"pageTitle":"NVIDIA Dynamo on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-dynamo#debug-commands","content":" # Check cluster status kubectl get nodes kubectl get pods -n dynamo-cloud # View logs kubectl logs -n dynamo-cloud deployment/dynamo-operator kubectl logs -n dynamo-cloud deployment/dynamo-api-store # Check deployments source dynamo_venv/bin/activate dynamo deployment list --endpoint &quot;$DYNAMO_CLOUD&quot;   ","version":"Next","tagName":"h3"},{"title":"Performance Optimization‚Äã","type":1,"pageTitle":"NVIDIA Dynamo on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-dynamo#performance-optimization","content":" Model Size Guidelines: Use appropriate architecture for model parametersResource Allocation: Match GPU count to model requirementsNetwork Configuration: Ensure EFA is enabled for multi-node setupsStorage: Use EFS for shared model storage and caching  ","version":"Next","tagName":"h3"},{"title":"Alternative Deployment Options‚Äã","type":1,"pageTitle":"NVIDIA Dynamo on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-dynamo#alternative-deployment-options","content":" ","version":"Next","tagName":"h2"},{"title":"For Existing EKS Clusters‚Äã","type":1,"pageTitle":"NVIDIA Dynamo on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-dynamo#for-existing-eks-clusters","content":" If you already have an EKS cluster with GPU nodes and prefer a simpler approach:  Direct Helm Installation: Use the official NVIDIA Dynamo Helm charts directly from the dynamo source repositoryManual Setup: Follow the upstream NVIDIA Dynamo documentation for Kubernetes deploymentCustom Integration: Integrate Dynamo components into your existing infrastructure  ","version":"Next","tagName":"h3"},{"title":"Why Use This Blueprint?‚Äã","type":1,"pageTitle":"NVIDIA Dynamo on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-dynamo#why-use-this-blueprint","content":" This blueprint is designed for users who want:  Complete Infrastructure: End-to-end setup from VPC to running inferenceProduction Readiness: Enterprise-grade monitoring, security, and scalabilityAWS Integration: Optimized for EKS, ECR, EFA, and other AWS servicesBest Practices: Follows ai-on-eks patterns and AWS recommendations  ","version":"Next","tagName":"h3"},{"title":"Repository Information‚Äã","type":1,"pageTitle":"NVIDIA Dynamo on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-dynamo#repository-information","content":" Repository: awslabs/ai-on-eksDocumentation: Complete NVIDIA Dynamo BlueprintNVIDIA Dynamo: Official DocumentationDynamo Source: NVIDIA Dynamo Repository  ","version":"Next","tagName":"h2"},{"title":"Next Steps‚Äã","type":1,"pageTitle":"NVIDIA Dynamo on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-dynamo#next-steps","content":" Explore Examples: Check the examples folder in the GitHub repositoryScale Deployments: Configure multi-node setups for larger modelsIntegrate Applications: Connect your applications to the inference endpointsMonitor Performance: Use Grafana dashboards for ongoing monitoringOptimize Costs: Implement auto-scaling and resource optimization  ","version":"Next","tagName":"h2"},{"title":"Clean Up‚Äã","type":1,"pageTitle":"NVIDIA Dynamo on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-dynamo#clean-up","content":" When you're finished with your NVIDIA Dynamo deployment, remove all resources using the cleanup script:  cd infra/nvidia-dynamo ./cleanup.sh   This safely destroys the NVIDIA Dynamo deployments and infrastructure components in the correct order, including:  Dynamo platform components and workloadsKubernetes resources and namespacesECR repositories and container imagesTerraform-managed infrastructure (EKS cluster, VPC, etc.)  The cleanup script ensures proper resource cleanup to avoid any lingering costs.  This deployment provides a production-ready NVIDIA Dynamo environment on Amazon EKS with enterprise-grade features including Karpenter automatic scaling, EFA networking, and seamless AWS service integration. ","version":"Next","tagName":"h2"},{"title":"DeepSeek-R1 on EKS with Ray and vLLM","type":0,"sectionRef":"#","url":"/ai-on-eks/docs/blueprints/inference/GPUs/ray-vllm-deepseek","content":"","keywords":"","version":"Next"},{"title":"Understanding the GPU Memory Requirements‚Äã","type":1,"pageTitle":"DeepSeek-R1 on EKS with Ray and vLLM","url":"/ai-on-eks/docs/blueprints/inference/GPUs/ray-vllm-deepseek#understanding-the-gpu-memory-requirements","content":" Deploying an 8B parameter model like DeepSeek-R1-Distill-Llama requires careful memory planning. Each model parameter typically consumes 2 bytes (BF16 precision), meaning the full model weights require around 14.99 GiB GPU memory. Below is the actual memory usage observed during deployment:  Log sample from Ray deployment  INFO model_runner.py:1115] Loading model weights took 14.99 GiB INFO worker.py:266] vLLM instance can use total GPU memory (22.30 GiB) x utilization (0.90) = 20.07 GiB INFO worker.py:266] Model weights: 14.99 GiB | Activation memory: 0.85 GiB | KV Cache: 4.17 GiB   G5 instances provide single A10G GPUs with 24 GiB memory, ideal for running one large LLM inference process per instance. For this deployment, we use G5.4xlarge, which has 1x NVIDIA A10G GPU (24 GiB), 16 vCPUs and 64 GiB RAM.  Using vLLM, we optimize memory utilization, enabling us to maximize inference speed while preventing out-of-memory (OOM) crashes.  Deploying EKS Cluster and Addons üëà  ","version":"Next","tagName":"h2"},{"title":"Deploying DeepSeek-R1-Distill-Llama-8B with RayServe and vLLM‚Äã","type":1,"pageTitle":"DeepSeek-R1 on EKS with Ray and vLLM","url":"/ai-on-eks/docs/blueprints/inference/GPUs/ray-vllm-deepseek#deploying-deepseek-r1-distill-llama-8b-with-rayserve-and-vllm","content":" With the EKS cluster deployed and all necessary components in place, we can now proceed with deploying DeepSeek-R1-Distill-Llama-8B using RayServe and vLLM. This guide outlines the steps to export the Hugging Face Hub token, create a Docker image (if required), and deploy the RayServe cluster.  Step1: Export the Hugging Face Hub Token  Before deploying the model, you need to authenticate with Hugging Face to access the required model files. Follow these steps:  Create a Hugging Face account (if you do not already have one).Generate an access token:  Navigate to Hugging Face Settings ‚Üí Access Tokens.Create a new token with read permissions.Copy the generated token.  Export the token as an environment variable in your terminal:  export HUGGING_FACE_HUB_TOKEN=$(echo -n &quot;Your-Hugging-Face-Hub-Token-Value&quot; | base64)   Note: The token must be base64-encoded before being used in Kubernetes secrets.  Step2: Create a Docker image  To deploy the model efficiently, you need a Docker image that includes Ray, vLLM, and Hugging Face dependencies. Follow these steps:  Use the provided Dockerfile:  gen-ai/inference/vllm-ray-gpu-deepseek/Dockerfile   This Dockerfile is based on a Ray image and includes vLLM and Hugging Face libraries. No additional packages are required for this deployment. Build and push the Docker image to Amazon ECR  OR  Use a pre-built image (for PoC deployments):  If you want to skip building and pushing a custom image, you can use the public ECR image:  public.ecr.aws/data-on-eks/ray-2.41.0-py310-cu118-vllm0.7.0  Note: If using a custom image, replace the image reference in the RayServe YAML file with your ECR image URI.  Step3: Deploy RayServe Cluster  RayServe cluster is defined in a YAML configuration file that includes multiple resources:  Namespace for isolating the deployment.Secret for securely storing the Hugging Face Hub token.ConfigMap containing the serving script (OpenAI-compatible API interface).RayServe definition that includes: A Ray head pod deployed on an x86 node.Ray worker pods deployed on GPU instances (g5.4xlarge).  Deployment Steps  Note: Ensure that the image: field in ray-vllm-deepseek.yml is correctly set to either your custom ECR image URI or the default public ECR image.  Navigate to the directory containing the RayServe configuration and Apply the configuration using kubectl  cd cd ai-on-eks/blueprints/inference/vllm-ray-gpu-deepseek/ envsubst &lt; ray-vllm-deepseek.yml | kubectl apply -f -   Output  namespace/rayserve-vllm created secret/hf-token created configmap/vllm-serve-script created rayservice.ray.io/vllm created   Step4: Monitor the deployment  To monitor the deployment and check the status of the pods, run:  kubectl get pod -n rayserve-vllm   info Note: The image pull process may take up to 8 minutes on the first deployment. Subsequent updates will leverage the local cache. This can be optimized by building leaner images containing only necessary dependencies.  NAME READY STATUS RESTARTS AGE vllm-raycluster-7qwlm-head-vkqsc 2/2 Running 0 8m47s vllm-raycluster-7qwlm-worker-gpu-group-vh2ng 0/1 PodInitializing 0 8m47s   This deployment also creates a DeepSeek-R1 service with multiple ports:  8265 - Ray Dashboard8000 - DeepSeek-R1 model endpoint  Run the following command to verify the services:  kubectl get svc -n rayserve-vllm NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE vllm ClusterIP 172.20.208.16 &lt;none&gt; 6379/TCP,8265/TCP,10001/TCP,8000/TCP,8080/TCP 48m vllm-head-svc ClusterIP 172.20.239.237 &lt;none&gt; 6379/TCP,8265/TCP,10001/TCP,8000/TCP,8080/TCP 37m vllm-serve-svc ClusterIP 172.20.196.195 &lt;none&gt; 8000/TCP 37m   To access the Ray dashboard, you can port-forward the relevant port to your local machine:  kubectl -n rayserve-vllm port-forward svc/vllm 8265:8265   You can then access the web UI at http://localhost:8265, which displays the deployment of jobs and actors within the Ray ecosystem.  info Model deploymen takes around 4 mins        ","version":"Next","tagName":"h2"},{"title":"Test the DeepSeek-R1 Model‚Äã","type":1,"pageTitle":"DeepSeek-R1 on EKS with Ray and vLLM","url":"/ai-on-eks/docs/blueprints/inference/GPUs/ray-vllm-deepseek#test-the-deepseek-r1-model","content":" Now it's time to test the DeepSeek-R1-Distill-Llama-8B chat model.  First, execute a port forward to the vllm-serve-svc Service using kubectl:  kubectl -n rayserve-vllm port-forward svc/vllm-serve-svc 8000:8000   Run a test inference request:  curl -X POST http://localhost:8000/v1/chat/completions -H &quot;Content-Type: application/json&quot; -d '{ &quot;model&quot;: &quot;deepseek-ai/DeepSeek-R1-Distill-Llama-8B&quot;, &quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: ‚ÄúExplain about DeepSeek model?‚Äù}], &quot;stream&quot;: false }'   Response:  {&quot;id&quot;:&quot;chatcmpl-b86feed9-1482-4d1c-981d-085651d12813&quot;,&quot;object&quot;:&quot;chat.completion&quot;,&quot;created&quot;:1739001265,&quot;model&quot;:&quot;deepseek-ai/DeepSeek-R1-Distill-Llama-8B&quot;,&quot;choices&quot;:[{&quot;index&quot;:0,&quot;message&quot;:{&quot;role&quot;:&quot;assistant&quot;,&quot;content&quot;:&quot;&lt;think&gt;\\n\\n&lt;/think&gt;\\n\\nDeepSeek is a powerful AI search engine developed by the Chinese Company DeepSeek Inc. It is designed to solve complex STEM (Science, Technology, Engineering, and Mathematics) problems through precise reasoning and efficient computation. The model works bymidtTeX, combining large-scale dataset and strong reasoning capabilities to provide accurate and reliable answers.\\n\\n### Key Features:\\n1. **AI-powered Search**: DeepSeek uses advanced AI techniques to understand and analyze vast amounts of data, providing more accurate and relevant search results compared to traditional search engines.\\n2. **Reasoning and Problem-solving**: The model is equipped with strong reasoning capabilities, enabling it to solve complex STEM problems, answer research-level questions, and assist in decision-making.\\n3. **Customization**: DeepSeek can be tailored to specific domains or industries, allowing it to be adapted for various use cases such as academic research, business analysis, and technical problem-solving.\\n4. **Efficiency**: The model is highly efficient, fast, and scalable, making it suitable for a wide range of applications and handling large-scale data processing tasks.\\n5. **Domain Expertise**: It can be trained on domain-specific data and knowledge, making it highly specialized in particular fields like mathematics, programming, or engineering.\\n\\n### Applications:\\n- **Education and Research**: Assisting students and researchers with complex STEM problems and research questions.\\n- **Business Analysis**: aiding in market research, data analysis, and strategic decision-making.\\n- **Technical Support**: solving technical issues and providing troubleshooting assistance.\\n- **Custom Problem Solving**: addressing specific challenges in various fields by leveraging domain-specific knowledge.\\n\\nDeepSeek is a valuable tool for any individual or organizationengaged in STEM fields or requires advanced AI-powered search and reasoning capabilities.&quot;,&quot;tool_calls&quot;:[]},&quot;logprobs&quot;:null,&quot;finish_reason&quot;:&quot;stop&quot;,&quot;stop_reason&quot;:null}],&quot;usage&quot;:{&quot;prompt_tokens&quot;:10,&quot;total_tokens&quot;:359,&quot;completion_tokens&quot;:349,&quot;prompt_tokens_details&quot;:null},&quot;prompt_logprobs&quot;:null}%   ","version":"Next","tagName":"h2"},{"title":"Deploy Open Web Ui‚Äã","type":1,"pageTitle":"DeepSeek-R1 on EKS with Ray and vLLM","url":"/ai-on-eks/docs/blueprints/inference/GPUs/ray-vllm-deepseek#deploy-open-web-ui","content":" Now, let's deploy the open-source Open WebUI, which provides a ChatGPT-style chat interface to interact with the DeepSeek model deployed on EKS. Open WebUI will use the model service to send requests and receive responses.  Deploy Open WebUI  Verify the YAML file gen-ai/inference/vllm-ray-gpu-deepseek/open-webui.yaml for Open WebUI. This is deployed as a container in EKS, and it communicates with the model service.Apply the Open WebUI deployment:  cd gen-ai/inference/vllm-ray-gpu-deepseek/ kubectl apply -f open-webui.yaml   Output:  namespace/openai-webui created deployment.apps/open-webui created service/open-webui created   Accessing the Open WebUI  To open the web UI, port-forward the Open WebUI service:  kubectl -n open-webui port-forward svc/open-webui 8080:80   Then, open a browser and navigate to: http://localhost:8080  You will see a registration page. Register with your name, email, and password.            After submitting a request, you can monitor the GPU and CPU usage returning to normal:    ","version":"Next","tagName":"h2"},{"title":"Key Takeaways‚Äã","type":1,"pageTitle":"DeepSeek-R1 on EKS with Ray and vLLM","url":"/ai-on-eks/docs/blueprints/inference/GPUs/ray-vllm-deepseek#key-takeaways","content":" 1. Model Initialization &amp; Memory Allocation  Once deployed, the model automatically detects CUDA and initializes its execution environment.GPU memory is allocated dynamically, with 90% utilization reserved for model weights (14.99 GiB), activation memory (0.85 GiB), and KV Cache (4.17 GiB).Expect some initial delay during the first model load, as weights are fetched and optimized for inference.  2. Inference Execution &amp; Optimization  The model supports multiple tasks but defaults to text generation (generate).Flash Attention is enabled, reducing memory overhead and improving inference speed.CUDA Graph Capture is applied, allowing for faster repeated inferences‚Äîbut if OOM issues arise, decreasing gpu_memory_utilization or enabling eager execution can help.  3. Token Generation &amp; Performance Metrics  The model will initially show 0 tokens/sec for prompt throughput, as it waits for input.Once inference starts, token generation throughput stabilizes at ~29 tokens/sec.GPU KV Cache utilization starts at ~12.5% and increases as more tokens are processed‚Äîensuring smoother text generation over time.  4. System Resource Utilization  Expect 8 CPU and 8 CUDA blocks handling parallel execution.Inference concurrency is limited to 4 requests for 8192 tokens per request, meaning simultaneous requests may be queued if the model is fully utilized.If encountering memory spikes, lowering max_num_seqs will help reduce GPU strain.  5. Monitoring &amp; Observability  You can track average prompt throughput, generation speed, and GPU KV Cache usage in the logs.If inference slows down, check the logs for pending or swapped requests, which may indicate memory pressure or scheduling delays.Real-time observability (e.g., tracing request latency) is disabled by default, but can be enabled for deeper monitoring.  What to Expect Post-Deployment?  The model will take a few minutes to initialize due to memory profiling and CUDA graph optimization.Once running, you should see stable throughput of ~29 tokens/sec with efficient memory usage.If performance dips, adjust KV Cache size, decrease memory utilization, or enable eager execution for better stability.  ","version":"Next","tagName":"h2"},{"title":"Cleanup‚Äã","type":1,"pageTitle":"DeepSeek-R1 on EKS with Ray and vLLM","url":"/ai-on-eks/docs/blueprints/inference/GPUs/ray-vllm-deepseek#cleanup","content":" Finally, we'll provide instructions for cleaning up and deprovisioning the resources when they are no longer needed.  Delete the RayCluster  cd ai-on-eks/blueprints/inference/vllm-rayserve-gpu kubectl delete -f open-webui.yaml kubectl delete -f ray-vllm-deepseek.yml   cd ai-on-eks/infra/jark-stack/terraform/monitoring kubectl delete -f serviceMonitor.yaml kubectl delete -f podMonitor.yaml   Destroy the EKS Cluster and resources  export AWS_DEAFULT_REGION=&quot;DEPLOYED_EKS_CLUSTER_REGION&gt;&quot; cd ai-on-eks/infra/jark-stack/terraform/ &amp;&amp; chmod +x cleanup.sh ./cleanup.sh  ","version":"Next","tagName":"h2"},{"title":"NVIDIA NIM LLM Deployment on Amazon EKS","type":0,"sectionRef":"#","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-nim-llama3","content":"","keywords":"","version":"Next"},{"title":"What is NVIDIA NIM?‚Äã","type":1,"pageTitle":"NVIDIA NIM LLM Deployment on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-nim-llama3#what-is-nvidia-nim","content":" NVIDIA NIM makes it easy for IT and DevOps teams to self-host large language models (LLMs) in their own managed environments while still providing developers with industry standard APIs that allow them to build powerful copilots, chatbots, and AI assistants that can transform their business. Leveraging NVIDIA‚Äôs cutting-edge GPU acceleration and scalable deployment, NIM offers the fastest path to inference with unparalleled performance.  ","version":"Next","tagName":"h2"},{"title":"Why NIM?‚Äã","type":1,"pageTitle":"NVIDIA NIM LLM Deployment on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-nim-llama3#why-nim","content":" NIM abstracts away model inference internals such as execution engine and runtime operations. They are also the most performant option available whether it be with TRT-LLM, vLLM or others.  NIMs are packaged as container images on a per model/model family basis. Each NIM container is with a model, such as meta/llama3-8b-instruct. These containers include a runtime that runs on any NVIDIA GPU with sufficient GPU memory, but some model/GPU combinations are optimized. NIM automatically downloads the model from NVIDIA NGC Catalog, leveraging a local filesystem cache if available.  ","version":"Next","tagName":"h2"},{"title":"Overview of this deployment pattern on Amazon EKS‚Äã","type":1,"pageTitle":"NVIDIA NIM LLM Deployment on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-nim-llama3#overview-of-this-deployment-pattern-on-amazon-eks","content":" This pattern combines the capabilities of NVIDIA NIM, Amazon Elastic Kubernetes Service (EKS), and various AWS services to deliver a high-performance and cost-optimized model serving infrastructure.  NVIDIA NIM Container Images: NVIDIA NIM provides a streamlined approach to hosting LLM models like Llama3 within containerized environments. This allows customers to leverage their private models while ensuring seamless integration with existing infrastructure. We will bring detailed setup steps to NIM deployments. Karpenter for Instance-Level Scaling: Karpenter, an open-source node provisioning project, enables rapid and efficient scaling of Amazon EKS clusters at the instance level. This ensures that the model serving infrastructure can adapt to dynamic workload demands, optimizing resource utilization and cost-effectiveness. Spot instances: Considering LLMs are stateless, customers can leverage spot instances to significantly reduce costs. Amazon Elastic File System (EFS): Amazon EFS provides scalable, elastic file storage for use with Amazon EKS. It allows multiple pods to access the same file system concurrently, making it ideal for storing and sharing model artifacts, datasets, and other persistent data across the cluster. EFS automatically grows and shrinks as you add and remove files, eliminating the need for capacity planning and management. Terraform with EKS Blueprints: To streamline the deployment and management of this solution, we leverage Terraform and EKS Blueprints. This infrastructure-as-code approach enables automated provisioning of the entire stack, ensuring consistency, reproducibility, and efficient resource management.  By combining these components, our proposed solution delivers a powerful and cost-effective model serving infrastructure tailored for large language models. With NVIDIA NIM's seamless integration, Amazon EKS's scalability with Karpenter, customers can achieve high performance while minimizing infrastructure costs.    ","version":"Next","tagName":"h2"},{"title":"Deploying the Solution‚Äã","type":1,"pageTitle":"NVIDIA NIM LLM Deployment on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-nim-llama3#deploying-the-solution","content":" ","version":"Next","tagName":"h2"},{"title":"Prerequisites‚Äã","type":1,"pageTitle":"NVIDIA NIM LLM Deployment on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-nim-llama3#prerequisites","content":" Before getting started with NVIDIA NIM, ensure you have the following:  Click to expand the NVIDIA NIM account setup details NVIDIA AI Enterprise Account Register for an NVIDIA AI Enterprise account. If you don't have one, you can sign up for a trial account using this link. NGC API Key Log in to your NVIDIA AI Enterprise account Navigate to the NGC (NVIDIA GPU Cloud) portal Generate a personal API key: Go to your account settings or navigate directly to: https://org.ngc.nvidia.com/setup/personal-keysClick on &quot;Generate Personal Key&quot;Ensure that at least &quot;NGC Catalog&quot; is selected from the &quot;Services Included&quot; dropdownCopy and securely store your API key, the key should have a prefix with nvapi- Validate NGC API Key and Test Image Pull To ensure your API key is valid and working correctly: Set up your NGC API key as an environment variable: export NGC_API_KEY=&lt;your_api_key_here&gt; Authenticate Docker with the NVIDIA Container Registry: echo &quot;$NGC_API_KEY&quot; | docker login nvcr.io --username '$oauthtoken' --password-stdin Test pulling an image from NGC: docker pull nvcr.io/nim/meta/llama3-8b-instruct:latest You do not have to wait for it to complete, just to make sure the API key is valid to pull the image.  The following are required to run this tutorial  An active AWS account with admin equivalent permissionsaws clikubectlTerraform  ","version":"Next","tagName":"h3"},{"title":"Deploy‚Äã","type":1,"pageTitle":"NVIDIA NIM LLM Deployment on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-nim-llama3#deploy","content":" Clone the repository  git clone https://github.com/awslabs/ai-on-eks.git   1. Configure the NGC API Key  Retrieve your NGC API key from NVIDIA and set it as an environment variable:  export TF_VAR_ngc_api_key=&lt;replace-with-your-NGC-API-KEY&gt;   2. Installation  Important Note: Ensure that you update the region in the blueprint.tfvars file before deploying the blueprint. Additionally, confirm that your local region setting matches the specified region to prevent any discrepancies. For example, set your export AWS_DEFAULT_REGION=&quot;&lt;REGION&gt;&quot; to the desired region:  Run the installation script:  info This pattern deploys a model called nvcr.io/nim/meta/llama3-8b-instruct. You can modify the nim_models variable in the blueprint.tfvars file to add more models. Multiple models can be deployed simultaneously using this pattern.  caution Ensure you have specified enough GPUs for each model before enabling additional models through these variables. Also, verify that your AWS account has access to sufficient GPUs. This pattern uses Karpenter to scale GPU nodes, restricted to G5 instances by default. You can modify the Karpenter node pool to include other instances like p4 and p5 if needed.  cd ai-on-eks/infra/nvidia-triton-server export TF_VAR_enable_nvidia_nim=true export TF_VAR_enable_nvidia_triton_server=false ./install.sh   This process will take approximately 20 minutes to complete.  3. Verify the Installation  Once the installation finishes, you may find the configure_kubectl command from the output. Run the following to configure EKS cluster access  # Creates k8s config file to authenticate with EKS aws eks --region us-west-2 update-kubeconfig --name nvidia-triton-server   Check the status of your pods deployed  kubectl get all -n nim   You should see output similar to the following:  Click to expand the deployment details NAME READY STATUS RESTARTS AGE pod/nim-llm-llama3-8b-instruct-0 1/1 Running 0 4h2m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/nim-llm-llama3-8b-instruct ClusterIP 172.20.5.230 &lt;none&gt; 8000/TCP 4h2m service/nim-llm-llama3-8b-instruct-sts ClusterIP None &lt;none&gt; 8000/TCP 4h2m NAME READY AGE statefulset.apps/nim-llm-llama3-8b-instruct 1/1 4h2m NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE horizontalpodautoscaler.autoscaling/nim-llm-llama3-8b-instruct StatefulSet/nim-llm-llama3-8b-instruct 2/5 1 5 1 4h2m   The llama3-8b-instruct model is deployed with a StatefulSet in nim namespace. As it is running, Karpenter provisioned a GPU Check the Karpenter provisioned node.  kubectl get node -l type=karpenter -L node.kubernetes.io/instance-type   NAME STATUS ROLES AGE VERSION INSTANCE-TYPE ip-100-64-77-39.us-west-2.compute.internal Ready &lt;none&gt; 4m46s v1.30.0-eks-036c24b g5.2xlarge   4. Verify the deployed model  Once all pods in nim namespace is ready with 1/1 status, use below command to verify it's ready to serve the traffic. To verify, expose the model serving service with port-forward using kubectl.  kubectl port-forward -n nim service/nim-llm-llama3-8b-instruct 8000   Then you can invoke the deployed model with a simple HTTP request with curl command.  curl -X 'POST' \\ &quot;http://localhost:8000/v1/completions&quot; \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ &quot;model&quot;: &quot;meta/llama3-8b-instruct&quot;, &quot;prompt&quot;: &quot;Once upon a time&quot;, &quot;max_tokens&quot;: 64 }'   you will see similar output like the following  { &quot;id&quot;: &quot;cmpl-63a0b66aeda1440c8b6ca1ce3583b173&quot;, &quot;object&quot;: &quot;text_completion&quot;, &quot;created&quot;: 1719742336, &quot;model&quot;: &quot;meta/llama3-8b-instruct&quot;, &quot;choices&quot;: [ { &quot;index&quot;: 0, &quot;text&quot;: &quot;, there was a young man named Jack who lived in a small village at the foot of a vast and ancient forest. Jack was a curious and adventurous soul, always eager to explore the world beyond his village. One day, he decided to venture into the forest, hoping to discover its secrets.\\nAs he wandered deeper into&quot;, &quot;logprobs&quot;: null, &quot;finish_reason&quot;: &quot;length&quot;, &quot;stop_reason&quot;: null } ], &quot;usage&quot;: { &quot;prompt_tokens&quot;: 5, &quot;total_tokens&quot;: 69, &quot;completion_tokens&quot;: 64 } }   ","version":"Next","tagName":"h3"},{"title":"Testing the Llama3 model deployed with NIM‚Äã","type":1,"pageTitle":"NVIDIA NIM LLM Deployment on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-nim-llama3#testing-the-llama3-model-deployed-with-nim","content":" It's time to test the Llama3 just deployed. First setup a simple environment for the testing.  cd ai-on-eks/blueprints/inference/nvidia-nim/nim-client python3 -m venv .venv source .venv/bin/activate pip install openai   We have prepared some prompts in prompts.txt , it contains 20 prompts. You can run following commands with the prompts to verify the generated outputs.  python3 client.py --input-prompts prompts.txt --results-file results.txt   You will see an output something like below:  Loading inputs from `prompts.txt`... Model meta/llama3-8b-instruct - Request 14: 4.68s (4678.46ms) Model meta/llama3-8b-instruct - Request 10: 6.43s (6434.32ms) Model meta/llama3-8b-instruct - Request 3: 7.82s (7824.33ms) Model meta/llama3-8b-instruct - Request 1: 8.54s (8540.69ms) Model meta/llama3-8b-instruct - Request 5: 8.81s (8807.52ms) Model meta/llama3-8b-instruct - Request 12: 8.95s (8945.85ms) Model meta/llama3-8b-instruct - Request 18: 9.77s (9774.75ms) Model meta/llama3-8b-instruct - Request 16: 9.99s (9994.51ms) Model meta/llama3-8b-instruct - Request 6: 10.26s (10263.60ms) Model meta/llama3-8b-instruct - Request 0: 10.27s (10274.35ms) Model meta/llama3-8b-instruct - Request 4: 10.65s (10654.39ms) Model meta/llama3-8b-instruct - Request 17: 10.75s (10746.08ms) Model meta/llama3-8b-instruct - Request 11: 10.86s (10859.91ms) Model meta/llama3-8b-instruct - Request 15: 10.86s (10857.15ms) Model meta/llama3-8b-instruct - Request 8: 11.07s (11068.78ms) Model meta/llama3-8b-instruct - Request 2: 12.11s (12105.07ms) Model meta/llama3-8b-instruct - Request 19: 12.64s (12636.42ms) Model meta/llama3-8b-instruct - Request 9: 13.37s (13370.75ms) Model meta/llama3-8b-instruct - Request 13: 13.57s (13571.28ms) Model meta/llama3-8b-instruct - Request 7: 14.90s (14901.51ms) Storing results into `results.txt`... Accumulated time for all requests: 206.31 seconds (206309.73 milliseconds) PASS: NVIDIA NIM example Actual execution time used with concurrency 20 is: 14.92 seconds (14.92 milliseconds)   Output for results.txt should look like the following  Click to expand the partial output The key differences between traditional machine learning models and very large language models (vLLM) are: 1. **Scale**: vLLMs are massive, with billions of parameters, whereas traditional models typically have millions. 2. **Training data**: vLLMs are trained on vast amounts of text data, often sourced from the internet, whereas traditional models are trained on smaller, curated datasets. 3. **Architecture**: vLLMs often use transformer architectures, which are designed for sequential data like text, whereas traditional models may use feedforward networks or recurrent neural networks. 4. **Training objectives**: vLLMs are often trained using masked language modeling or next sentence prediction tasks, whereas traditional models may use classification, regression, or clustering objectives. 5. **Evaluation metrics**: vLLMs are typically evaluated using metrics like perplexity, accuracy, or fluency, whereas traditional models may use metrics like accuracy, precision, or recall. 6. **Interpretability**: vLLMs are often less interpretable due to their massive size and complex architecture, whereas traditional models may be more interpretable due to their smaller size and simpler architecture. These differences enable vLLMs to excel in tasks like language translation, text generation, and conversational AI, whereas traditional models are better suited for tasks like image classification or regression. ========= TensorRT (Triton Runtime) optimizes LLM (Large Language Model) inference on NVIDIA hardware by: 1. **Model Pruning**: Removing unnecessary weights and connections to reduce model size and computational requirements. 2. **Quantization**: Converting floating-point models to lower-precision integer formats (e.g., INT8) to reduce memory bandwidth and improve performance. 3. **Kernel Fusion**: Combining multiple kernel launches into a single launch to reduce overhead and improve parallelism. 4. **Optimized Tensor Cores**: Utilizing NVIDIA's Tensor Cores for matrix multiplication, which provides significant performance boosts. 5. **Batching**: Processing multiple input batches concurrently to improve throughput. 6. **Mixed Precision**: Using a combination of floating-point and integer precision to balance accuracy and performance. 7. **Graph Optimization**: Reordering and reorganizing the computation graph to minimize memory access and optimize data transfer. By applying these optimizations, TensorRT can significantly accelerate LLM inference on NVIDIA hardware, achieving faster inference times and improved performance. =========   ","version":"Next","tagName":"h3"},{"title":"Open WebUI Deployment‚Äã","type":1,"pageTitle":"NVIDIA NIM LLM Deployment on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-nim-llama3#open-webui-deployment","content":" info Open WebUI is compatible only with models that work with the OpenAI API server and Ollama.  1. Deploy the WebUI  Deploy the Open WebUI by running the following command:  kubectl apply -f ai-on-eks/blueprints/inference/nvidia-nim/openai-webui-deployment.yaml   2. Port Forward to Access WebUI  Use kubectl port-forward to access the WebUI locally:  kubectl port-forward svc/open-webui 8081:80 -n openai-webui   3. Access the WebUI  Open your browser and go to http://localhost:8081  4. Sign Up  Sign up using your name, email, and a dummy password.  5. Start a New Chat  Click on New Chat and select the model from the dropdown menu, as shown in the screenshot below:    6. Enter Test Prompt  Enter your prompt, and you will see the streaming results, as shown below:    ","version":"Next","tagName":"h2"},{"title":"Performance Testing with NVIDIA GenAI-Perf Tool‚Äã","type":1,"pageTitle":"NVIDIA NIM LLM Deployment on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-nim-llama3#performance-testing-with-nvidia-genai-perf-tool","content":" GenAI-Perf is a command line tool for measuring the throughput and latency of generative AI models as served through an inference server.  GenAI-Perf can be used as standard tool to benchmark with other models deployed with inference server. But this tool requires a GPU. To make it easier, we provide you a pre-configured manifest genaiperf-deploy.yaml to run the tool.  cd ai-on-eks/blueprints/inference/nvidia-nim kubectl apply -f genaiperf-deploy.yaml   Once the pod is ready with running status 1/1, can execute into the pod.  export POD_NAME=$(kubectl get po -l app=tritonserver -ojsonpath='{.items[0].metadata.name}') kubectl exec -it $POD_NAME -- bash   Run the testing to the deployed NIM Llama3 model  genai-perf \\ -m meta/llama3-8b-instruct \\ --service-kind openai \\ --endpoint v1/completions \\ --endpoint-type completions \\ --num-prompts 100 \\ --random-seed 123 \\ --synthetic-input-tokens-mean 200 \\ --synthetic-input-tokens-stddev 0 \\ --output-tokens-mean 100 \\ --output-tokens-stddev 0 \\ --tokenizer hf-internal-testing/llama-tokenizer \\ --concurrency 10 \\ --measurement-interval 4000 \\ --profile-export-file my_profile_export.json \\ --url nim-llm-llama3-8b-instruct.nim:8000   You should see similar output like the following  2024-07-11 03:32 [INFO] genai_perf.parser:166 - Model name 'meta/llama3-8b-instruct' cannot be used to create artifact directory. Instead, 'meta_llama3-8b-instruct' will be used. 2024-07-11 03:32 [INFO] genai_perf.wrapper:137 - Running Perf Analyzer : 'perf_analyzer -m meta/llama3-8b-instruct --async --input-data artifacts/meta_llama3-8b-instruct-openai-completions-concurrency10/llm_inputs.json --endpoint v1/completions --service-kind openai -u nim-llm.nim:8000 --measurement-interval 4000 --stability-percentage 999 --profile-export-file artifacts/meta_llama3-8b-instruct-openai-completions-concurrency10/my_profile_export.json -i http --concurrency-range 10' LLM Metrics ‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì ‚îÉ Statistic ‚îÉ avg ‚îÉ min ‚îÉ max ‚îÉ p99 ‚îÉ p90 ‚îÉ p75 ‚îÉ ‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î© ‚îÇ Request latency (ns) ‚îÇ 3,934,624,446 ‚îÇ 3,897,758,114 ‚îÇ 3,936,987,882 ‚îÇ 3,936,860,185 ‚îÇ 3,936,429,317 ‚îÇ 3,936,333,682 ‚îÇ ‚îÇ Num output token ‚îÇ 112 ‚îÇ 105 ‚îÇ 119 ‚îÇ 119 ‚îÇ 117 ‚îÇ 115 ‚îÇ ‚îÇ Num input token ‚îÇ 200 ‚îÇ 200 ‚îÇ 200 ‚îÇ 200 ‚îÇ 200 ‚îÇ 200 ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò Output token throughput (per sec): 284.64 Request throughput (per sec): 2.54   You should be able to see the metrics that genai-perf collects, including Request latency, Out token throughput, Request throughput.  To understand the command line options, please refer to this documentation.  ","version":"Next","tagName":"h2"},{"title":"Observability‚Äã","type":1,"pageTitle":"NVIDIA NIM LLM Deployment on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-nim-llama3#observability","content":" As part of this blueprint, we have also deployed the Kube Prometheus stack, which provides Prometheus server and Grafana deployments for monitoring and observability.  First, let's verify the services deployed by the Kube Prometheus stack:  kubectl get svc -n monitoring   You should see output similar to this:  NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kube-prometheus-stack-grafana ClusterIP 172.20.225.77 &lt;none&gt; 80/TCP 10m kube-prometheus-stack-kube-state-metrics ClusterIP 172.20.237.248 &lt;none&gt; 8080/TCP 10m kube-prometheus-stack-operator ClusterIP 172.20.118.163 &lt;none&gt; 443/TCP 10m kube-prometheus-stack-prometheus ClusterIP 172.20.132.214 &lt;none&gt; 9090/TCP,8080/TCP 10m kube-prometheus-stack-prometheus-node-exporter ClusterIP 172.20.213.178 &lt;none&gt; 9100/TCP 10m prometheus-adapter ClusterIP 172.20.171.163 &lt;none&gt; 443/TCP 10m prometheus-operated ClusterIP None &lt;none&gt; 9090/TCP 10m   The NVIDIA NIM LLM service expose metrics via /metrics endpoint from nim-llm-llama3-8b-instruct service at port 8000. Verify it by running  kubectl get svc -n nim kubectl port-forward -n nim svc/nim-llm-llama3-8b-instruct 8000 curl localhost:8000/metrics # run this in another terminal   ","version":"Next","tagName":"h2"},{"title":"Grafana Dashboard‚Äã","type":1,"pageTitle":"NVIDIA NIM LLM Deployment on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-nim-llama3#grafana-dashboard","content":" We provides a pre-configured Grafana dashboard to better visualize NIM status. In the Grafana dashboard below, it contains several important metrics:  Time to First Token (TTFT): The latency between the initial inference request to the model and the return of the first token.Inter-Token Latency (ITL): The latency between each token after the first.Total Throughput: The total number of tokens generated per second by the NIM.  You can find more metrics description from this document.    You can monitor metrics such as Time-to-First-Token, Inter-Token-Latency, KV Cache Utilization metrics.    To view the Grafana dashboard to monitor these metrics, follow the steps below:  Click to expand details 1. Retrieve the Grafana password. The password is saved in the AWS Secret Manager. Below Terraform command will show you the secret name. terraform output grafana_secret_name Then use the output secret name to run below command, aws secretsmanager get-secret-value --secret-id &lt;grafana_secret_name_output&gt; --region $AWS_REGION --query &quot;SecretString&quot; --output text 2. Expose the Grafana Service Use port-forward to expose the Grafana service. kubectl port-forward svc/kube-prometheus-stack-grafana 3000:80 -n monitoring 3. Login to Grafana: Open your web browser and navigate to http://localhost:3000.Login with the username admin and the password retrieved from AWS Secrets Manager. 4. Open the NIM Monitoring Dashboard: Once logged in, click &quot;Dashboards&quot; on the left sidebar and search &quot;nim&quot;You can find the Dashboard NVIDIA NIM Monitoring from the listClick and entering to the dashboard. You should now see the metrics displayed on the Grafana dashboard, allowing you to monitor the performance your NVIDIA NIM service deployment.  info As of writing this guide, NVIDIA also provides an example Grafana dashboard. You can check it from here.  ","version":"Next","tagName":"h3"},{"title":"Cleanup‚Äã","type":1,"pageTitle":"NVIDIA NIM LLM Deployment on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-nim-llama3#cleanup","content":" To remove all resources created by this deployment, run:  ./cleanup.sh  ","version":"Next","tagName":"h2"},{"title":"NVIDIA NIM Operator on Amazon EKS","type":0,"sectionRef":"#","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-nim-operator","content":"","keywords":"","version":"Next"},{"title":"What is NVIDIA NIM?‚Äã","type":1,"pageTitle":"NVIDIA NIM Operator on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-nim-operator#what-is-nvidia-nim","content":" NVIDIA NIM (NVIDIA Inference Microservices) is a set of containerized microservices that make it easier to deploy and host large language models (LLMs) and other AI models in your own environment. NIM provides standard APIs (similar to OpenAI or other AI services) for developers to build applications like chatbots and AI assistants, while leveraging NVIDIA‚Äôs GPU acceleration for high-performance inference. In essence, NIM abstracts away the complexities of model runtime and optimization, offering a fast path to inference with optimized backends (e.g., TensorRT-LLM, FasterTransformer, etc.) under the hood.  ","version":"Next","tagName":"h2"},{"title":"NVIDIA NIM Operator for Kubernetes‚Äã","type":1,"pageTitle":"NVIDIA NIM Operator on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-nim-operator#nvidia-nim-operator-for-kubernetes","content":" The NVIDIA NIM Operator is a Kubernetes operator that automates the deployment, scaling, and management of NVIDIA NIM microservices on a Kubernetes cluster.    Instead of manually pulling containers, provisioning GPU nodes, or writing YAML for every model, the NIM Operator introduces three primary Custom Resource Definitions (CRDs):  NIMCacheNIMService[NIMPipeline] (https://docs.nvidia.com/nim-operator/latest/pipelines.html)  These CRDs allow you to declaratively define model deployments using native Kubernetes syntax.  The Operator handles:  Pulling the model image from NVIDIA GPU Cloud (NGC)Caching model weights and optimized runtime profilesLaunching model-serving pods with GPU allocationExposing inference endpoints via Kubernetes ServicesIntegrating with autoscaling (e.g., HPA + Karpenter)Chaining multiple models together into inference pipelines using NIMPipeline  ","version":"Next","tagName":"h2"},{"title":"NIMCache ‚Äì Model Caching for Faster Load Times‚Äã","type":1,"pageTitle":"NVIDIA NIM Operator on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-nim-operator#nimcache--model-caching-for-faster-load-times","content":" A NIMCache (nimcaches.apps.nvidia.com) is a custom resource that pre-downloads and stores a model‚Äôs weights, tokenizer, and runtime-optimized engine files (such as TensorRT-LLM profiles) into a shared persistent volume.  This ensures:  Faster cold start times: no repeated downloads from NGCStorage reuse across nodes and replicasCentralized, shared model store (typically on EFS or FSx for Lustre in EKS)  Model profiles are optimized for specific GPUs (e.g., A10G, L4) and precisions (e.g., FP16). When NIMCache is created, the Operator discovers available model profiles and selects the best one for your cluster.  üìå Tip: Using NIMCache is highly recommended for production, especially when running multiple replicas or restarting models frequently.  ","version":"Next","tagName":"h3"},{"title":"NIMService ‚Äì Deploying and Managing the Model Server‚Äã","type":1,"pageTitle":"NVIDIA NIM Operator on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-nim-operator#nimservice--deploying-and-managing-the-model-server","content":" A NIMService (nimservices.apps.nvidia.com) represents a running instance of a NIM model server on your cluster. It specifies the container image, GPU resources, number of replicas, and optionally the name of a NIMCache.  Key benefits:  Declarative model deployment using Kubernetes YAMLAutomatic node scheduling for GPU nodesShared cache support using NIMCacheAutoscaling via HPA or external triggersClusterIP or Ingress support to expose APIs  For example, deploying the Meta Llama 3.1 8B Instruct model involves creating:  A NIMCache to store the model (optional but recommended)A NIMService pointing to the cached model and allocating GPUs  If NIMCache is not used, the model will be downloaded each time a pod starts, which may increase startup latency.  ","version":"Next","tagName":"h3"},{"title":"NIMPipeline‚Äã","type":1,"pageTitle":"NVIDIA NIM Operator on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-nim-operator#nimpipeline","content":" NIMPipeline is another CRD that can group multiple NIMService resources into an ordered inference pipeline. This is useful for multi-model workflows like:  Retrieval-Augmented Generation (RAG)Embeddings + LLM chainingPreprocessing + classification pipelines  In this tutorial, we focus on a single model deployment using NIMCache and NIMService.  ","version":"Next","tagName":"h3"},{"title":"Overview of this deployment pattern on Amazon EKS‚Äã","type":1,"pageTitle":"NVIDIA NIM Operator on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-nim-operator#overview-of-this-deployment-pattern-on-amazon-eks","content":" This deployment blueprint demonstrates how to run the Meta Llama 3.1 8B Instruct model on Amazon EKS using the NVIDIA NIM Operator with multi-GPU support and optimized model caching for fast startup times.    The model is served using:  G5 instances (g5.12xlarge): These instances come with 4 NVIDIA A10G GPUsTensor Parallelism (TP): Set to 2, meaning the model will run in parallel across 2 GPUsPersistent Shared Cache: Backed by Amazon EFS to speed up model startup by reusing previously generated engine files  By combining these components, the model is deployed as a scalable Kubernetes workload that supports:  Efficient GPU scheduling with KarpenterFast model load using the NIMCacheScalable serving endpoint via NIMService  üìå Note: You can modify the tensorParallelism setting or select a different instance type (e.g., G6 with L4 GPUs) based on your performance and cost requirements.  warning Note: Before implementing NVIDIA NIM, please be aware it is part of NVIDIA AI Enterprise, which may introduce potential cost and licensing for production use. For evaluation, NVIDIA also offers a free evaluation license to try NVIDIA AI Enterprise for 90 days, and you can register it with your corporate email.  ","version":"Next","tagName":"h2"},{"title":"Deploying the Solution‚Äã","type":1,"pageTitle":"NVIDIA NIM Operator on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-nim-operator#deploying-the-solution","content":" In this tutorial, the entire AWS infrastructure is provisioned using Terraform, including:  Amazon VPC with public and private subnetsAmazon EKS clusterGPU nodepools using KarpenterAddons such as: NVIDIA device pluginEFS CSI driverNVIDIA NIM Operator  As a demonstration, the Meta Llama-3.1 8B Instruct model will be deployed using a NIMService, optionally backed by a NIMCache for improved cold start performance.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites‚Äã","type":1,"pageTitle":"NVIDIA NIM Operator on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-nim-operator#prerequisites","content":" Before getting started with NVIDIA NIM, ensure you have the following:  Click to expand the NVIDIA NIM account setup details NVIDIA AI Enterprise Account Register for an NVIDIA AI Enterprise account. If you don't have one, you can sign up for a trial account using this link. NGC API Key Log in to your NVIDIA AI Enterprise account Navigate to the NGC (NVIDIA GPU Cloud) portal Generate a personal API key: Go to your account settings or navigate directly to: https://org.ngc.nvidia.com/setup/personal-keysClick on &quot;Generate Personal Key&quot;Ensure that at least &quot;NGC Catalog&quot; is selected from the &quot;Services Included&quot; dropdownCopy and securely store your API key, the key should have a prefix with nvapi- Validate NGC API Key and Test Image Pull To ensure your API key is valid and working correctly: Set up your NGC API key as an environment variable: export NGC_API_KEY=&lt;your_api_key_here&gt; Authenticate Docker with the NVIDIA Container Registry: echo &quot;$NGC_API_KEY&quot; | docker login nvcr.io --username '$oauthtoken' --password-stdin Test pulling an image from NGC: docker pull nvcr.io/nim/meta/llama3-8b-instruct:latest You do not have to wait for it to complete, just to make sure the API key is valid to pull the image.  The following are required to run this tutorial  An active AWS account with admin equivalent permissionsaws clikubectlTerraform  ","version":"Next","tagName":"h3"},{"title":"Deploy‚Äã","type":1,"pageTitle":"NVIDIA NIM Operator on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-nim-operator#deploy","content":" Clone the ai-on-eks repository that contains the Terraform code for this deployment pattern:  git clone https://github.com/awslabs/ai-on-eks.git   Navigate to the NVIDIA NIM deployment directory and run the install script to deploy the infrastructure:  cd ai-on-eks/infra/nvidia-nim ./install.sh   This deployment will take approximately ~20 minute to complete.  Once the installation finishes, you may find the configure_kubectl command from the output. Run the following to configure EKS cluster access  # Creates k8s config file to authenticate with EKS aws eks --region us-west-2 update-kubeconfig --name nvidia-nim-eks   Verify the deployments - Click to expand the deployment details $ kubectl get all -n nim-operator kubectl get all -n nim-operator NAME READY STATUS RESTARTS AGE pod/nim-operator-k8s-nim-operator-6fdffdf97f-56fxc 1/1 Running 0 26h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/k8s-nim-operator-metrics-service ClusterIP 172.20.148.6 &lt;none&gt; 8080/TCP 26h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nim-operator-k8s-nim-operator 1/1 1 1 26h NAME DESIRED CURRENT READY AGE replicaset.apps/nim-operator-k8s-nim-operator-6fdffdf97f 1 1 1 26h $ kubectl get crds | grep nim nimcaches.apps.nvidia.com 2025-03-27T17:39:00Z nimpipelines.apps.nvidia.com 2025-03-27T17:39:00Z nimservices.apps.nvidia.com 2025-03-27T17:39:01Z $ kubectl get crds | grep nemo nemocustomizers.apps.nvidia.com 2025-03-27T17:38:59Z nemodatastores.apps.nvidia.com 2025-03-27T17:38:59Z nemoentitystores.apps.nvidia.com 2025-03-27T17:38:59Z nemoevaluators.apps.nvidia.com 2025-03-27T17:39:00Z nemoguardrails.apps.nvidia.com 2025-03-27T17:39:00Z To list Karpenter autoscaling Nodepools $ kubectl get nodepools NAME NODECLASS NODES READY AGE g5-gpu-karpenter g5-gpu-karpenter 1 True 47h g6-gpu-karpenter g6-gpu-karpenter 0 True 7h56m inferentia-inf2 inferentia-inf2 0 False 47h trainium-trn1 trainium-trn1 0 False 47h x86-cpu-karpenter x86-cpu-karpenter 0 True 47h   ","version":"Next","tagName":"h3"},{"title":"Deploy llama-3.1-8b-instruct with NIM Operator‚Äã","type":1,"pageTitle":"NVIDIA NIM Operator on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-nim-operator#deploy-llama-31-8b-instruct-with-nim-operator","content":" Step 1: Create Secrets for Authentication‚Äã  To access the NVIDIA container registry and model artifacts, you'll need to provide your NGC API key. This script creates two Kubernetes secrets: ngc-secret for Docker image pulls and ngc-api-secret for model authorization.  cd blueprints/inference/gpu/nvidia-nim-operator-llama3-8b NGC_API_KEY=&quot;your-real-ngc-key&quot; ./deploy-nim-auth.sh   Step 2: Cache the Model to EFS using NIMCache CRD‚Äã  The NIMCache custom resource will pull the model and cache optimized engine profiles to EFS. This dramatically reduces startup time when launching the model later via NIMService.  cd blueprints/inference/gpu/nvidia-nim-operator-llama3-8b kubectl apply -f nim-cache-llama3-8b-instruct.yaml   Check status:  kubectl get nimcaches.apps.nvidia.com -n nim-service   Expected output:  NAME STATUS PVC AGE meta-llama3-8b-instruct Ready meta-llama3-8b-instruct-pvc 21h   Display cached model profiles:  kubectl get nimcaches.apps.nvidia.com -n nim-service \\ meta-llama3-8b-instruct -o=jsonpath=&quot;{.status.profiles}&quot; | jq .   Sample output:  [ { &quot;config&quot;: { &quot;feat_lora&quot;: &quot;false&quot;, &quot;gpu&quot;: &quot;A10G&quot;, &quot;llm_engine&quot;: &quot;tensorrt_llm&quot;, &quot;precision&quot;: &quot;fp16&quot;, &quot;profile&quot;: &quot;throughput&quot;, &quot;tp&quot;: &quot;2&quot; } } ]   Step 3: Deploy the Model using NIMService CRD‚Äã  Now launch the model service using the cached engine profiles.  cd blueprints/inference/gpu/nvidia-nim-operator-llama3-8b kubectl apply -f nim-service-llama3-8b-instruct.yaml   Check the deployed resources:  kubectl get all -n nim-service   Exepcted Output:  NAME READY STATUS RESTARTS AGE pod/meta-llama3-8b-instruct-6cdf47d6f6-hlbnf 1/1 Running 0 6h35m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/meta-llama3-8b-instruct ClusterIP 172.20.85.8 &lt;none&gt; 8000/TCP 6h35m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/meta-llama3-8b-instruct 1/1 1 1 6h35m NAME DESIRED CURRENT READY AGE replicaset.apps/meta-llama3-8b-instruct-6cdf47d6f6 1 1 1 6h35m   ","version":"Next","tagName":"h3"},{"title":"üöÄ Model Startup Timeline‚Äã","type":1,"pageTitle":"NVIDIA NIM Operator on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-nim-operator#-model-startup-timeline","content":" The following sample is captured from the pod/meta-llama3-8b-instruct-6cdf47d6f6-hlbnf log  Step\tTimestamp\tDescriptionStart\t~20:00:50\tPod starts, NIM container logs begin Profile Match\t20:00:50.100\tDetects and selects cached profile (tp=2) Workspace Ready\t20:00:50.132\tModel workspace initialized via EFS in 0.126s TensorRT Init\t20:00:51.168\tTensorRT-LLM engine begins setup Engine Ready\t20:01:06\tEngine loaded and profiles activated (~16.6 GiB across 2 GPUs) API Server Ready\t20:02:11.036\tFastAPI + Uvicorn starts Health Check OK\t20:02:18.781\t/v1/health/ready endpoint returns 200 OK  ‚ö° Startup time (cold boot to ready): ~81 seconds thanks to cached engine on EFS.  ","version":"Next","tagName":"h3"},{"title":"Test the Model with a Prompt‚Äã","type":1,"pageTitle":"NVIDIA NIM Operator on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-nim-operator#test-the-model-with-a-prompt","content":" Step 1: Port Forward the Model Service‚Äã  Expose the model locally using port forwarding:  kubectl port-forward -n nim-service service/meta-llama3-8b-instruct 8001:8000   Step 2: Send a Sample Prompt Using curl‚Äã  Run the following command to test the model with a chat prompt:  curl -X POST \\ http://localhost:8001/v1/chat/completions \\ -H 'Accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ &quot;model&quot;: &quot;meta/llama-3.1-8b-instruct&quot;, &quot;messages&quot;: [ { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What should I do for a 4 day vacation at Cape Hatteras National Seashore?&quot; } ], &quot;top_p&quot;: 1, &quot;n&quot;: 1, &quot;max_tokens&quot;: 1024, &quot;stream&quot;: false, &quot;frequency_penalty&quot;: 0.0, &quot;stop&quot;: [&quot;STOP&quot;] }'   Sample Response (Shortened):  {&quot;id&quot;:&quot;chat-061a9dba9179437fa24cab7f7c767f19&quot;,&quot;object&quot;:&quot;chat.completion&quot;,&quot;created&quot;:1743215809,&quot;model&quot;:&quot;meta/llama-3.1-8b-instruct&quot;,&quot;choices&quot;:[{&quot;index&quot;:0,&quot;message&quot;:{&quot;role&quot;:&quot;assistant&quot;,&quot;content&quot;:&quot;Cape Hatteras National Seashore is a beautiful coastal destination with a rich history, pristine beaches, ... exploration of the area's natural beauty and history. Feel free to modify it to suit your interests and preferences. Safe travels!&quot;},&quot;logprobs&quot;:null,&quot;finish_reason&quot;:&quot;stop&quot;,&quot;stop_reason&quot;:null}],&quot;usage&quot;:{&quot;prompt_tokens&quot;:30,&quot;total_tokens&quot;:773,&quot;completion_tokens&quot;:743},&quot;prompt_logprobs&quot;:null}%   üß† The model is now running with Tensor Parallelism = 2 across two A10G GPUs, each utilizing approximately 21.4 GiB of memory. Thanks to NIMCache backed by EFS, the model loaded quickly and is ready for low-latency inference.  ","version":"Next","tagName":"h3"},{"title":"Open WebUI Deployment‚Äã","type":1,"pageTitle":"NVIDIA NIM Operator on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-nim-operator#open-webui-deployment","content":" info Open WebUI is compatible only with models that work with the OpenAI API server and Ollama.  1. Deploy the WebUI  Deploy the Open WebUI by running the following command:  kubectl apply -f ai-on-eks/blueprints/inference/gpu/nvidia-nim-operator-llama3-8b/openai-webui-deployment.yaml   2. Port Forward to Access WebUI  Use kubectl port-forward to access the WebUI locally:  kubectl port-forward svc/open-webui 8081:80 -n openai-webui   3. Access the WebUI  Open your browser and go to http://localhost:8081  4. Sign Up  Sign up using your name, email, and a dummy password.  5. Start a New Chat  Click on New Chat and select the model from the dropdown menu, as shown in the screenshot below:    6. Enter Test Prompt  Enter your prompt, and you will see the streaming results, as shown below:    ","version":"Next","tagName":"h2"},{"title":"Performance Testing with NVIDIA GenAI-Perf Tool‚Äã","type":1,"pageTitle":"NVIDIA NIM Operator on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-nim-operator#performance-testing-with-nvidia-genai-perf-tool","content":" GenAI-Perf is a command line tool for measuring the throughput and latency of generative AI models as served through an inference server.  GenAI-Perf can be used as standard tool to benchmark with other models deployed with inference server. But this tool requires a GPU. To make it easier, we provide you a pre-configured manifest genaiperf-deploy.yaml to run the tool.  cd ai-on-eks/blueprints/inference/gpu/nvidia-nim-operator-llama3-8b kubectl apply -f genaiperf-deploy.yaml   Once the pod is ready with running status 1/1, can execute into the pod.  export POD_NAME=$(kubectl get po -l app=genai-perf -ojsonpath='{.items[0].metadata.name}') kubectl exec -it $POD_NAME -- bash   Run the testing to the deployed NIM Llama3 model  genai-perf profile -m meta/llama-3.1-8b-instruct \\ --url meta-llama3-8b-instruct.nim-service:8000 \\ --service-kind openai \\ --endpoint-type chat \\ --num-prompts 100 \\ --synthetic-input-tokens-mean 200 \\ --synthetic-input-tokens-stddev 0 \\ --output-tokens-mean 100 \\ --output-tokens-stddev 0 \\ --concurrency 20 \\ --streaming \\ --tokenizer hf-internal-testing/llama-tokenizer   You should see similar output like the following    You should be able to see the metrics that genai-perf collects, including Request latency, Out token throughput, Request throughput.  To understand the command line options, please refer to this documentation.  ","version":"Next","tagName":"h2"},{"title":"Grafana Dashboard‚Äã","type":1,"pageTitle":"NVIDIA NIM Operator on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-nim-operator#grafana-dashboard","content":" NVIDIA provided a Grafana dashboard to better visualize NIM status. In the Grafana dashboard, it contains several important metrics:  Time to First Token (TTFT): The latency between the initial inference request to the model and the return of the first token.Inter-Token Latency (ITL): The latency between each token after the first.Total Throughput: The total number of tokens generated per second by the NIM.  You can find more metrics description from this document.    You can monitor metrics such as Time-to-First-Token, Inter-Token-Latency, KV Cache Utilization metrics.    To view the Grafana dashboard to monitor these metrics, follow the steps below:  Click to expand details 1. Retrieve the Grafana password. The password is saved in the AWS Secret Manager. Below Terraform command will show you the secret name. terraform output grafana_secret_name Then use the output secret name to run below command, aws secretsmanager get-secret-value --secret-id &lt;grafana_secret_name_output&gt; --region $AWS_REGION --query &quot;SecretString&quot; --output text 2. Expose the Grafana Service Use port-forward to expose the Grafana service. kubectl port-forward svc/kube-prometheus-stack-grafana 3000:80 -n monitoring 3. Login to Grafana: Open your web browser and navigate to http://localhost:3000.Login with the username admin and the password retrieved from AWS Secrets Manager. 4. Open the NIM Monitoring Dashboard: Once logged in, click &quot;Dashboards&quot; on the left sidebar and search &quot;nim&quot;You can find the Dashboard NVIDIA NIM Monitoring from the listClick and entering to the dashboard. You should now see the metrics displayed on the Grafana dashboard, allowing you to monitor the performance your NVIDIA NIM service deployment.  info As of writing this guide, NVIDIA also provides an example Grafana dashboard. You can check it from here.  ","version":"Next","tagName":"h3"},{"title":"Conclusion‚Äã","type":1,"pageTitle":"NVIDIA NIM Operator on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-nim-operator#conclusion","content":" This blueprint showcases how to deploy and scale large language models like Meta‚Äôs Llama 3.1 8B Instruct efficiently on Amazon EKS using the NVIDIA NIM Operator.  By combining OpenAI-compatible APIs with GPU-accelerated inference, declarative Kubernetes CRDs (NIMCache, NIMService), and fast model startup via EFS-based caching, you get a streamlined, production-grade model deployment experience.  ","version":"Next","tagName":"h2"},{"title":"Key Benefits:‚Äã","type":1,"pageTitle":"NVIDIA NIM Operator on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-nim-operator#key-benefits","content":" Faster cold starts through shared, persistent model cacheDeclarative and repeatable deployments with CRDsDynamic GPU autoscaling powered by KarpenterOne-click infrastructure provisioning using Terraform  In just ~20 minutes, you can go from zero to a scalable LLM service on Kubernetes ‚Äî ready to serve real-world prompts with low latency and high efficiency.  ","version":"Next","tagName":"h3"},{"title":"Cleanup‚Äã","type":1,"pageTitle":"NVIDIA NIM Operator on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-nim-operator#cleanup","content":" To tear down the deployed model and associated infrastructure:  ","version":"Next","tagName":"h2"},{"title":"Step 1: Delete Model Resources‚Äã","type":1,"pageTitle":"NVIDIA NIM Operator on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-nim-operator#step-1-delete-model-resources","content":" Delete the deployed NIMService and NIMCache objects from your cluster:  cd blueprints/inference/gpu/nvidia-nim-operator-llama3-8b kubectl delete -f nim-service-llama3-8b-instruct.yaml kubectl delete -f nim-cache-llama3-8b-instruct.yaml   Verify deletion:  kubectl get nimservices.apps.nvidia.com -n nim-service kubectl get nimcaches.apps.nvidia.com -n nim-service   ","version":"Next","tagName":"h3"},{"title":"Step 2: Destroy AWS Infrastructure‚Äã","type":1,"pageTitle":"NVIDIA NIM Operator on Amazon EKS","url":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-nim-operator#step-2-destroy-aws-infrastructure","content":" Navigate back to the root Terraform module and run the cleanup script. This will destroy all AWS resources created for this blueprint, including the VPC, EKS cluster, EFS, and node groups:  cd ai-on-eks/infra/nvidia-nim ./cleanup.sh  ","version":"Next","tagName":"h3"},{"title":"Deploying Stable Diffusion v2 with GPUs, Ray Serve and Gradio","type":0,"sectionRef":"#","url":"/ai-on-eks/docs/blueprints/inference/GPUs/stablediffusion-gpus","content":"","keywords":"","version":"Next"},{"title":"What is Stable Diffusion?‚Äã","type":1,"pageTitle":"Deploying Stable Diffusion v2 with GPUs, Ray Serve and Gradio","url":"/ai-on-eks/docs/blueprints/inference/GPUs/stablediffusion-gpus#what-is-stable-diffusion","content":" Stable Diffusion is a cutting-edge text-to-image model that generates stunning, detailed images from text descriptions. It's a powerful tool for artists, designers, and anyone who wants to unleash their imagination through image generation. This model stands out by offering a high degree of creative control and flexibility in the image generation process.  ","version":"Next","tagName":"h3"},{"title":"Deploying the Solution‚Äã","type":1,"pageTitle":"Deploying Stable Diffusion v2 with GPUs, Ray Serve and Gradio","url":"/ai-on-eks/docs/blueprints/inference/GPUs/stablediffusion-gpus#deploying-the-solution","content":" Let's get Stable Diffusion v2-1 up and running on Amazon EKS! In this section, we'll cover:  Prerequisites: Ensuring you have everything in place.Infrastructure Setup: Creating your EKS cluster and preparing it for deployment.Deploying the Ray Cluster: The core of your image generation pipeline, providing scalability and efficiency.Building the Gradio Web UI: A user-friendly interface for interacting with Stable Diffusion.  Prerequisites üëà  ","version":"Next","tagName":"h2"},{"title":"Deploying the Ray Cluster with Stable Diffusion Model‚Äã","type":1,"pageTitle":"Deploying Stable Diffusion v2 with GPUs, Ray Serve and Gradio","url":"/ai-on-eks/docs/blueprints/inference/GPUs/stablediffusion-gpus#deploying-the-ray-cluster-with-stable-diffusion-model","content":" Once the jark-stack cluster is deployed, you can proceed to use kubectl to deploy the ray-service-stablediffusion.yaml from /ai-on-eks/blueprints/inference/stable-diffusion-rayserve-gpu/ path.  In this step, we will deploy the Ray Serve cluster, which comprises one Head Pod on x86 CPU instances using Karpenter autoscaling, as well as Ray workers on g5.2xlarge instances, autoscaled by Karpenter.  Let's take a closer look at the key files used in this deployment and understand their functionalities before proceeding with the deployment:  ray_serve_sd.py:This script sets up a FastAPI application with two main components deployed using Ray Serve, which enables scalable model serving on GPU-equipped infrastructure: StableDiffusionV2 Deployment: This class initializes the Stable Diffusion V2 model using a scheduler and moves it to a GPU for processing. It includes functionality to generate images based on textual prompts, with the image size customizable via the input parameter.APIIngress: This FastAPI endpoint acts as an interface to the Stable Diffusion model. It exposes a GET method on the /imagine path that takes a text prompt and an optional image size. It generates an image using the Stable Diffusion model and returns it as a PNG file. ray-service-stablediffusion.yaml:This RayServe deployment pattern sets up a scalable service for hosting the Stable Diffusion model on Amazon EKS with GPU support. It creates a dedicated namespace and configures a RayService with autoscaling capabilities to efficiently manage resource utilization based on incoming traffic. The deployment ensures that the model, served under the RayService umbrella, can automatically adjust between 1 and 4 replicas, depending on demand, with each replica requiring a GPU. This pattern makes use of custom container images designed to maximize performance and minimizes startup delays by ensuring that heavy dependencies are preloaded.  ","version":"Next","tagName":"h2"},{"title":"Deploy the Stable Diffusion V2 Model‚Äã","type":1,"pageTitle":"Deploying Stable Diffusion v2 with GPUs, Ray Serve and Gradio","url":"/ai-on-eks/docs/blueprints/inference/GPUs/stablediffusion-gpus#deploy-the-stable-diffusion-v2-model","content":" Ensure the cluster is configured locally  aws eks --region us-west-2 update-kubeconfig --name jark-stack   Deploy RayServe Cluster  cd ai-on-eks/blueprints/inference/stable-diffusion-rayserve-gpu kubectl apply -f ray-service-stablediffusion.yaml   Verify the deployment by running the following commands  info If you did not preload container images into the data volume, the deployment process may take up to 10 to 12 minutes. The Head Pod is expected to be ready within 2 to 3 minutes, while the Ray Serve worker pod may take up to 10 minutes for image retrieval and Model deployment from Huggingface.  This deployment establishes a Ray head pod running on an x86 instance and a worker pod on a GPU G5 instance as shown below.  kubectl get pods -n stablediffusion NAME READY STATUS rservice-raycluster-hb4l4-worker-gpu-worker-group-z8gdw 1/1 Running stablediffusion-service-raycluster-hb4l4-head-4kfzz 2/2 Running   If you have preload container images into the data volume, you can find the message showing Container image &quot;public.ecr.aws/data-on-eks/ray2.11.0-py310-gpu-stablediffusion:latest&quot; already present on machine in the output of kubectl describe pod -n stablediffusion.  kubectl describe pod -n stablediffusion ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 41m default-scheduler 0/8 nodes are available: 1 Insufficient cpu, 3 Insufficient memory, 8 Insufficient nvidia.com/gpu. preemption: 0/8 nodes are available: 8 No preemption victims found for incoming pod. Normal Nominated 41m karpenter Pod should schedule on: nodeclaim/gpu-ljvhl Normal Scheduled 40m default-scheduler Successfully assigned stablediffusion/stablediffusion-raycluster-ms6pl-worker-gpu-85d22 to ip-100-64-136-72.us-west-2.compute.internal Normal Pulled 40m kubelet Container image &quot;public.ecr.aws/data-on-eks/ray2.11.0-py310-gpu-stablediffusion:latest&quot; already present on machine Normal Created 40m kubelet Created container wait-gcs-ready Normal Started 40m kubelet Started container wait-gcs-ready Normal Pulled 39m kubelet Container image &quot;public.ecr.aws/data-on-eks/ray2.11.0-py310-gpu-stablediffusion:latest&quot; already present on machine Normal Created 39m kubelet Created container worker Normal Started 38m kubelet Started container worker   This deployment also sets up a stablediffusion service with multiple ports configured; port 8265 is designated for the Ray dashboard and port 8000 for the Stable Diffusion model endpoint.  kubectl get svc -n stablediffusion NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) stablediffusion-service NodePort 172.20.223.142 &lt;none&gt; 8080:30213/TCP,6379:30386/TCP,8265:30857/TCP,10001:30666/TCP,8000:31194/TCP stablediffusion-service-head-svc NodePort 172.20.215.100 &lt;none&gt; 8265:30170/TCP,10001:31246/TCP,8000:30376/TCP,8080:32646/TCP,6379:31846/TCP stablediffusion-service-serve-svc NodePort 172.20.153.125 &lt;none&gt; 8000:31459/TCP   For the Ray dashboard, you can port-forward these ports individually to access the web UI locally using localhost.  kubectl port-forward svc/stablediffusion-service 8266:8265 -n stablediffusion   Access the web UI via http://localhost:8265 . This interface displays the deployment of jobs and actors within the Ray ecosystem.    The screenshots provided will show the Serve deployment and the Ray Cluster deployment, offering a visual overview of the setup and operational status.    ","version":"Next","tagName":"h3"},{"title":"Deploying the Gradio WebUI App‚Äã","type":1,"pageTitle":"Deploying Stable Diffusion v2 with GPUs, Ray Serve and Gradio","url":"/ai-on-eks/docs/blueprints/inference/GPUs/stablediffusion-gpus#deploying-the-gradio-webui-app","content":" Discover how to create a user-friendly chat interface using Gradio that integrates seamlessly with deployed models.  Let's move forward with setting up the Gradio app as a Docker container running on localhost. This setup will enable interaction with the Stable Diffusion XL model, which is deployed using RayServe.  ","version":"Next","tagName":"h2"},{"title":"Build the Gradio app docker container‚Äã","type":1,"pageTitle":"Deploying Stable Diffusion v2 with GPUs, Ray Serve and Gradio","url":"/ai-on-eks/docs/blueprints/inference/GPUs/stablediffusion-gpus#build-the-gradio-app-docker-container","content":" First, lets build the docker container for the client app.  cd ai-on-eks/blueprints/inference/gradio-ui docker build --platform=linux/amd64 \\ -t gradio-app:sd \\ --build-arg GRADIO_APP=&quot;gradio-app-stable-diffusion.py&quot; \\ .   ","version":"Next","tagName":"h3"},{"title":"Deploy the Gradio container‚Äã","type":1,"pageTitle":"Deploying Stable Diffusion v2 with GPUs, Ray Serve and Gradio","url":"/ai-on-eks/docs/blueprints/inference/GPUs/stablediffusion-gpus#deploy-the-gradio-container","content":" Deploy the Gradio app as a container on localhost using docker:  docker run --rm -it -p 7860:7860 -p 8000:8000 gradio-app:sd   info If you are not running Docker Desktop on your machine and using something like finch instead then you will need to additional flags for a custom host-to-IP mapping inside the container. docker run --rm -it \\ --add-host ray-service:&lt;workstation-ip&gt; \\ -e &quot;SERVICE_NAME=http://ray-service:8000&quot; \\ -p 7860:7860 gradio-app:sd   Invoke the WebUI‚Äã  Open your web browser and access the Gradio WebUI by navigating to the following URL:  Running on local URL: http://localhost:7860  You should now be able to interact with the Gradio application from your local machine.    ","version":"Next","tagName":"h3"},{"title":"Ray Autoscaling‚Äã","type":1,"pageTitle":"Deploying Stable Diffusion v2 with GPUs, Ray Serve and Gradio","url":"/ai-on-eks/docs/blueprints/inference/GPUs/stablediffusion-gpus#ray-autoscaling","content":" The Ray Autoscaling configuration detailed in the ray-serve-stablediffusion.yaml file leverages the capabilities of Ray on Kubernetes to dynamically scale applications based on computational needs.  Incoming Traffic: Incoming requests to your stable-diffusion deployment trigger Ray Serve to monitor the load on existing replicas.Metrics-Based Scaling: Ray Serve tracks the average number of ongoing requests per replica. This configuration has target_num_ongoing_requests_per_replica set to 1. If this metric exceeds the threshold, it signals the need for more replicas.Replica Creation (Within Node): If a node has sufficient GPU capacity, Ray Serve will attempt to add a new replica within the existing node. Your deployment requests 1 GPU per replica (ray_actor_options: num_gpus: 1).Node Scaling (Karpenter): If a node cannot accommodate an additional replica (e.g., only one GPU per node), Ray will signal to Kubernetes that it needs more resources. Karpenter observes pending pod requests from Kubernetes and provisions a new g5 GPU node to fulfill the resource need.Replica Creation (Across Nodes): Once the new node is ready, Ray Serve schedules an additional replica on the newly provisioned node.  To simulate autoscaling:  Generate Load: Create a script or use a load testing tool to send a burst of image generation requests to your stable diffusion service.Observe (Ray Dashboard): Access the Ray Dashboard (via port-forwarding or public NLB if configured) at http://your-cluster/dashboard. Observe how these metrics change: The number of replicas for your deployment. The number of nodes in your Ray cluster.Observe (Kubernetes): Use kubectl get pods -n stablediffusion to see the creation of new pods. Use kubectl get nodes to observe new nodes provisioned by Karpenter.  ","version":"Next","tagName":"h3"},{"title":"Cleanup‚Äã","type":1,"pageTitle":"Deploying Stable Diffusion v2 with GPUs, Ray Serve and Gradio","url":"/ai-on-eks/docs/blueprints/inference/GPUs/stablediffusion-gpus#cleanup","content":" Finally, we'll provide instructions for cleaning up and deprovisioning the resources when they are no longer needed.  Step1: Delete Gradio Container  Ctrl-c on the localhost terminal window where docker run is running to kill the container running the Gradio app. Optionally clean up the docker image  docker rmi gradio-app:sd   Step2: Delete Ray Cluster  cd ai-on-eks/blueprints/inference/stable-diffusion-rayserve-gpu kubectl delete -f ray-service-stablediffusion.yaml   Step3: Cleanup the EKS Cluster This script will cleanup the environment using -target option to ensure all the resources are deleted in correct order.  cd ai-on-eks/infra/jark-stack/ ./cleanup.sh  ","version":"Next","tagName":"h2"},{"title":"AI on EKS Inference Charts","type":0,"sectionRef":"#","url":"/ai-on-eks/docs/blueprints/inference/inference-charts","content":"","keywords":"","version":"Next"},{"title":"Overview‚Äã","type":1,"pageTitle":"AI on EKS Inference Charts","url":"/ai-on-eks/docs/blueprints/inference/inference-charts#overview","content":" The inference charts support the following deployment types:  GPU-based VLLM deployments - Single-node VLLM inferenceGPU-based Ray-VLLM deployments - Distributed VLLM inference with RayNeuron-based VLLM deployments - VLLM inference on AWS Inferentia chipsNeuron-based Ray-VLLM deployments - Distributed VLLM inference with Ray on Inferentia  ","version":"Next","tagName":"h2"},{"title":"Prerequisites‚Äã","type":1,"pageTitle":"AI on EKS Inference Charts","url":"/ai-on-eks/docs/blueprints/inference/inference-charts#prerequisites","content":" Before deploying the inference charts, ensure you have:  Amazon EKS cluster with GPU or AWS Neuron nodes (JARK-stack for a quick start)Helm 3.0+For GPU deployments: NVIDIA device plugin installedFor Neuron deployments: AWS Neuron device plugin installedHugging Face Hub token (stored as a Kubernetes secret)  ","version":"Next","tagName":"h2"},{"title":"Quick Start‚Äã","type":1,"pageTitle":"AI on EKS Inference Charts","url":"/ai-on-eks/docs/blueprints/inference/inference-charts#quick-start","content":" ","version":"Next","tagName":"h2"},{"title":"1. Create Hugging Face Token Secret‚Äã","type":1,"pageTitle":"AI on EKS Inference Charts","url":"/ai-on-eks/docs/blueprints/inference/inference-charts#1-create-hugging-face-token-secret","content":" Create a Kubernetes secret with your Hugging Face token:  kubectl create secret generic hf-token --from-literal=token=your_huggingface_token   ","version":"Next","tagName":"h3"},{"title":"2. Deploy a Pre-configured Model‚Äã","type":1,"pageTitle":"AI on EKS Inference Charts","url":"/ai-on-eks/docs/blueprints/inference/inference-charts#2-deploy-a-pre-configured-model","content":" Choose from the available pre-configured models and deploy:  warning These deployments will need GPU/Neuron resources which need to be enabled and cost more than CPU only instances.  # Deploy Llama 3.2 1B on GPU with VLLM helm install llama-inference ./blueprints/inference/inference-charts \\ --values ./blueprints/inference/inference-charts/values-llama-32-1b-vllm.yaml # Deploy DeepSeek R1 Distill on GPU with Ray-VLLM helm install deepseek-inference ./blueprints/inference/inference-charts \\ --values ./blueprints/inference/inference-charts/values-deepseek-r1-distill-llama-8b-ray-vllm-gpu.yaml   ","version":"Next","tagName":"h3"},{"title":"Supported Models‚Äã","type":1,"pageTitle":"AI on EKS Inference Charts","url":"/ai-on-eks/docs/blueprints/inference/inference-charts#supported-models","content":" The inference charts include pre-configured values files for the following models:  ","version":"Next","tagName":"h2"},{"title":"GPU Models‚Äã","type":1,"pageTitle":"AI on EKS Inference Charts","url":"/ai-on-eks/docs/blueprints/inference/inference-charts#gpu-models","content":" Model\tSize\tFramework\tValues FileDeepSeek R1 Distill Llama\t8B\tRay-VLLM\tvalues-deepseek-r1-distill-llama-8b-ray-vllm-gpu.yaml Llama 3.2\t1B\tVLLM\tvalues-llama-32-1b-vllm.yaml Llama 3.2\t1B\tRay-VLLM\tvalues-llama-32-1b-ray-vllm.yaml Llama 4 Scout\t17B\tVLLM\tvalues-llama-4-scout-17b-vllm.yaml Mistral Small\t24B\tRay-VLLM\tvalues-mistral-small-24b-ray-vllm.yaml  ","version":"Next","tagName":"h3"},{"title":"Neuron Models (AWS Inferentia/Trainium)‚Äã","type":1,"pageTitle":"AI on EKS Inference Charts","url":"/ai-on-eks/docs/blueprints/inference/inference-charts#neuron-models-aws-inferentiatrainium","content":" Model\tSize\tFramework\tValues FileDeepSeek R1 Distill Llama\t8B\tVLLM\tvalues-deepseek-r1-distill-llama-8b-vllm-neuron.yaml Llama 2\t13B\tRay-VLLM\tvalues-llama-2-13b-ray-vllm-neuron.yaml Llama 3\t70B\tRay-VLLM\tvalues-llama-3-70b-ray-vllm-neuron.yaml Llama 3.1\t8B\tVLLM\tvalues-llama-31-8b-vllm-neuron.yaml Llama 3.1\t8B\tRay-VLLM\tvalues-llama-31-8b-ray-vllm-neuron.yaml  ","version":"Next","tagName":"h3"},{"title":"Deployment Examples‚Äã","type":1,"pageTitle":"AI on EKS Inference Charts","url":"/ai-on-eks/docs/blueprints/inference/inference-charts#deployment-examples","content":" ","version":"Next","tagName":"h2"},{"title":"GPU Deployments‚Äã","type":1,"pageTitle":"AI on EKS Inference Charts","url":"/ai-on-eks/docs/blueprints/inference/inference-charts#gpu-deployments","content":" Deploy Llama 3.2 1B with VLLM‚Äã  helm install llama32-vllm ./blueprints/inference/inference-charts \\ --values ./blueprints/inference/inference-charts/values-llama-32-1b-vllm.yaml   Deploy DeepSeek R1 Distill with Ray-VLLM‚Äã  helm install deepseek-ray ./blueprints/inference/inference-charts \\ --values ./blueprints/inference/inference-charts/values-deepseek-r1-distill-llama-8b-ray-vllm-gpu.yaml   Deploy Mistral Small 24B with Ray-VLLM‚Äã  helm install mistral-ray ./blueprints/inference/inference-charts \\ --values ./blueprints/inference/inference-charts/values-mistral-small-24b-ray-vllm.yaml   ","version":"Next","tagName":"h3"},{"title":"Neuron Deployments‚Äã","type":1,"pageTitle":"AI on EKS Inference Charts","url":"/ai-on-eks/docs/blueprints/inference/inference-charts#neuron-deployments","content":" Deploy Llama 3.1 8B with VLLM on Inferentia‚Äã  helm install llama31-neuron ./blueprints/inference/inference-charts \\ --values ./blueprints/inference/inference-charts/values-llama-31-8b-vllm-neuron.yaml   Deploy Llama 3 70B with Ray-VLLM on Inferentia‚Äã  helm install llama3-70b-neuron ./blueprints/inference/inference-charts \\ --values ./blueprints/inference/inference-charts/values-llama-3-70b-ray-vllm-neuron.yaml   ","version":"Next","tagName":"h3"},{"title":"Configuration Options‚Äã","type":1,"pageTitle":"AI on EKS Inference Charts","url":"/ai-on-eks/docs/blueprints/inference/inference-charts#configuration-options","content":" ","version":"Next","tagName":"h2"},{"title":"Key Parameters‚Äã","type":1,"pageTitle":"AI on EKS Inference Charts","url":"/ai-on-eks/docs/blueprints/inference/inference-charts#key-parameters","content":" The chart provides extensive configuration options. Here are the most important parameters:  Parameter\tDescription\tDefaultinference.accelerator\tAccelerator type (gpu or neuron)\tgpu inference.framework\tFramework type (vllm or ray-vllm)\tvllm inference.serviceName\tName of the inference service\tinference inference.modelServer.deployment.replicas\tNumber of replicas\t1 modelParameters.modelId\tModel ID from Hugging Face Hub\tNousResearch/Llama-3.2-1B modelParameters.gpuMemoryUtilization\tGPU memory utilization\t0.8 modelParameters.maxModelLen\tMaximum model sequence length\t8192 modelParameters.tensorParallelSize\tTensor parallel size\t1 service.type\tService type\tClusterIP service.port\tService port\t8000  ","version":"Next","tagName":"h3"},{"title":"Custom Deployment‚Äã","type":1,"pageTitle":"AI on EKS Inference Charts","url":"/ai-on-eks/docs/blueprints/inference/inference-charts#custom-deployment","content":" Create your own values file for custom configurations:  inference: accelerator: gpu # or neuron framework: vllm # or ray-vllm serviceName: custom-inference modelServer: deployment: replicas: 2 resources: gpu: requests: nvidia.com/gpu: 1 limits: nvidia.com/gpu: 1 modelParameters: modelId: &quot;your-custom-model-id&quot; gpuMemoryUtilization: &quot;0.9&quot; maxModelLen: &quot;4096&quot; tensorParallelSize: &quot;1&quot;   Deploy with custom values:  helm install custom-inference ./blueprints/inference/inference-charts \\ --values custom-values.yaml   ","version":"Next","tagName":"h3"},{"title":"API Endpoints‚Äã","type":1,"pageTitle":"AI on EKS Inference Charts","url":"/ai-on-eks/docs/blueprints/inference/inference-charts#api-endpoints","content":" Once deployed, the service exposes OpenAI-compatible API endpoints:  /v1/models - List available models/v1/completions - Text completion API/v1/chat/completions - Chat completion API/metrics - Prometheus metrics endpoint  ","version":"Next","tagName":"h2"},{"title":"Example API Usage‚Äã","type":1,"pageTitle":"AI on EKS Inference Charts","url":"/ai-on-eks/docs/blueprints/inference/inference-charts#example-api-usage","content":" Note: These deployments do not create an ingress, you will need to kubectl port-forward to test from your machine, eg (for deepseek):  kubectl get svc | grep deepseek # Note the service name for deepseek, in this case deepseekr1-dis-lllama-8b-ray-vllm-gpu-ray-vllm kubectl port-forward svc/deepseekr1-dis-llama-8b-ray-vllm-gpu-ray-vllm 8000   # List models curl http://localhost:8000/v1/models # Chat completion curl -X POST http://localhost:8000/v1/chat/completions \\ -H &quot;Content-Type: application/json&quot; \\ -d '{ &quot;model&quot;: &quot;your-model-name&quot;, &quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hello!&quot;}], &quot;max_tokens&quot;: 100 }'   ","version":"Next","tagName":"h3"},{"title":"Monitoring and Observability‚Äã","type":1,"pageTitle":"AI on EKS Inference Charts","url":"/ai-on-eks/docs/blueprints/inference/inference-charts#monitoring-and-observability","content":" The charts include built-in observability features:  Fluent Bit for log collectionPrometheus metrics for monitoringGrafana dashboards for visualizations  Access metrics at the /metrics endpoint of your deployed service.  ","version":"Next","tagName":"h2"},{"title":"Troubleshooting‚Äã","type":1,"pageTitle":"AI on EKS Inference Charts","url":"/ai-on-eks/docs/blueprints/inference/inference-charts#troubleshooting","content":" ","version":"Next","tagName":"h2"},{"title":"Common Issues‚Äã","type":1,"pageTitle":"AI on EKS Inference Charts","url":"/ai-on-eks/docs/blueprints/inference/inference-charts#common-issues","content":" Pod stuck in Pending state Check if GPU/Neuron nodes are availableVerify resource requests match available hardware Model download failures Ensure Hugging Face token is correctly configuredCheck network connectivity to Hugging Face Hub Out of memory errors Adjust gpuMemoryUtilization parameterConsider using tensor parallelism for larger models  ","version":"Next","tagName":"h3"},{"title":"Logs‚Äã","type":1,"pageTitle":"AI on EKS Inference Charts","url":"/ai-on-eks/docs/blueprints/inference/inference-charts#logs","content":" Check deployment logs:  kubectl logs -l app=inference-server   For Ray deployments, check Ray cluster status:  kubectl exec -it &lt;ray-head-pod&gt; -- ray status   ","version":"Next","tagName":"h3"},{"title":"Next Steps‚Äã","type":1,"pageTitle":"AI on EKS Inference Charts","url":"/ai-on-eks/docs/blueprints/inference/inference-charts#next-steps","content":" Explore GPU-specific configurations for GPU deploymentsLearn about Neuron-specific configurations for Inferentia deployments ","version":"Next","tagName":"h2"},{"title":"Deploying Multiple Large Language Models with NVIDIA Triton Server and vLLM","type":0,"sectionRef":"#","url":"/ai-on-eks/docs/blueprints/inference/GPUs/vLLM-NVIDIATritonServer","content":"","keywords":"","version":"Next"},{"title":"What to Expect‚Äã","type":1,"pageTitle":"Deploying Multiple Large Language Models with NVIDIA Triton Server and vLLM","url":"/ai-on-eks/docs/blueprints/inference/GPUs/vLLM-NVIDIATritonServer#what-to-expect","content":" When you deploy everything as described, you can expect quick response times for your inference requests. Below is an example output from running the triton-client.py script with the Llama-2-7b-chat-hf and Mistral-7B-Instruct-v0.2 models:  Click to expand comparison results Run 1: Llama2\tRun 2: Mistral7bpython3 triton-client.py --model-name llama2 --input-prompts prompts.txt --results-file llama2_results.txt\tpython3 triton-client.py --model-name mistral7b --input-prompts prompts.txt --results-file mistral_results.txt Loading inputs from prompts.txt...\tLoading inputs from prompts.txt... Model llama2 - Request 11: 0.00 ms\tModel mistral7b - Request 3: 0.00 ms Model llama2 - Request 15: 0.02 ms\tModel mistral7b - Request 14: 0.00 ms Model llama2 - Request 3: 0.00 ms\tModel mistral7b - Request 11: 0.00 ms Model llama2 - Request 8: 0.01 ms\tModel mistral7b - Request 15: 0.00 ms Model llama2 - Request 0: 0.01 ms\tModel mistral7b - Request 5: 0.00 ms Model llama2 - Request 9: 0.01 ms\tModel mistral7b - Request 0: 0.01 ms Model llama2 - Request 14: 0.01 ms\tModel mistral7b - Request 7: 0.01 ms Model llama2 - Request 16: 0.00 ms\tModel mistral7b - Request 13: 0.00 ms Model llama2 - Request 19: 0.02 ms\tModel mistral7b - Request 9: 0.00 ms Model llama2 - Request 4: 0.02 ms\tModel mistral7b - Request 16: 0.01 ms Model llama2 - Request 10: 0.02 ms\tModel mistral7b - Request 18: 0.01 ms Model llama2 - Request 6: 0.01 ms\tModel mistral7b - Request 4: 0.01 ms Model llama2 - Request 1: 0.02 ms\tModel mistral7b - Request 8: 0.01 ms Model llama2 - Request 7: 0.02 ms\tModel mistral7b - Request 1: 0.01 ms Model llama2 - Request 18: 0.01 ms\tModel mistral7b - Request 6: 0.00 ms Model llama2 - Request 12: 0.01 ms\tModel mistral7b - Request 12: 0.00 ms Model llama2 - Request 2: 0.01 ms\tModel mistral7b - Request 17: 0.00 ms Model llama2 - Request 17: 0.02 ms\tModel mistral7b - Request 2: 0.01 ms Model llama2 - Request 13: 0.01 ms\tModel mistral7b - Request 19: 0.01 ms Model llama2 - Request 5: 0.02 ms\tModel mistral7b - Request 10: 0.02 ms Storing results into llama2_results.txt...\tStoring results into mistral_results.txt... Total time for all requests: 0.00 seconds (0.18 milliseconds)\tTotal time for all requests: 0.00 seconds (0.11 milliseconds) PASS: vLLM example\tPASS: vLLM example  Triton Server Internals and Backend Integration  NVIDIA Triton Inference Server is engineered for high-performance inference across a wide range of model types and deployment scenarios. The core strength of Triton lies in its support for various backends, which provide the flexibility and power needed to handle different types of models and workloads effectively.  Once a request reaches the Triton K8s Service, it is processed by the Triton Server. The server supports dynamic batching, allowing multiple inference requests to be grouped together to optimize processing. This is particularly useful in scenarios with high throughput requirements, as it helps reduce latency and improve overall performance.  Requests are then managed by scheduled queues, ensuring that each model's inference requests are processed in an orderly manner. The Triton Server supports selective and compute model loading, which means it can dynamically load models based on the current workload and resource availability. This feature is crucial for efficiently managing the resources in a multi-model deployment.  The backbone of Triton‚Äôs inference capabilities are its various backends, including TensorRT-LLM and vLLM:  TensorRT-LLM: TensorRT-LLM backend optimizes large language model (LLM) inference on NVIDIA GPUs. Leveraging TensorRT's high-performance capabilities, it accelerates inference, providing low-latency and high-throughput performance. TensorRT is particularly well-suited for deep learning models that require intensive computational resources, making it ideal for real-time AI applications.  vLLM: vLLM backend is specifically designed to handle various LLM workloads. It offers efficient memory management and execution pipelines tailored for large models. This backend ensures that memory resources are used optimally, allowing for the deployment of very large models without running into memory bottlenecks. vLLM is crucial for applications that need to serve multiple large models simultaneously, providing a robust and scalable solution.    ","version":"Next","tagName":"h2"},{"title":"Mistralai/Mistral-7B-Instruct-v0.2‚Äã","type":1,"pageTitle":"Deploying Multiple Large Language Models with NVIDIA Triton Server and vLLM","url":"/ai-on-eks/docs/blueprints/inference/GPUs/vLLM-NVIDIATritonServer#mistralaimistral-7b-instruct-v02","content":" Mistralai/Mistral-7B-Instruct-v0.2 is a state-of-the-art large language model designed to provide high-quality, instructive responses. Trained on a diverse dataset, it excels in understanding and generating human-like text across a variety of topics. Its capabilities make it suitable for applications requiring detailed explanations, complex queries, and natural language understanding.  ","version":"Next","tagName":"h3"},{"title":"Meta-llama/Llama-2-7b-chat-hf‚Äã","type":1,"pageTitle":"Deploying Multiple Large Language Models with NVIDIA Triton Server and vLLM","url":"/ai-on-eks/docs/blueprints/inference/GPUs/vLLM-NVIDIATritonServer#meta-llamallama-2-7b-chat-hf","content":" Meta-llama/Llama-2-7b-chat-hf is an advanced conversational AI model developed by Meta. It is optimized for chat applications, delivering coherent and contextually relevant responses. With its robust training on extensive dialogue datasets, this model excels in maintaining engaging and dynamic conversations, making it ideal for customer service bots, interactive agents, and other chat-based applications.  ","version":"Next","tagName":"h3"},{"title":"Deploying the Solution‚Äã","type":1,"pageTitle":"Deploying Multiple Large Language Models with NVIDIA Triton Server and vLLM","url":"/ai-on-eks/docs/blueprints/inference/GPUs/vLLM-NVIDIATritonServer#deploying-the-solution","content":" To get started with deploying both mistralai/Mistral-7B-Instruct-v0.2 and meta-llama/Llama-2-7b-chat-hf on Amazon EKS, we will cover the necessary prerequisites and guide you through the deployment process step by step. This process includes setting up the infrastructure, deploying the NVIDIA Triton Inference Server, and creating the Triton client Python application that sends gRPC requests to the Triton server for inferencing.  danger Important: Deploying on g5.24xlarge instances, which are equipped with multiple GPUs, can be expensive. Ensure you carefully monitor and manage your usage to avoid unexpected costs. Consider setting budget alerts and usage limits to keep track of your expenditures.  Prerequisites üëà  ","version":"Next","tagName":"h2"},{"title":"NVIDIA Triton Server with vLLM Backend‚Äã","type":1,"pageTitle":"Deploying Multiple Large Language Models with NVIDIA Triton Server and vLLM","url":"/ai-on-eks/docs/blueprints/inference/GPUs/vLLM-NVIDIATritonServer#nvidia-triton-server-with-vllm-backend","content":" This blueprint uses Triton helm chart to install and configure the Triton server on Amazon EKS. The deployment is configured using the following Terraform code in the blueprint.  Click to expand the deployment code module &quot;triton_server_vllm&quot; { depends_on = [module.eks_blueprints_addons.kube_prometheus_stack] source = &quot;aws-ia/eks-data-addons/aws&quot; version = &quot;~&gt; 1.32.0&quot; # ensure to update this to the latest/desired version oidc_provider_arn = module.eks.oidc_provider_arn enable_nvidia_triton_server = false nvidia_triton_server_helm_config = { version = &quot;1.0.0&quot; timeout = 120 wait = false namespace = kubernetes_namespace_v1.triton.metadata[0].name values = [ &lt;&lt;-EOT replicaCount: 1 image: repository: nvcr.io/nvidia/tritonserver tag: &quot;24.06-vllm-python-py3&quot; serviceAccount: create: false name: ${kubernetes_service_account_v1.triton.metadata[0].name} modelRepositoryPath: s3://${module.s3_bucket.s3_bucket_id}/model_repository environment: - name: model_name value: ${local.default_model_name} - name: &quot;LD_PRELOAD&quot; value: &quot;&quot; - name: &quot;TRANSFORMERS_CACHE&quot; value: &quot;/home/triton-server/.cache&quot; - name: &quot;shm-size&quot; value: &quot;5g&quot; - name: &quot;NCCL_IGNORE_DISABLED_P2P&quot; value: &quot;1&quot; - name: tensor_parallel_size value: &quot;1&quot; - name: gpu_memory_utilization value: &quot;0.9&quot; - name: dtype value: &quot;auto&quot; secretEnvironment: - name: &quot;HUGGING_FACE_TOKEN&quot; secretName: ${kubernetes_secret_v1.huggingface_token.metadata[0].name} key: &quot;HF_TOKEN&quot; resources: limits: cpu: 6 memory: 25Gi nvidia.com/gpu: 4 requests: cpu: 6 memory: 25Gi nvidia.com/gpu: 4 nodeSelector: NodeGroupType: g5-gpu-karpenter type: karpenter tolerations: - key: &quot;nvidia.com/gpu&quot; operator: &quot;Exists&quot; effect: &quot;NoSchedule&quot; EOT ] } }   Note: The container image that's being used for Triton server is nvcr.io/nvidia/tritonserver:24.02-vllm-python-py3 and is vLLM backend enabled. You can choose appropriate tags in the NGC Catalog.  Model Repository: The Triton Inference Server serves models from one or more model repositories specified at server startup. Triton can access models from locally accessible file paths and cloud storage locations like Amazon S3.  The directories and files that compose a model repository must follow a required layout. The repository layout should be structured as follows:  Click to expand the model directory hierarchy &lt;model-repository-path&gt;/ &lt;model-name&gt;/ [config.pbtxt] [&lt;output-labels-file&gt; ...] &lt;version&gt;/ &lt;model-definition-file&gt; &lt;version&gt;/ &lt;model-definition-file&gt; &lt;model-name&gt;/ [config.pbtxt] [&lt;output-labels-file&gt; ...] &lt;version&gt;/ &lt;model-definition-file&gt; &lt;version&gt;/ &lt;model-definition-file&gt; ... ------------- Example: ------------- model-repository/ mistral-7b/ config.pbtxt 1/ model.py llama-2/ config.pbtxt 1/ model.py   For vLLM enabled Triton model, the model_repository can be found at ai/inference/vllm-nvidia-triton-server-gpu/model_repository location. During the deployment, the blueprint creates an S3 bucket and syncs the local model_repository contents to the S3 bucket.  model.py: This script uses vLLM library as Triton backend framework and initializes a TritonPythonModel class by loading the model configuration and configuring vLLM engine. The huggingface_hub library's login function is used to establish access to the hugging face repository for model access. It then starts an asyncio event loop to process the received requests asynchronously. The script has several functions that processes the inference requests, issues the requests to vLLM backend and return the response.  config.pbtxt: This is a model configuration file that specifies parameters such as  Name - The name of the model must match the name of the model repository directory containing the model.max_batch_size - The max_batch_size value indicates the maximum batch size that the model supports for the type of batching that can be exploited by TritonInputs and Outputs - Each model input and output must specify a name, datatype, and shape. An input shape indicates the shape of an input tensor expected by the model and by Triton in inference requests. An output shape indicates the shape of an output tensor produced by the model and returned by Triton in response to an inference request. Input and output shapes are specified by a combination of max_batch_size and the dimensions specified by input dims or output dims.  ","version":"Next","tagName":"h3"},{"title":"Verify Deployment‚Äã","type":1,"pageTitle":"Deploying Multiple Large Language Models with NVIDIA Triton Server and vLLM","url":"/ai-on-eks/docs/blueprints/inference/GPUs/vLLM-NVIDIATritonServer#verify-deployment","content":" To verify that the Triton Inference Server has been deployed successfully, run the following command:  kubectl get all -n triton-vllm   Output below shows that there is one pod running the Triton server, which is hosting two models. There is one service to interact with the models, and one ReplicaSet for Triton servers. Deployment will be horizontally scaled based on custom metrics and the HPA object.  NAME READY STATUS RESTARTS AGE pod/nvidia-triton-server-triton-inference-server-c49bd559d-szlpf 1/1 Running 0 13m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/nvidia-triton-server-triton-inference-server ClusterIP 172.20.193.97 &lt;none&gt; 8000/TCP,8001/TCP,8002/TCP 13m service/nvidia-triton-server-triton-inference-server-metrics ClusterIP 172.20.5.247 &lt;none&gt; 8080/TCP 13m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nvidia-triton-server-triton-inference-server 1/1 1 1 13m NAME DESIRED CURRENT READY AGE replicaset.apps/nvidia-triton-server-triton-inference-server-c49bd559d 1 1 1 13m NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE horizontalpodautoscaler.autoscaling/nvidia-triton-server-triton-inference-server Deployment/nvidia-triton-server-triton-inference-server &lt;unknown&gt;/80%, &lt;unknown&gt;/80% 1 5 1 13m   This output indicates that the Triton server pods are running, the services are correctly set up, and the deployment is functioning as expected. The Horizontal Pod Autoscaler is also active, ensuring that the number of pods scales based on the specified metrics.  ","version":"Next","tagName":"h3"},{"title":"Testing Llama-2-7b Chat and Mistral-7b Chat Models‚Äã","type":1,"pageTitle":"Deploying Multiple Large Language Models with NVIDIA Triton Server and vLLM","url":"/ai-on-eks/docs/blueprints/inference/GPUs/vLLM-NVIDIATritonServer#testing-llama-2-7b-chat-and-mistral-7b-chat-models","content":" It's time to test both the Llama-2-7b chat and Mistral-7b chat models. We will run the following commands with the same prompts to verify the outputs generated by both models.  First, execute a port forward to the Triton-inference-server Service using kubectl:  kubectl -n triton-vllm port-forward svc/nvidia-triton-server-triton-inference-server 8001:8001   Next, run the Triton client for each model using the same prompts:  cd ai-on-eks/blueprints/inference/vllm-nvidia-triton-server-gpu/triton-client python3 -m venv .venv source .venv/bin/activate pip install tritonclient[all] python3 triton-client.py --model-name mistral7b --input-prompts prompts.txt --results-file mistral_results.txt   You will see an output something like below:  python3 triton-client.py --model-name mistral7b --input-prompts prompts.txt --results-file mistral_results.txt Loading inputs from `prompts.txt`... Model mistral7b - Request 3: 0.00 ms Model mistral7b - Request 14: 0.00 ms Model mistral7b - Request 11: 0.00 ms Model mistral7b - Request 15: 0.00 ms Model mistral7b - Request 5: 0.00 ms Model mistral7b - Request 0: 0.01 ms Model mistral7b - Request 7: 0.01 ms Model mistral7b - Request 13: 0.00 ms Model mistral7b - Request 9: 0.00 ms Model mistral7b - Request 16: 0.01 ms Model mistral7b - Request 18: 0.01 ms Model mistral7b - Request 4: 0.01 ms Model mistral7b - Request 8: 0.01 ms Model mistral7b - Request 1: 0.01 ms Model mistral7b - Request 6: 0.00 ms Model mistral7b - Request 12: 0.00 ms Model mistral7b - Request 17: 0.00 ms Model mistral7b - Request 2: 0.01 ms Model mistral7b - Request 19: 0.01 ms Model mistral7b - Request 10: 0.02 ms Storing results into `mistral_results.txt`... Total time for all requests: 0.00 seconds (0.11 milliseconds) PASS: vLLM example   Output for mistral_results.txt should like below:  Click to expand Mistral results partial output &lt;s&gt;[INST]&lt;&lt;SYS&gt;&gt; Keep short answers of no more than 100 sentences. &lt;&lt;/SYS&gt;&gt; What are the key differences between traditional machine learning models and very large language models (vLLM)? [/INST] Traditional machine learning models (MLMs) are trained on specific datasets and features to learn patterns and make predictions based on that data. They require labeled data for training and are limited by the size and diversity of the training data. MLMs can be effective for solving structured problems, such as image recognition or speech recognition. Very Large Language Models (vLLMs), on the other hand, are trained on vast amounts of text data using deep learning techniques. They learn to generate human-like text based on the input they receive. vLLMs can understand and generate text in a more contextually aware and nuanced way than MLMs. They can also perform a wider range of tasks, such as text summarization, translation, and question answering. However, vLLMs can be more computationally expensive and require large amounts of data and power to train. They also have the potential to generate inaccurate or biased responses if not properly managed. ========= &lt;s&gt;[INST]&lt;&lt;SYS&gt;&gt; Keep short answers of no more than 100 sentences. &lt;&lt;/SYS&gt;&gt; Can you explain how TensorRT optimizes LLM inference on NVIDIA hardware? [/INST] TensorRT is a deep learning inference optimization tool from NVIDIA. It utilizes dynamic and static analysis to optimize deep learning models for inference on NVIDIA GPUs. For Maximum Likelihood Modeling (LLM) inference, TensorRT applies the following optimizations: 1. Model Optimization: TensorRT converts the LLM model into an optimized format, such as INT8 or FP16, which reduces memory usage and increases inference speed. 2. Engine Generation: TensorRT generates a custom engine for the optimized model, which includes kernel optimizations for specific NVIDIA GPUs. 3. Memory Optimization: TensorRT minimizes memory usage by using data layout optimizations, memory pooling, and other techniques. 4. Execution Optimization: TensorRT optimizes the execution of the engine on the GPU by scheduling and managing thread execution, reducing latency and increasing throughput. 5. I/O Optimization: TensorRT optimizes input and output data transfer between the host and the GPU, reducing the time spent on data transfer and increasing overall inference speed. 6. Dynamic Batching: TensorRT dynamically batches input data to maximize GPU utilization and reduce latency. 7. Multi-Streaming: TensorRT supports multi-streaming, allowing multiple inference requests to be processed concurrently, increasing overall throughput. 8. Profiling and Monitoring: TensorRT provides profiling and monitoring tools to help developers identify performance bottlenecks and optimize their models further. Overall, TensorRT optimizes LLM inference on NVIDIA hardware by applying a combination of model, engine, memory, execution, I/O, dynamic batching, multi-streaming, and profiling optimizations.   Now, try to run the inference on the Llama-2-7b-chat model with the same prompts and observe the output under a new file called llama2_results.txt.  python3 triton-client.py --model-name llama2 --input-prompts prompts.txt --results-file llama2_results.txt   Output should look like:  python3 triton-client.py --model-name llama2 --input-prompts prompts.txt --results-file llama2_results.txt Loading inputs from `prompts.txt`... Model llama2 - Request 11: 0.00 ms Model llama2 - Request 15: 0.02 ms Model llama2 - Request 3: 0.00 ms Model llama2 - Request 8: 0.03 ms Model llama2 - Request 5: 0.02 ms Model llama2 - Request 0: 0.00 ms Model llama2 - Request 14: 0.00 ms Model llama2 - Request 16: 0.01 ms Model llama2 - Request 19: 0.02 ms Model llama2 - Request 4: 0.01 ms Model llama2 - Request 1: 0.01 ms Model llama2 - Request 10: 0.01 ms Model llama2 - Request 9: 0.01 ms Model llama2 - Request 7: 0.01 ms Model llama2 - Request 18: 0.01 ms Model llama2 - Request 12: 0.00 ms Model llama2 - Request 2: 0.00 ms Model llama2 - Request 6: 0.00 ms Model llama2 - Request 17: 0.01 ms Model llama2 - Request 13: 0.01 ms Storing results into `llama2_results.txt`... Total time for all requests: 0.00 seconds (0.18 milliseconds) PASS: vLLM example   ","version":"Next","tagName":"h3"},{"title":"Observability‚Äã","type":1,"pageTitle":"Deploying Multiple Large Language Models with NVIDIA Triton Server and vLLM","url":"/ai-on-eks/docs/blueprints/inference/GPUs/vLLM-NVIDIATritonServer#observability","content":" ","version":"Next","tagName":"h2"},{"title":"Observability with AWS CloudWatch and Neuron Monitor‚Äã","type":1,"pageTitle":"Deploying Multiple Large Language Models with NVIDIA Triton Server and vLLM","url":"/ai-on-eks/docs/blueprints/inference/GPUs/vLLM-NVIDIATritonServer#observability-with-aws-cloudwatch-and-neuron-monitor","content":" This blueprint deploys the CloudWatch Observability Agent as a managed add-on, providing comprehensive monitoring for containerized workloads. It includes container insights for tracking key performance metrics such as CPU and memory utilization. Additionally, the blueprint integrates GPU metrics using NVIDIA's DCGM plugin, which is essential for monitoring high-performance GPU workloads. For machine learning models running on AWS Inferentia or Trainium, the Neuron Monitor plugin is added to capture and report Neuron-specific metrics.  All metrics, including container insights, GPU performance, and Neuron metrics, are sent to Amazon CloudWatch, where you can monitor and analyze them in real-time. After the deployment is complete, you should be able to access these metrics directly from the CloudWatch console, allowing you to manage and optimize your workloads effectively.  In addition to deploying CloudWatch EKS addon, we have also deployed the Kube Prometheus stack, which provides Prometheus server and Grafana deployments for monitoring and observability.  First, let's verify the services deployed by the Kube Prometheus stack:  kubectl get svc -n kube-prometheus-stack   You should see output similar to this:  kubectl get svc -n kube-prometheus-stack NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kube-prometheus-stack-grafana ClusterIP 172.20.252.10 &lt;none&gt; 80/TCP 11d kube-prometheus-stack-kube-state-metrics ClusterIP 172.20.34.181 &lt;none&gt; 8080/TCP 11d kube-prometheus-stack-operator ClusterIP 172.20.186.93 &lt;none&gt; 443/TCP 11d kube-prometheus-stack-prometheus ClusterIP 172.20.147.64 &lt;none&gt; 9090/TCP,8080/TCP 11d kube-prometheus-stack-prometheus-node-exporter ClusterIP 172.20.171.165 &lt;none&gt; 9100/TCP 11d prometheus-operated ClusterIP None &lt;none&gt; 9090/TCP 11d   To expose the NVIDIA Triton server metrics, we have deployed a metrics service(nvidia-triton-server-triton-inference-server-metrics) on port 8080. Verify it by running  kubectl get svc -n triton-vllm   The output should be:  kubectl get svc -n triton-vllm NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nvidia-triton-server-triton-inference-server ClusterIP 172.20.193.97 &lt;none&gt; 8000/TCP,8001/TCP,8002/TCP 34m nvidia-triton-server-triton-inference-server-metrics ClusterIP 172.20.5.247 &lt;none&gt; 8080/TCP 34m   This confirms that the NVIDIA Triton server metrics are being scraped by the Prometheus server. You can visualize these metrics using the Grafana dashboard.  In the Grafana dashboard below, you can see several important metrics:  Average GPU Power Usage: This gauge shows the current power usage of the GPU, which is crucial for monitoring the efficiency and performance of your inference tasks.Compute Time (milliseconds): This bar graph displays the time taken to compute inference requests, helping identify any latency issues.Cumulative Inference Requests: This graph shows the total number of inference requests processed over time, providing insights into the workload and performance trends.Queue Time (milliseconds): This line graph indicates the time requests spend in the queue before being processed, highlighting potential bottlenecks in the system.    To create a new Grafana dashboard to monitor these metrics, follow the steps below:  - Port-forward Grafana service: kubectl port-forward svc/kube-prometheus-stack-grafana 8080:80 -n kube-prometheus-stack - Grafana Admin user admin - Get secret name from Terraform output terraform output grafana_secret_name - Get admin user password aws secretsmanager get-secret-value --secret-id &lt;grafana_secret_name_output&gt; --region $AWS_REGION --query &quot;SecretString&quot; --output text   Login to Grafana:  Open your web browser and navigate to http://localhost:8080.Login with the username admin and the password retrieved from AWS Secrets Manager.  Import an Open Source Grafana Dashboard:  Once logged in, click on the &quot;+&quot; icon on the left sidebar and select &quot;Import&quot;.Enter the following URL to import the dashboard JSON: Triton Server Grafana DashboardFollow the prompts to complete the import process.  You should now see the metrics displayed on your new Grafana dashboard, allowing you to monitor the performance and health of your NVIDIA Triton Inference Server deployment.    ","version":"Next","tagName":"h3"},{"title":"Conclusion‚Äã","type":1,"pageTitle":"Deploying Multiple Large Language Models with NVIDIA Triton Server and vLLM","url":"/ai-on-eks/docs/blueprints/inference/GPUs/vLLM-NVIDIATritonServer#conclusion","content":" Deploying and managing multiple large language models on Amazon EKS with NVIDIA Triton Inference Server and vLLM backend offers a powerful and scalable solution for modern AI applications. By following this blueprint, you have set up the necessary infrastructure, deployed the Triton server, and configured robust observability using the Kube Prometheus stack and Grafana.  ","version":"Next","tagName":"h2"},{"title":"Cleanup‚Äã","type":1,"pageTitle":"Deploying Multiple Large Language Models with NVIDIA Triton Server and vLLM","url":"/ai-on-eks/docs/blueprints/inference/GPUs/vLLM-NVIDIATritonServer#cleanup","content":" Finally, we'll provide instructions for cleaning up and deprovisioning the resources when they are no longer needed.  Cleanup the EKS Cluster:This script will cleanup the environment using -target option to ensure all the resources are deleted in correct order.  export AWS_DEAFULT_REGION=&quot;DEPLOYED_EKS_CLUSTER_REGION&gt;&quot; cd ai-on-eks/infra/nvidia-triton-server/ &amp;&amp; chmod +x cleanup.sh ./cleanup.sh  ","version":"Next","tagName":"h2"},{"title":"Serving Llama-2-13b Chat Model with Inferentia, Ray Serve and Gradio","type":0,"sectionRef":"#","url":"/ai-on-eks/docs/blueprints/inference/Neuron/llama2-inf2","content":"","keywords":"","version":"Next"},{"title":"What is Llama-2?‚Äã","type":1,"pageTitle":"Serving Llama-2-13b Chat Model with Inferentia, Ray Serve and Gradio","url":"/ai-on-eks/docs/blueprints/inference/Neuron/llama2-inf2#what-is-llama-2","content":" Llama-2 is a pretrained large language model (LLM) trained on 2 trillion tokens of text and code. It is one of the largest and most powerful LLMs available today. Llama-2 can be used for a variety of tasks, including natural language processing, text generation, and translation.  Llama-2-chat‚Äã  Llama-2 is a remarkable language model that has undergone a rigorous training process. It starts with pretraining using publicly available online data. An initial version of Llama-2-chat is then created through supervised fine-tuning. Following that, Llama-2-chat undergoes iterative refinement using Reinforcement Learning from Human Feedback (RLHF), which includes techniques like rejection sampling and proximal policy optimization (PPO). This process results in a highly capable and fine-tuned language model that we will guide you to deploy and utilize effectively on Amazon EKS with Ray Serve.  Llama-2 is available in three different model sizes:  Llama-2-70b: This is the largest Llama-2 model, with 70 billion parameters. It is the most powerful Llama-2 model and can be used for the most demanding tasks.Llama-2-13b: This is a medium-sized Llama-2 model, with 13 billion parameters. It is a good balance between performance and efficiency, and can be used for a variety of tasks.Llama-2-7b: This is the smallest Llama-2 model, with 7 billion parameters. It is the most efficient Llama-2 model and can be used for tasks that do not require the highest level of performance.  ","version":"Next","tagName":"h3"},{"title":"Which Llama-2 model size should I use?‚Äã","type":1,"pageTitle":"Serving Llama-2-13b Chat Model with Inferentia, Ray Serve and Gradio","url":"/ai-on-eks/docs/blueprints/inference/Neuron/llama2-inf2#which-llama-2-model-size-should-i-use","content":" The best Llama-2 model size for you will depend on your specific needs. and it may not always be the largest model for achieving the highest performance. It's advisable to evaluate your needs and consider factors such as computational resources, response time, and cost-efficiency when selecting the appropriate Llama-2 model size. The decision should be based on a comprehensive assessment of your application's goals and constraints.  ","version":"Next","tagName":"h3"},{"title":"Inference on Trn1/Inf2 Instances: Unlocking the Full Potential of Llama-2‚Äã","type":1,"pageTitle":"Serving Llama-2-13b Chat Model with Inferentia, Ray Serve and Gradio","url":"/ai-on-eks/docs/blueprints/inference/Neuron/llama2-inf2#inference-on-trn1inf2-instances-unlocking-the-full-potential-of-llama-2","content":" Llama-2 can be deployed on a variety of hardware platforms, each with its own set of advantages. However, when it comes to maximizing the efficiency, scalability, and cost-effectiveness of Llama-2, AWS Trn1/Inf2 instances shine as the optimal choice.  Scalability and AvailabilityOne of the key challenges in deploying large language models (LLMs) like Llama-2 is the scalability and availability of suitable hardware. Traditional GPU instances often face scarcity due to high demand, making it challenging to provision and scale resources effectively. In contrast, Trn1/Inf2 instances, such as trn1.32xlarge, trn1n.32xlarge, inf2.24xlarge and inf2.48xlarge, are purpose built for high-performance deep learning (DL) training and inference of generative AI models, including LLMs. They offer both scalability and availability, ensuring that you can deploy and scale your Llama-2 models as needed, without resource bottlenecks or delays.  Cost Optimization:Running LLMs on traditional GPU instances can be cost-prohibitive, especially given the scarcity of GPUs and their competitive pricing.Trn1/Inf2 instances provide a cost-effective alternative. By offering dedicated hardware optimized for AI and machine learning tasks, Trn1/Inf2 instances allow you to achieve top-notch performance at a fraction of the cost. This cost optimization enables you to allocate your budget efficiently, making LLM deployment accessible and sustainable.  Performance BoostWhile Llama-2 can achieve high-performance inference on GPUs, Neuron accelerators take performance to the next level. Neuron accelerators are purpose-built for machine learning workloads, providing hardware acceleration that significantly enhances Llama-2's inference speeds. This translates to faster response times and improved user experiences when deploying Llama-2 on Trn1/Inf2 instances.  ","version":"Next","tagName":"h2"},{"title":"Model Specification‚Äã","type":1,"pageTitle":"Serving Llama-2-13b Chat Model with Inferentia, Ray Serve and Gradio","url":"/ai-on-eks/docs/blueprints/inference/Neuron/llama2-inf2#model-specification","content":" The table provides information about the different sizes of Llama-2 models, their weights, and the hardware requirements for deploying them. This information can be used to design the infrastructure required to deploy any size of Llama-2 model. For example, if you want to deploy the Llama-2-13b-chat model, you will need to use an instance type with at least 26 GB of total accelerator memory.  Model\tWeights\tBytes\tParameter Size (Billions)\tTotal Accelerator Memory (GB)\tAccelerator Memory Size for NeuronCore (GB)\tRequired Neuron Cores\tRequired Neuron Accelerators\tInstance Type\ttp_degreeMeta/Llama-2-70b\tfloat16\t2\t70\t140\t16\t9\t5\tinf2.48x\t24 Meta/Llama-2-13b\tfloat16\t2\t13\t26\t16\t2\t1\tinf2.24x\t12 Meta/Llama-2-7b\tfloat16\t2\t7\t14\t16\t1\t1\tinf2.24x\t12  ","version":"Next","tagName":"h3"},{"title":"Example usecase‚Äã","type":1,"pageTitle":"Serving Llama-2-13b Chat Model with Inferentia, Ray Serve and Gradio","url":"/ai-on-eks/docs/blueprints/inference/Neuron/llama2-inf2#example-usecase","content":" A company wants to deploy a Llama-2 chatbot to provide customer support. The company has a large customer base and expects to receive a high volume of chat requests at peak times. The company needs to design an infrastructure that can handle the high volume of requests and provide a fast response time.  The company can use Inferentia2 instances to scale its Llama-2 chatbot efficiently. Inferentia2 instances are specialized hardware accelerators for machine learning tasks. They can provide up to 20x better performance and up to 7x lower cost than GPUs for machine learning workloads.  The company can also use Ray Serve to horizontally scale its Llama-2 chatbot. Ray Serve is a distributed framework for serving machine learning models. It can automatically scale your models up or down based on demand.  To scale its Llama-2 chatbot, the company can deploy multiple Inferentia2 instances and use Ray Serve to distribute the traffic across the instances. This will allow the company to handle a high volume of requests and provide a fast response time.  ","version":"Next","tagName":"h3"},{"title":"Solution Architecture‚Äã","type":1,"pageTitle":"Serving Llama-2-13b Chat Model with Inferentia, Ray Serve and Gradio","url":"/ai-on-eks/docs/blueprints/inference/Neuron/llama2-inf2#solution-architecture","content":" In this section, we will delve into the architecture of our solution, which combines Llama-2 model, Ray Serve and Inferentia2 on Amazon EKS.    ","version":"Next","tagName":"h2"},{"title":"Deploying the Solution‚Äã","type":1,"pageTitle":"Serving Llama-2-13b Chat Model with Inferentia, Ray Serve and Gradio","url":"/ai-on-eks/docs/blueprints/inference/Neuron/llama2-inf2#deploying-the-solution","content":" To get started with deploying Llama-2-13b chat on Amazon EKS, we will cover the necessary prerequisites and guide you through the deployment process step by step. This includes setting up the infrastructure, deploying the Ray cluster, and creating the Gradio WebUI app.  Prerequisites üëà  ","version":"Next","tagName":"h2"},{"title":"Deploying the Ray Cluster with Llama-2-Chat Model‚Äã","type":1,"pageTitle":"Serving Llama-2-13b Chat Model with Inferentia, Ray Serve and Gradio","url":"/ai-on-eks/docs/blueprints/inference/Neuron/llama2-inf2#deploying-the-ray-cluster-with-llama-2-chat-model","content":" Once the Trainium on EKS Cluster is deployed, you can proceed to use kubectl to deploy the ray-service-Llama-2.yaml.  In this step, we will deploy the Ray Serve cluster, which comprises one Head Pod on x86 CPU instances using Karpenter autoscaling, as well as Ray workers on Inf2.48xlarge instances, autoscaled by Karpenter.  Let's take a closer look at the key files used in this deployment and understand their functionalities before proceeding with the deployment:  ray_serve_Llama-2.py:This script uses FastAPI, Ray Serve, and PyTorch-based Hugging Face Transformers to create an efficient API for text generation using the NousResearch/Llama-2-13b-chat-hf language model. Alternatively, users have the flexibility to switch to the meta-llama/Llama-2-13b-chat-hf model. The script establishes an endpoint that accepts input sentences and efficiently generates text outputs, benefiting from Neuron acceleration for enhanced performance. With its high configurability, users can fine-tune model parameters to suit a wide range of natural language processing applications, including chatbots and text generation tasks. ray-service-Llama-2.yaml:This Ray Serve YAML file serves as a Kubernetes configuration for deploying the Ray Serve service, facilitating efficient text generation using the Llama-2-13b-chat model. It defines a Kubernetes namespace named Llama-2 to isolate resources. Within the configuration, the RayService specification, named Llama-2-service, is created and hosted within the Llama-2 namespace. The RayService specification leverages the Python script ray_serve_Llama-2.py (copied into the Dockerfile located within the same folder) to create the Ray Serve service. The Docker image used in this example is publicly available on Amazon Elastic Container Registry (ECR) for ease of deployment. Users can also modify the Dockerfile to suit their specific requirements and push it to their own ECR repository, referencing it in the YAML file.  ","version":"Next","tagName":"h2"},{"title":"Step1: Deploy the Llama-2-Chat Model‚Äã","type":1,"pageTitle":"Serving Llama-2-13b Chat Model with Inferentia, Ray Serve and Gradio","url":"/ai-on-eks/docs/blueprints/inference/Neuron/llama2-inf2#step1-deploy-the-llama-2-chat-model","content":" Ensure the cluster is configured locally  aws eks --region us-west-2 update-kubeconfig --name trainium-inferentia   Deploy RayServe Cluster  cd ai-on-eks/blueprints/inference/llama2-13b-chat-rayserve-inf2 kubectl apply -f ray-service-llama2.yaml   Verify the deployment by running the following commands  info The deployment process may take up to 10 minutes. The Head Pod is expected to be ready within 2 to 3 minutes, while the Ray Serve worker pod may take up to 10 minutes for image retrieval and Model deployment from Huggingface.  kubectl get all -n llama2   Output:  NAME READY STATUS RESTARTS AGE pod/llama2-raycluster-fcmtr-head-bf58d 1/1 Running 0 67m pod/llama2-raycluster-fcmtr-worker-inf2-lgnb2 1/1 Running 0 5m30s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/llama2 ClusterIP 172.20.118.243 &lt;none&gt; 10001/TCP,8000/TCP,8080/TCP,6379/TCP,8265/TCP 67m service/llama2-head-svc ClusterIP 172.20.168.94 &lt;none&gt; 8080/TCP,6379/TCP,8265/TCP,10001/TCP,8000/TCP 57m service/llama2-serve-svc ClusterIP 172.20.61.167 &lt;none&gt; 8000/TCP 57m NAME DESIRED WORKERS AVAILABLE WORKERS CPUS MEMORY GPUS STATUS AGE raycluster.ray.io/llama2-raycluster-fcmtr 1 1 184 704565270Ki 0 ready 67m NAME SERVICE STATUS NUM SERVE ENDPOINTS rayservice.ray.io/llama2 Running 2   kubectl get ingress -n llama2   Output:  NAME CLASS HOSTS ADDRESS PORTS AGE llama2 nginx * k8s-ingressn-ingressn-aca7f16a80-1223456666.elb.us-west-2.amazonaws.com 80 69m   caution This blueprint deploys an internal load balancer for security reasons, so you may not be able to access it from the browser unless you are in the same vpc. You can modify the blueprint to make the NLB public by following the instructions here. Alternatively, you can use port-forwarding to test the service without using a load balancer.  Now, you can access the Ray Dashboard using the Load Balancer URL below by replacing &lt;NLB_DNS_NAME&gt; with your NLB endpoint:  http://\\&lt;NLB_DNS_NAME\\&gt;/dashboard/#/serve   If you don't have access to a public Load Balancer, you can use port-forwarding and browse the Ray Dashboard using localhost with the following command:  kubectl port-forward service/llama2 8265:8265 -n llama2   Open the link in the browser: http://localhost:8265/  From this webpage, you will be able to monitor the progress of Model deployment, as shown in the image below:    ","version":"Next","tagName":"h3"},{"title":"Step2: To Test the Llama-2-Chat Model‚Äã","type":1,"pageTitle":"Serving Llama-2-13b Chat Model with Inferentia, Ray Serve and Gradio","url":"/ai-on-eks/docs/blueprints/inference/Neuron/llama2-inf2#step2-to-test-the-llama-2-chat-model","content":" Once you see the status of the model deployment is in running state then you can start using Llama-2-chat.  Using Port-Forwarding  First, use port-forwarding to access the service locally:  kubectl port-forward service/llama2-serve-svc 8000:8000 -n llama2   Then, you can test the model using the following URL with a query added at the end of the URL:  http://localhost:8000/infer?sentence=what is data parallelism and tensor parallelism and the differences   You will see an output like this in your browser.    Using the NLB:  If you prefer to use a Network Load Balancer (NLB), you can modify the blueprint to make the NLB public by following the instructions here.  Then, you can use the following URL with a query added at the end of the URL:  http://\\&lt;NLB_DNS_NAME\\&gt;/serve/infer?sentence=what is data parallelism and tensor parallelisma and the differences   You will see an output like this in your browser:    ","version":"Next","tagName":"h3"},{"title":"Step3: Deploying the Gradio WebUI App‚Äã","type":1,"pageTitle":"Serving Llama-2-13b Chat Model with Inferentia, Ray Serve and Gradio","url":"/ai-on-eks/docs/blueprints/inference/Neuron/llama2-inf2#step3-deploying-the-gradio-webui-app","content":" Gradio Web UI is used to interact with the Llama2 inference service deployed on EKS Clusters using inf2 instances. The Gradio UI communicates internally with the Llama2 service(llama2-serve-svc.llama2.svc.cluster.local:8000), which is exposed on port 8000, using its service name and port.  We have created a base Docker(ai/inference/gradio-ui/Dockerfile-gradio-base) image for the Gradio app, which can be used with any model inference. This image is published on Public ECR.  Steps to Deploy a Gradio App:‚Äã  The following YAML script (ai/inference/llama2-13b-chat-rayserve-inf2/gradio-ui.yaml) creates a dedicated namespace, deployment, service, and a ConfigMap where your model client script goes.  To deploy this, execute:  cd ai-on-eks/blueprints/inference/llama2-13b-chat-rayserve-inf2/ kubectl apply -f gradio-ui.yaml   Verification Steps:Run the following commands to verify the deployment, service, and ConfigMap:  kubectl get deployments -n gradio-llama2-inf2 kubectl get services -n gradio-llama2-inf2 kubectl get configmaps -n gradio-llama2-inf2   Port-Forward the Service:  Run the port-forward command so that you can access the Web UI locally:  kubectl port-forward service/gradio-service 7860:7860 -n gradio-llama2-inf2   Invoke the WebUI‚Äã  Open your web browser and access the Gradio WebUI by navigating to the following URL:  Running on local URL: http://localhost:7860  You should now be able to interact with the Gradio application from your local machine.    ","version":"Next","tagName":"h3"},{"title":"Conclusion‚Äã","type":1,"pageTitle":"Serving Llama-2-13b Chat Model with Inferentia, Ray Serve and Gradio","url":"/ai-on-eks/docs/blueprints/inference/Neuron/llama2-inf2#conclusion","content":" In conclusion, you will have successfully deployed the Llama-2-13b chat model on EKS with Ray Serve and created a chatGPT-style chat web UI using Gradio. This opens up exciting possibilities for natural language processing and chatbot development.  In summary, when it comes to deploying and scaling Llama-2, AWS Trn1/Inf2 instances offer a compelling advantage. They provide the scalability, cost optimization, and performance boost needed to make running large language models efficient and accessible, all while overcoming the challenges associated with the scarcity of GPUs. Whether you're building chatbots, natural language processing applications, or any other LLM-driven solution, Trn1/Inf2 instances empower you to harness the full potential of Llama-2 on the AWS cloud.  ","version":"Next","tagName":"h2"},{"title":"Cleanup‚Äã","type":1,"pageTitle":"Serving Llama-2-13b Chat Model with Inferentia, Ray Serve and Gradio","url":"/ai-on-eks/docs/blueprints/inference/Neuron/llama2-inf2#cleanup","content":" Finally, we'll provide instructions for cleaning up and deprovisioning the resources when they are no longer needed.  Step1: Delete Gradio App and Llama2 Inference deployment  cd ai-on-eks/blueprints/inference/llama2-13b-chat-rayserve-inf2 kubectl delete -f gradio-ui.yaml kubectl delete -f ray-service-llama2.yaml   Step2: Cleanup the EKS Cluster This script will cleanup the environment using -target option to ensure all the resources are deleted in correct order.  cd ai-on-eks/infra/trainium-inferentia ./cleanup.sh  ","version":"Next","tagName":"h2"},{"title":"Deploying LLMs with RayServe and vLLM","type":0,"sectionRef":"#","url":"/ai-on-eks/docs/blueprints/inference/GPUs/vLLM-rayserve","content":"","keywords":"","version":"Next"},{"title":"RayServe and vLLM Backend Integration‚Äã","type":1,"pageTitle":"Deploying LLMs with RayServe and vLLM","url":"/ai-on-eks/docs/blueprints/inference/GPUs/vLLM-rayserve#rayserve-and-vllm-backend-integration","content":" vLLM: is a high-throughput and memory-efficient inference and serving engine for large language models (LLMs), specifically designed to optimize deployment and inference performance. A standout feature is PagedAttention, an innovative attention algorithm inspired by virtual memory paging in operating systems. PagedAttention efficiently manages the attention key and value tensors (KV cache) by storing them in non-contiguous memory spaces, which significantly reduces memory fragmentation and waste. Checkout the blog for comparing vLLM with HuggingFace Transformers (HF) and HuggingFace Text Generation Inference (TGI).  vLLM employs continuous batching of incoming requests, optimizing the use of computational resources and improving inference speed by grouping multiple requests together. This dynamic batching maximizes throughput and reduces latency. The engine also features optimized CUDA kernels for accelerated model execution on GPUs. Another key advantage is vLLM's efficient memory sharing during parallel sampling, where multiple output sequences are generated from a single prompt. This reduces memory usage by up to 55% and improves throughput by up to 2.2 times.  ","version":"Next","tagName":"h2"},{"title":"Mistralai/Mistral-7B-Instruct-v0.2‚Äã","type":1,"pageTitle":"Deploying LLMs with RayServe and vLLM","url":"/ai-on-eks/docs/blueprints/inference/GPUs/vLLM-rayserve#mistralaimistral-7b-instruct-v02","content":" In this guide, we are deploying the Mistral-7B-Instruct-v0.2 model with RayServe and vLLM. You can easily adapt these instructions to deploy any large language model, such as Llama2. Mistral-7B-Instruct-v0.2 is a state-of-the-art large language model designed to provide high-quality, instructive responses. Trained on a diverse dataset, it excels in understanding and generating human-like text across a variety of topics, making it suitable for applications requiring detailed explanations, complex queries, and natural language understanding.  Deploying the Solution üëà  ","version":"Next","tagName":"h3"},{"title":"Deploying Mistral-7B-Instruct-v0.2 with RayServe and vLLM‚Äã","type":1,"pageTitle":"Deploying LLMs with RayServe and vLLM","url":"/ai-on-eks/docs/blueprints/inference/GPUs/vLLM-rayserve#deploying-mistral-7b-instruct-v02-with-rayserve-and-vllm","content":" Having deployed the EKS cluster with all the necessary components, we can now proceed with the steps to deploy Mistral-7B-Instruct-v0.2 using RayServe and vLLM.  Step1: As a prerequisite for this deployment, you must ensure that you have access to the model via your Hugging Face account:    Step2: Export Hugginface Hub Token  To deploy the Mistral-7B-Instruct-v0.2 model with RayServe and vLLM backend, it's essential to configure your Hugging Face Hub token as an environment variable. This token is required for authentication and accessing the model. For guidance on how to create and manage your Hugging Face tokens, please visit Hugging Face Token Management.  Replace Your-Hugging-Face-Hub-Token-Value with your actual Hugging Face Hub Token. This step ensures that your deployment has the necessary authentication to access the Mistral-7B-Instruct-v0.2 model.  export HUGGING_FACE_HUB_TOKEN=$(echo -n &quot;Your-Hugging-Face-Hub-Token-Value&quot; | base64)   Step3: To deploy the RayService cluster, navigate to the directory containing the ray-service-vllm.yaml file and execute the deployment command. For more information about the RayService YAML configuration, you can refer to the file located at ai-on-eks/blueprints/inference/vllm-rayserve-gpu/ray-service-vllm.yaml.  Execute the following commands in your terminal. This will apply the RayService configuration and deploy the cluster on your EKS setup.  cd ai-on-eks/blueprints/inference/vllm-rayserve-gpu envsubst &lt; ray-service-vllm.yaml| kubectl apply -f -   Step4: Verify the deployment by running the following commands  To ensure that the deployment has been successfully completed, run the following commands:  info Deployment process may take up to 10 minutes. The Head Pod is expected to be ready within 5 to 6 minutes, while the Ray Serve worker pod may take up to 10 minutes for image retrieval and Model deployment from Huggingface.  According to the RayServe configuration, you will have one Ray head pod running on an x86 instance and one worker pod running on a g5 GPU instance. You can modify the RayServe YAML file to run multiple replicas; however, be aware that each additional replica will require a separate GPU, potentially creating new instances.  kubectl get pod -n rayserve-vllm   NAME READY STATUS RESTARTS AGE vllm-raycluster-nvtxg-head-g2cg8 1/1 Running 0 47m vllm-raycluster-nvtxg-worker-gpu-group-msl5p 1/1 Running 0 47m   This deployment also configures a Mistral service with multiple ports. Port 8265 is designated for the Ray dashboard, and port 8000 is for the Mistral model endpoint.  Run the following command to verify the services:  kubectl get svc -n rayserve-vllm NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE vllm ClusterIP 172.20.208.16 &lt;none&gt; 6379/TCP,8265/TCP,10001/TCP,8000/TCP,8080/TCP 48m vllm-head-svc ClusterIP 172.20.239.237 &lt;none&gt; 6379/TCP,8265/TCP,10001/TCP,8000/TCP,8080/TCP 37m vllm-serve-svc ClusterIP 172.20.196.195 &lt;none&gt; 8000/TCP 37m   To access the Ray dashboard, you can port-forward the relevant port to your local machine:  kubectl -n rayserve-vllm port-forward svc/vllm 8265:8265   You can then access the web UI at http://localhost:8265, which displays the deployment of jobs and actors within the Ray ecosystem.    Once the deployment is complete, the Controller and Proxy status should be HEALTHY and Application status should be RUNNING    ","version":"Next","tagName":"h2"},{"title":"Testing Mistral-7b Chat Model‚Äã","type":1,"pageTitle":"Deploying LLMs with RayServe and vLLM","url":"/ai-on-eks/docs/blueprints/inference/GPUs/vLLM-rayserve#testing-mistral-7b-chat-model","content":" Now it's time to test the Mistral-7B chat model. We'll use a Python client script to send prompts to the RayServe inference endpoint and verify the outputs generated by the model. The script reads prompts from a prompts.txt file and writes the responses to a results.txt file in the same location. It also logs the response times and token lengths for each response.  First, execute a port forward to the vllm-serve-svc Service using kubectl:  kubectl -n rayserve-vllm port-forward svc/vllm-serve-svc 8000:8000   client.py uses the HTTP POST method to send a list of prompts to the inference endpoint for text completion and Q&amp;A, targeting the /vllm model endpoint.  You can test with your custom prompts by adding them to the prompts.txt file.  To run the Python client application in a virtual environment, follow these steps:  cd ai-on-eks/blueprints/inference/vllm-rayserve-gpu python3 -m venv .venv source .venv/bin/activate pip install requests python3 client.py   You will see an output something like below in the terminal:  Click to expand Python Client Terminal output python3 client.py INFO:__main__:Warm-up successful INFO:__main__:Response status: 200 INFO:__main__:Response status: 200 INFO:__main__:Response status: 200 INFO:__main__:Response status: 200 INFO:__main__:Response status: 200 INFO:__main__:Response status: 200 INFO:__main__:Response status: 200 INFO:__main__:Response status: 200 INFO:__main__:Response status: 200 INFO:__main__:Response status: 200 INFO:__main__:Response status: 200 INFO:__main__:Response status: 200 INFO:__main__:Response status: 200 INFO:__main__:Response status: 200 INFO:__main__:Response status: 200 INFO:__main__:Response status: 200 INFO:__main__:Response status: 200 INFO:__main__:Response status: 200 INFO:__main__:Response status: 200 INFO:__main__:Response status: 200 Prompt: [INST] Explain the concept of generative adversarial networks (GANs). [/INST] Response Time: 20.72 seconds Token Length: 440 ================================================================================ Prompt: [INST] How does a variational autoencoder (VAE) work? [/INST] Response Time: 18.88 seconds Token Length: 397 ================================================================================ Prompt: [INST] What are the applications of generative AI in healthcare? [/INST] Response Time: 15.22 seconds Token Length: 323 ================================================================================ Prompt: [INST] Describe the process of training a GAN. [/INST] Response Time: 20.82 seconds Token Length: 437 ================================================================================ Prompt: [INST] How can generative AI be used in creative arts? [/INST] Response Time: 21.64 seconds Token Length: 454 ================================================================================ Prompt: [INST] What is the difference between supervised and unsupervised learning in the context of generative AI? [/INST] Response Time: 13.76 seconds Token Length: 310 ================================================================================ Prompt: [INST] Explain the role of a discriminator in a GAN. [/INST] Response Time: 11.96 seconds Token Length: 259 ================================================================================ Prompt: [INST] How can generative AI improve natural language processing (NLP)? [/INST] Response Time: 19.92 seconds Token Length: 393 ================================================================================ Prompt: [INST] What are the ethical considerations of using generative AI? [/INST] Response Time: 17.59 seconds Token Length: 361 ================================================================================ Prompt: [INST] How is generative AI used in drug discovery? [/INST] Response Time: 14.31 seconds Token Length: 311 ================================================================================ Prompt: [INST] Describe the architecture of a Transformer model. [/INST] Response Time: 26.96 seconds Token Length: 521 ================================================================================ Prompt: [INST] How can generative AI be applied in the gaming industry? [/INST] Response Time: 16.43 seconds Token Length: 348 ================================================================================ Prompt: [INST] What is the purpose of latent space in generative models? [/INST] Response Time: 11.55 seconds Token Length: 253 ================================================================================ Prompt: [INST] How does text generation with GPT-3 work? [/INST] Response Time: 12.64 seconds Token Length: 265 ================================================================================ Prompt: [INST] What are the challenges of using generative AI in finance? [/INST] Response Time: 18.21 seconds Token Length: 331 ================================================================================ Prompt: [INST] Explain the concept of zero-shot learning in generative AI. [/INST] Response Time: 14.92 seconds Token Length: 340 ================================================================================ Prompt: [INST] How can generative AI be used for image synthesis? [/INST] Response Time: 17.81 seconds Token Length: 352 ================================================================================ Prompt: [INST] What are some real-world applications of deepfakes? [/INST] Response Time: 14.39 seconds Token Length: 284 ================================================================================ Prompt: [INST] How can generative AI contribute to personalized medicine? [/INST] Response Time: 16.90 seconds Token Length: 338 ================================================================================ Prompt: [INST] Describe the use of generative AI in autonomous vehicles. [/INST] Response Time: 13.99 seconds Token Length: 299 ================================================================================   Verify the results.txt file for actual responses for each prompt.  Click to expand Mistral results partial output Prompt: [INST] Explain the theory of relativity. Response: [INST] Explain the theory of relativity. [/INST] The theory of relativity, developed by Albert Einstein, is a fundamental theory in physics that describes the relationship between space and time, and how matter and energy interact within that framework. It is actually composed of two parts: the Special Theory of Relativity, published in 1905, and the General Theory of Relativity, published in 1915. The Special Theory of Relativity is based on two postulates: the first one states that the laws of physics are the same in all inertial frames of reference (frames that are not accelerating); the second one asserts that the speed of light in a vacuum is the same for all observers, regardless of their motion or the source of the light. From these two postulates, several counter-intuitive consequences follow. For example, the length of an object contracts when it is in motion relative to an observer, and time dilation occurs, meaning that a moving clock appears to tick slower than a stationary one. These phenomena have been confirmed by numerous experiments. The General Theory of Relativity is a theory of gravitation, which extended the Special Theory of Relativity by incorporating gravity into the fabric of spacetime. In this theory, mass causes a distortion or curvature in spacetime, which is felt as a gravitational force. This is in contrast to the Newtonian view of gravity as a force acting at a distance between two masses. One of the most famous predictions of General Relativity is the bending of light by gravity, which was first observed during a solar eclipse in 1919. The theory has been extremely successful in explaining various phenomena, such as the precession of Mercury's orbit, the gravitational redshift of light, and the existence of black holes and gravitational waves. In summary, the theory of relativity is a groundbreaking theory in physics that fundamentally changed our understanding of space, time, and matter. It has been incredibly successful in making accurate predictions about the natural world and has stood the test of time through numerous experiments and observations. --------------------------------------------------------------------------------   ","version":"Next","tagName":"h2"},{"title":"Observability‚Äã","type":1,"pageTitle":"Deploying LLMs with RayServe and vLLM","url":"/ai-on-eks/docs/blueprints/inference/GPUs/vLLM-rayserve#observability","content":" As part of this blueprint, we have also deployed the Kube Prometheus stack, which provides Prometheus server and Grafana deployments for monitoring and observability.  First, let's verify the services deployed by the Kube Prometheus stack:  kubectl get svc -n kube-prometheus-stack   You should see output similar to this:  kubectl get svc -n kube-prometheus-stack NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kube-prometheus-stack-grafana ClusterIP 172.20.252.10 &lt;none&gt; 80/TCP 11d kube-prometheus-stack-kube-state-metrics ClusterIP 172.20.34.181 &lt;none&gt; 8080/TCP 11d kube-prometheus-stack-operator ClusterIP 172.20.186.93 &lt;none&gt; 443/TCP 11d kube-prometheus-stack-prometheus ClusterIP 172.20.147.64 &lt;none&gt; 9090/TCP,8080/TCP 11d kube-prometheus-stack-prometheus-node-exporter ClusterIP 172.20.171.165 &lt;none&gt; 9100/TCP 11d prometheus-operated ClusterIP None &lt;none&gt; 9090/TCP 11d   After verifying the Kube Prometheus stack services, we need to configure Prometheus to monitor our Ray cluster comprehensively. This requires deploying both ServiceMonitor and PodMonitor resources:  ServiceMonitor is used to collect metrics from the Ray head node, which has a Kubernetes Service exposing its metrics endpoint.PodMonitor is necessary because KubeRay operator does not create a Kubernetes service for the Ray worker Pods. Therefore, we cannot use a ServiceMonitor to scrape metrics from worker Pods and must use PodMonitors CRD instead.  cd ai-on-eks/infra/jark-stack/terraform/monitoring   kubectl apply -f serviceMonitor.yaml kubectl apply -f podMonitor.yaml   Grafana and Prometheus Integration  To integrate Grafana and Prometheus with the Ray Dashboard, we set specific environment variables in the Ray cluster configuration:  env: - name: RAY_GRAFANA_HOST value: http://kube-prometheus-stack-grafana.kube-prometheus-stack.svc:80 - name: RAY_PROMETHEUS_HOST value: http://kube-prometheus-stack-prometheus.kube-prometheus-stack.svc:9090   These environment variables are crucial for enabling the embedding of Grafana panels within the Ray Dashboard and for proper communication between Ray, Grafana, and Prometheus:  RAY_GRAFANA_HOST defines the internal Kubernetes service URL for Grafana. The Ray head pod uses this for backend health checks and communication within the cluster.RAY_PROMETHEUS_HOST specifies the internal Kubernetes service URL for Prometheus, allowing Ray to query metrics when needed.  Access Prometheus Web UI  # Forward the port of Prometheus Web UI in the Prometheus server Pod. kubectl port-forward prometheus-kube-prometheus-stack-prometheus-0 -n kube-prometheus-stack 9090:9090   Go to (YOUR_IP):9090/targets (e.g. 127.0.0.1:9090/targets). You should be able to see: podMonitor/kube-prometheus-stack/ray-workers-monitor/0 (1/1 up)serviceMonitor/kube-prometheus-stack/ray-head-monitor/0 (2/2 up)    Access Grafana  - Port-forward Grafana service: kubectl port-forward deployment/kube-prometheus-stack-grafana -n kube-prometheus-stack 3000:3000 # Check (YOUR_IP):3000/login for the Grafana login page (e.g. 127.0.0.1:3000/login). - Grafana Admin user admin - Get secret name from Terraform output terraform output grafana_secret_name - Get admin user password aws secretsmanager get-secret-value --secret-id &lt;grafana_secret_name_output&gt; --region $AWS_REGION --query &quot;SecretString&quot; --output text   Note: kubectl port-forward is not recommended for production use. Refer to this Grafana document for exposing Grafana behind a reverse proxy.  Import Open Source Grafana Dashboards  Create new dashboard by importing JSON file via Dashboards menu.Click 'Dashboards' icon in left panel, 'New', 'Import', then 'Upload JSON file'.Choose a JSON file. Case 1: If you are using Ray 2.24.0, you can use the sample config files in GitHub repository. The file names have a pattern of xxx_grafana_dashboard.json.Case 2: Otherwise, you should import the JSON files from /tmp/ray/session_latest/metrics/grafana/dashboards/ in the head Pod. You can use kubectl cp to copy the files from the head Pod to your local machine. Click ‚ÄúImport‚Äù.  TODO: Note that importing the dashboard manually is not ideal. We should find a way to import the dashboard automatically.   In the Grafana dashboard below, you can see several metrics:  Scheduler Task State displays the current number of tasks in a particular state.Active Tasks by Name shows the current number of (live) tasks with a particular name.Scheduler Actor State illustrates the current number of actors in a particular state.Active Actors by Name presents the current number of (live) actors with a particular name.    ","version":"Next","tagName":"h2"},{"title":"Conclusion‚Äã","type":1,"pageTitle":"Deploying LLMs with RayServe and vLLM","url":"/ai-on-eks/docs/blueprints/inference/GPUs/vLLM-rayserve#conclusion","content":" Integrating Ray Serve with a vLLM backend offers numerous benefits for large language model (LLM) inference, particularly in terms of efficiency, scalability, and cost-effectiveness. Ray Serve's ability to handle concurrent requests and dynamically batch them ensures optimal GPU utilization, which is crucial for high-throughput LLM applications. The integration with vLLM enhances this further by enabling continuous batching, which significantly improves throughput and reduces latency compared to static batching. Overall, the combination of Ray Serve and vLLM provides a robust, scalable, and cost-efficient solution for deploying LLMs in production.  ","version":"Next","tagName":"h2"},{"title":"Cleanup‚Äã","type":1,"pageTitle":"Deploying LLMs with RayServe and vLLM","url":"/ai-on-eks/docs/blueprints/inference/GPUs/vLLM-rayserve#cleanup","content":" Finally, we'll provide instructions for cleaning up and deprovisioning the resources when they are no longer needed.  Delete the RayCluster  cd ai-on-eks/blueprints/inference/vllm-rayserve-gpu kubectl delete -f ray-service-vllm.yaml   cd ai-on-eks/infra/jark-stack/terraform/monitoring kubectl delete -f serviceMonitor.yaml kubectl delete -f podMonitor.yaml   Destroy the EKS Cluster and resources  export AWS_DEAFULT_REGION=&quot;DEPLOYED_EKS_CLUSTER_REGION&gt;&quot; cd ai-on-eks/infra/jark-stack/terraform/ &amp;&amp; chmod +x cleanup.sh ./cleanup.sh  ","version":"Next","tagName":"h2"},{"title":"Serving Llama-3-8B Instruct Model with Inferentia, Ray Serve and Gradio","type":0,"sectionRef":"#","url":"/ai-on-eks/docs/blueprints/inference/Neuron/llama3-inf2","content":"","keywords":"","version":"Next"},{"title":"What is Llama-3-8B Instruct?‚Äã","type":1,"pageTitle":"Serving Llama-3-8B Instruct Model with Inferentia, Ray Serve and Gradio","url":"/ai-on-eks/docs/blueprints/inference/Neuron/llama3-inf2#what-is-llama-3-8b-instruct","content":" Meta developed and released the Meta Llama 3 family of large language models (LLMs), a collection of pretrained and instruction tuned generative text models in 8 and 70B sizes. The Llama 3 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks. Further, in developing these models, we took great care to optimize helpfulness and safety.  More information on Llama3 sizes and model architecture can be found here.  Scalability and Availability  One of the key challenges in deploying large language models (LLMs) like Llama-3 is the scalability and availability of suitable hardware. Traditional GPU instances often face scarcity due to high demand, making it challenging to provision and scale resources effectively.  In contrast, Trn1/Inf2 instances, such as trn1.32xlarge, trn1n.32xlarge, inf2.24xlarge and inf2.48xlarge, are purpose built for high-performance deep learning (DL) training and inference of generative AI models, including LLMs. They offer both scalability and availability, ensuring that you can deploy and scale your Llama-3 models as needed, without resource bottlenecks or delays.  Cost Optimization  Running LLMs on traditional GPU instances can be cost-prohibitive, especially given the scarcity of GPUs and their competitive pricing. Trn1/Inf2 instances provide a cost-effective alternative. By offering dedicated hardware optimized for AI and machine learning tasks, Trn1/Inf2 instances allow you to achieve top-notch performance at a fraction of the cost. This cost optimization enables you to allocate your budget efficiently, making LLM deployment accessible and sustainable.  Performance Boost  While Llama-3 can achieve high-performance inference on GPUs, Neuron accelerators take performance to the next level. Neuron accelerators are purpose-built for machine learning workloads, providing hardware acceleration that significantly enhances Llama-3's inference speeds. This translates to faster response times and improved user experiences when deploying Llama-3 on Trn1/Inf2 instances.  ","version":"Next","tagName":"h3"},{"title":"Example usecase‚Äã","type":1,"pageTitle":"Serving Llama-3-8B Instruct Model with Inferentia, Ray Serve and Gradio","url":"/ai-on-eks/docs/blueprints/inference/Neuron/llama3-inf2#example-usecase","content":" A company wants to deploy a Llama-3 chatbot to provide customer support. The company has a large customer base and expects to receive a high volume of chat requests at peak times. The company needs to design an infrastructure that can handle the high volume of requests and provide a fast response time.  The company can use Inferentia2 instances to scale its Llama-3 chatbot efficiently. Inferentia2 instances are specialized hardware accelerators for machine learning tasks. They can provide up to 20x better performance and up to 7x lower cost than GPUs for machine learning workloads.  The company can also use Ray Serve to horizontally scale its Llama-3 chatbot. Ray Serve is a distributed framework for serving machine learning models. It can automatically scale your models up or down based on demand.  To scale its Llama-3 chatbot, the company can deploy multiple Inferentia2 instances and use Ray Serve to distribute the traffic across the instances. This will allow the company to handle a high volume of requests and provide a fast response time.  ","version":"Next","tagName":"h3"},{"title":"Solution Architecture‚Äã","type":1,"pageTitle":"Serving Llama-3-8B Instruct Model with Inferentia, Ray Serve and Gradio","url":"/ai-on-eks/docs/blueprints/inference/Neuron/llama3-inf2#solution-architecture","content":" In this section, we will delve into the architecture of our solution, which combines Llama-3 model, Ray Serve and Inferentia2 on Amazon EKS.    ","version":"Next","tagName":"h2"},{"title":"Deploying the Solution‚Äã","type":1,"pageTitle":"Serving Llama-3-8B Instruct Model with Inferentia, Ray Serve and Gradio","url":"/ai-on-eks/docs/blueprints/inference/Neuron/llama3-inf2#deploying-the-solution","content":" To get started with deploying Llama-4-8b-instruct on Amazon EKS, we will cover the necessary prerequisites and guide you through the deployment process step by step.  This includes setting up the infrastructure, deploying the Ray cluster, and creating the Gradio WebUI app.  Prerequisites üëà  ","version":"Next","tagName":"h2"},{"title":"Deploying the Ray Cluster with Llama3 Model‚Äã","type":1,"pageTitle":"Serving Llama-3-8B Instruct Model with Inferentia, Ray Serve and Gradio","url":"/ai-on-eks/docs/blueprints/inference/Neuron/llama3-inf2#deploying-the-ray-cluster-with-llama3-model","content":" Once the Trainium on EKS Cluster is deployed, you can proceed to use kubectl to deploy the ray-service-Llama-3.yaml.  In this step, we will deploy the Ray Serve cluster, which comprises one Head Pod on x86 CPU instances using Karpenter autoscaling, as well as Ray workers on Inf2.48xlarge instances, autoscaled by Karpenter.  Let's take a closer look at the key files used in this deployment and understand their functionalities before proceeding with the deployment:  ray_serve_Llama-3.py:  This script uses FastAPI, Ray Serve, and PyTorch-based Hugging Face Transformers to create an efficient API for text generation using the meta-llama/Meta-Llama-3-8B-Instruct language model.  The script establishes an endpoint that accepts input sentences and efficiently generates text outputs, benefiting from Neuron acceleration for enhanced performance. With its high configurability, users can fine-tune model parameters to suit a wide range of natural language processing applications, including chatbots and text generation tasks.  ray-service-Llama-3.yaml:  This Ray Serve YAML file serves as a Kubernetes configuration for deploying the Ray Serve service, facilitating efficient text generation using the llama-3-8B-Instruct model.  It defines a Kubernetes namespace named llama3 to isolate resources. Within the configuration, the RayService specification, named llama-3, is created and hosted within the llama3 namespace. The RayService specification leverages the Python script ray_serve_llama3.py (copied into the Dockerfile located within the same folder) to create the Ray Serve service.  The Docker image used in this example is publicly available on Amazon Elastic Container Registry (ECR) for ease of deployment. Users can also modify the Dockerfile to suit their specific requirements and push it to their own ECR repository, referencing it in the YAML file.  ","version":"Next","tagName":"h2"},{"title":"Deploy the Llama-3-Instruct Model‚Äã","type":1,"pageTitle":"Serving Llama-3-8B Instruct Model with Inferentia, Ray Serve and Gradio","url":"/ai-on-eks/docs/blueprints/inference/Neuron/llama3-inf2#deploy-the-llama-3-instruct-model","content":" Ensure the cluster is configured locally  aws eks --region us-west-2 update-kubeconfig --name trainium-inferentia   Deploy RayServe Cluster  info To deploy the llama3-8B-Instruct model, it's essential to configure your Hugging Face Hub token as an environment variable. This token is required for authentication and accessing the model. For guidance on how to create and manage your Hugging Face tokens, please visit Hugging Face Token Management.  # set the Hugging Face Hub Token as an environment variable. This variable will be substituted when applying the ray-service-mistral.yaml file export HUGGING_FACE_HUB_TOKEN=&lt;Your-Hugging-Face-Hub-Token-Value&gt; cd ai-on-eks/blueprints/inference/llama3-8b-rayserve-inf2 envsubst &lt; ray-service-llama3.yaml| kubectl apply -f -   Verify the deployment by running the following commands  info The deployment process may take up to 10 minutes. The Head Pod is expected to be ready within 2 to 3 minutes, while the Ray Serve worker pod may take up to 10 minutes for image retrieval and Model deployment from Huggingface.  $ kubectl get all -n llama3 NAME READY STATUS RESTARTS AGE pod/llama3-raycluster-smqrl-head-4wlbb 0/1 Running 0 77s pod/service-raycluster-smqrl-worker-inf2-wjxqq 0/1 Running 0 77s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/llama3 ClusterIP 172.20.246.48 &lt;none&gt; 8000:32138/TCP,52365:32653/TCP,8080:32604/TCP,6379:32739/TCP,8265:32288/TCP,10001:32419/TCP 78s $ kubectl get ingress -n llama3 NAME CLASS HOSTS ADDRESS PORTS AGE llama3 nginx * k8s-ingressn-ingressn-randomid-randomid.elb.us-west-2.amazonaws.com 80 2m4s   Now, you can access the Ray Dashboard from the Load balancer URL below.  http://&lt;NLB_DNS_NAME&gt;/dashboard/#/serve  If you don't have access to a public Load Balancer, you can use port-forwarding and browse the Ray Dashboard using localhost with the following command:  kubectl port-forward svc/llama3 8265:8265 -n llama3 # Open the link in the browser http://localhost:8265/   From this webpage, you will be able to monitor the progress of Model deployment, as shown in the image below:    ","version":"Next","tagName":"h3"},{"title":"To Test the Llama3 Model‚Äã","type":1,"pageTitle":"Serving Llama-3-8B Instruct Model with Inferentia, Ray Serve and Gradio","url":"/ai-on-eks/docs/blueprints/inference/Neuron/llama3-inf2#to-test-the-llama3-model","content":" Once you see the status of the model deployment is in running state then you can start using Llama-3-instruct.  You can use the following URL with a query added at the end of the URL.  http://&lt;NLB_DNS_NAME&gt;/serve/infer?sentence=what is data parallelism and tensor parallelisma and the differences  You will see an output like this in your browser:    ","version":"Next","tagName":"h3"},{"title":"Deploying the Gradio WebUI App‚Äã","type":1,"pageTitle":"Serving Llama-3-8B Instruct Model with Inferentia, Ray Serve and Gradio","url":"/ai-on-eks/docs/blueprints/inference/Neuron/llama3-inf2#deploying-the-gradio-webui-app","content":" Discover how to create a user-friendly chat interface using Gradio that integrates seamlessly with deployed models.  Let's deploy Gradio app locally on your machine to interact with the LLama-3-Instruct model deployed using RayServe.  info The Gradio app interacts with the locally exposed service created solely for the demonstration. Alternatively, you can deploy the Gradio app on EKS as a Pod with Ingress and Load Balancer for wider accessibility.  ","version":"Next","tagName":"h2"},{"title":"Execute Port Forward to the llama3 Ray Service‚Äã","type":1,"pageTitle":"Serving Llama-3-8B Instruct Model with Inferentia, Ray Serve and Gradio","url":"/ai-on-eks/docs/blueprints/inference/Neuron/llama3-inf2#execute-port-forward-to-the-llama3-ray-service","content":" First, execute a port forward to the Llama-3 Ray Service using kubectl:  kubectl port-forward svc/llama2-service 8000:8000 -n llama3   ","version":"Next","tagName":"h3"},{"title":"Deploying the Gradio WebUI App‚Äã","type":1,"pageTitle":"Serving Llama-3-8B Instruct Model with Inferentia, Ray Serve and Gradio","url":"/ai-on-eks/docs/blueprints/inference/Neuron/llama3-inf2#deploying-the-gradio-webui-app-1","content":" Discover how to create a user-friendly chat interface using Gradio that integrates seamlessly with deployed models.  Let's move forward with setting up the Gradio app as a Docker container running on localhost. This setup will enable interaction with the Stable Diffusion XL model, which is deployed using RayServe.  ","version":"Next","tagName":"h2"},{"title":"Build the Gradio app docker container‚Äã","type":1,"pageTitle":"Serving Llama-3-8B Instruct Model with Inferentia, Ray Serve and Gradio","url":"/ai-on-eks/docs/blueprints/inference/Neuron/llama3-inf2#build-the-gradio-app-docker-container","content":" First, lets build the docker container for the client app.  cd ai-on-eks/blueprints/inference/gradio-ui docker build --platform=linux/amd64 \\ -t gradio-app:llama \\ --build-arg GRADIO_APP=&quot;gradio-app-llama.py&quot; \\ .   ","version":"Next","tagName":"h3"},{"title":"Deploy the Gradio container‚Äã","type":1,"pageTitle":"Serving Llama-3-8B Instruct Model with Inferentia, Ray Serve and Gradio","url":"/ai-on-eks/docs/blueprints/inference/Neuron/llama3-inf2#deploy-the-gradio-container","content":" Deploy the Gradio app as a container on localhost using docker:  docker run --rm -it -p 7860:7860 -p 8000:8000 gradio-app:llama   info If you are not running Docker Desktop on your machine and using something like finch instead then you will need to additional flags for a custom host-to-IP mapping inside the container. docker run --rm -it \\ --add-host ray-service:&lt;workstation-ip&gt; \\ -e &quot;SERVICE_NAME=http://ray-service:8000&quot; \\ -p 7860:7860 gradio-app:llama   Invoke the WebUI‚Äã  Open your web browser and access the Gradio WebUI by navigating to the following URL:  Running on local URL: http://localhost:7860  You should now be able to interact with the Gradio application from your local machine.    ","version":"Next","tagName":"h3"},{"title":"Conclusion‚Äã","type":1,"pageTitle":"Serving Llama-3-8B Instruct Model with Inferentia, Ray Serve and Gradio","url":"/ai-on-eks/docs/blueprints/inference/Neuron/llama3-inf2#conclusion","content":" In summary, when it comes to deploying and scaling Llama-3, AWS Trn1/Inf2 instances offer a compelling advantage. They provide the scalability, cost optimization, and performance boost needed to make running large language models efficient and accessible, all while overcoming the challenges associated with the scarcity of GPUs. Whether you're building chatbots, natural language processing applications, or any other LLM-driven solution, Trn1/Inf2 instances empower you to harness the full potential of Llama-3 on the AWS cloud.  ","version":"Next","tagName":"h2"},{"title":"Cleanup‚Äã","type":1,"pageTitle":"Serving Llama-3-8B Instruct Model with Inferentia, Ray Serve and Gradio","url":"/ai-on-eks/docs/blueprints/inference/Neuron/llama3-inf2#cleanup","content":" Finally, we'll provide instructions for cleaning up and deprovisioning the resources when they are no longer needed.  Step1: Delete Gradio Container  Ctrl-c on the localhost terminal window where docker run is running to kill the container running the Gradio app. Optionally clean up the docker image  docker rmi gradio-app:llama   Step2: Delete Ray Cluster  cd ai-on-eks/blueprints/inference/llama3-8b-instruct-rayserve-inf2 kubectl delete -f ray-service-llama3.yaml   Step3: Cleanup the EKS Cluster This script will cleanup the environment using -target option to ensure all the resources are deleted in correct order.  cd ai-on-eks/infra/trainium-inferentia/ ./cleanup.sh  ","version":"Next","tagName":"h2"},{"title":"Serving Mistral-7B-Instruct-v0.2 using Inferentia2, Ray Serve, Gradio","type":0,"sectionRef":"#","url":"/ai-on-eks/docs/blueprints/inference/Neuron/Mistral-7b-inf2","content":"","keywords":"","version":"Next"},{"title":"What is Mistral-7B-Instruct-v0.2 Model?‚Äã","type":1,"pageTitle":"Serving Mistral-7B-Instruct-v0.2 using Inferentia2, Ray Serve, Gradio","url":"/ai-on-eks/docs/blueprints/inference/Neuron/Mistral-7b-inf2#what-is-mistral-7b-instruct-v02-model","content":" The mistralai/Mistral-7B-Instruct-v0.2 is an instruction-tuned version of the Mistral-7B-v0.2 base model, which has been fine-tuned using publicly available conversation datasets. It is designed to follow instructions and complete tasks, making it suitable for applications such as chatbots, virtual assistants, and task-oriented dialogue systems. It is built on top of the Mistral-7B-v0.2 base model, which has 7.3 billion parameters and employs a state-of-the-art architecture including Grouped-Query Attention (GQA) for faster inference and a Byte-fallback BPE tokenizer for improved robustness.  Please refer to the Model Card for more detail.  ","version":"Next","tagName":"h3"},{"title":"Deploying the Solution‚Äã","type":1,"pageTitle":"Serving Mistral-7B-Instruct-v0.2 using Inferentia2, Ray Serve, Gradio","url":"/ai-on-eks/docs/blueprints/inference/Neuron/Mistral-7b-inf2#deploying-the-solution","content":" Let's get Mistral-7B-Instruct-v0.2 model up and running on Amazon EKS! In this section, we'll cover:  Prerequisites: Ensuring all necessary tools are installed before you begin.Infrastructure Setup: Creating your your EKS cluster and setting the stage for deployment.Deploying the Ray Cluster: The core of your image generation pipeline, providing scalability and efficiency.Building the Gradio Web UI: Creating a user-friendly interface for seamless interaction with the Mistral 7B model.  Prerequisites üëà  ","version":"Next","tagName":"h2"},{"title":"Deploying the Ray Cluster with Mistral 7B Model‚Äã","type":1,"pageTitle":"Serving Mistral-7B-Instruct-v0.2 using Inferentia2, Ray Serve, Gradio","url":"/ai-on-eks/docs/blueprints/inference/Neuron/Mistral-7b-inf2#deploying-the-ray-cluster-with-mistral-7b-model","content":" Once the trainium-inferentia EKS cluster is deployed, you can proceed to use kubectl to deploy the ray-service-mistral.yaml from /ai-on-eks/blueprints/inference/mistral-7b-rayserve-inf2/ path.  In this step, we will deploy the Ray Serve cluster, which comprises one Head Pod on x86 CPU instances using Karpenter autoscaling, as well as Ray workers on inf2.24xlarge instances, autoscaled by Karpenter.  Let's take a closer look at the key files used in this deployment and understand their functionalities before proceeding with the deployment:  ray_serve_mistral.py:This script sets up a FastAPI application with two main components deployed using Ray Serve, which enables scalable model serving on AWS Neuron infrastructure(Inf2): mistral-7b Deployment: This class initializes the Mistral 7B model using a scheduler and moves it to an Inf2 node for processing. The script leverages Transformers Neuron support for grouped-query attention (GQA) models for this Mistral model. The mistral-7b-instruct-v0.2 is a chat based model. The script also adds the required prefix for instructions by adding [INST] and [/INST] tokens surrounding the actual prompt.APIIngress: This FastAPI endpoint acts as an interface to the Mistral 7B model. It exposes a GET method on the /infer path that takes a text prompt. It responds to the prompt by replying with a text. ray-service-mistral.yaml:This RayServe deployment pattern sets up a scalable service for hosting the Mistral-7B-Instruct-v0.2 Model model on Amazon EKS with AWS Inferentia2 support. It creates a dedicated namespace and configures a RayService with autoscaling capabilities to efficiently manage resource utilization based on incoming traffic. The deployment ensures that the model, served under the RayService umbrella, can automatically adjust replicas, depending on demand, with each replica requiring 2 neuron cores. This pattern makes use of custom container images designed to maximize performance and minimizes startup delays by ensuring that heavy dependencies are preloaded.  ","version":"Next","tagName":"h2"},{"title":"Deploy the Mistral-7B-Instruct-v0.2 Model‚Äã","type":1,"pageTitle":"Serving Mistral-7B-Instruct-v0.2 using Inferentia2, Ray Serve, Gradio","url":"/ai-on-eks/docs/blueprints/inference/Neuron/Mistral-7b-inf2#deploy-the-mistral-7b-instruct-v02-model","content":" Ensure the cluster is configured locally  aws eks --region us-west-2 update-kubeconfig --name trainium-inferentia   Deploy RayServe Cluster  info To deploy the Mistral-7B-Instruct-v0.2 model, it's essential to configure your Hugging Face Hub token as an environment variable. This token is required for authentication and accessing the model. For guidance on how to create and manage your Hugging Face tokens, please visit Hugging Face Token Management.  # set the Hugging Face Hub Token as an environment variable. This variable will be substituted when applying the ray-service-mistral.yaml file export HUGGING_FACE_HUB_TOKEN=$(echo -n &quot;Your-Hugging-Face-Hub-Token-Value&quot; | base64) cd ai-on-eks/blueprints/inference/mistral-7b-rayserve-inf2 envsubst &lt; ray-service-mistral.yaml| kubectl apply -f -   Verify the deployment by running the following commands  info The deployment process may take up to 10 minutes. The Head Pod is expected to be ready within 2 to 3 minutes, while the Ray Serve worker pod may take up to 10 minutes for image retrieval and Model deployment from Huggingface.  This deployment establishes a Ray head pod running on an x86 instance and a worker pod on inf2.24xl instance as shown below.  kubectl get pods -n mistral NAME READY STATUS service-raycluster-68tvp-worker-inf2-worker-group-2kckv 1/1 Running mistral-service-raycluster-68tvp-head-dmfz5 2/2 Running   This deployment also sets up a mistral service with multiple ports configured; port 8265 is designated for the Ray dashboard and port 8000 for the Mistral model endpoint.  kubectl get svc -n mistral NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) mistral-service NodePort 172.20.118.238 &lt;none&gt; 10001:30998/TCP,8000:32437/TCP,52365:31487/TCP,8080:30351/TCP,6379:30392/TCP,8265:30904/TCP mistral-service-head-svc NodePort 172.20.245.131 &lt;none&gt; 6379:31478/TCP,8265:31393/TCP,10001:32627/TCP,8000:31251/TCP,52365:31492/TCP,8080:31471/TCP mistral-service-serve-svc NodePort 172.20.109.223 &lt;none&gt; 8000:31679/TCP   For the Ray dashboard, you can port-forward these ports individually to access the web UI locally using localhost.  kubectl -n mistral port-forward svc/mistral-service 8265:8265   Access the web UI via http://localhost:8265 . This interface displays the deployment of jobs and actors within the Ray ecosystem.    Once the deployment is complete, the Controller and Proxy status should be HEALTHY and Application status should be RUNNING    You can monitor Serve deployment and the Ray Cluster deployment including resource utilization using the Ray Dashboard.    ","version":"Next","tagName":"h3"},{"title":"Deploying the Gradio WebUI App‚Äã","type":1,"pageTitle":"Serving Mistral-7B-Instruct-v0.2 using Inferentia2, Ray Serve, Gradio","url":"/ai-on-eks/docs/blueprints/inference/Neuron/Mistral-7b-inf2#deploying-the-gradio-webui-app","content":" Gradio Web UI is used to interact with the Mistral7b inference service deployed on EKS Clusters using inf2 instances. The Gradio UI communicates internally with the mistral service(mistral-serve-svc.mistral.svc.cluster.local:8000), which is exposed on port 8000, using its service name and port.  We have created a base Docker(ai/inference/gradio-ui/Dockerfile-gradio-base) image for the Gradio app, which can be used with any model inference. This image is published on Public ECR.  Steps to Deploy a Gradio App:‚Äã  The following YAML script (ai/inference/mistral-7b-rayserve-inf2/gradio-ui.yaml) creates a dedicated namespace, deployment, service, and a ConfigMap where your model client script goes.  To deploy this, execute:  cd ai-on-eks/blueprints/inference/mistral-7b-rayserve-inf2/ kubectl apply -f gradio-ui.yaml   Verification Steps:Run the following commands to verify the deployment, service, and ConfigMap:  kubectl get deployments -n gradio-mistral7b-inf2 kubectl get services -n gradio-mistral7b-inf2 kubectl get configmaps -n gradio-mistral7b-inf2   Port-Forward the Service:  Run the port-forward command so that you can access the Web UI locally:  kubectl port-forward service/gradio-service 7860:7860 -n gradio-mistral7b-inf2   Invoke the WebUI‚Äã  Open your web browser and access the Gradio WebUI by navigating to the following URL:  Running on local URL: http://localhost:7860  You should now be able to interact with the Gradio application from your local machine.    Interaction With Mistral Model‚Äã  Mistral-7B-Instruct-v0.2 Model can be used for purposes such as chat applications (Q&amp;A, conversation), text generation, knowledge retrieval and others.  Below screenshots provide some examples of the model response based on different text prompts.        ","version":"Next","tagName":"h2"},{"title":"Cleanup‚Äã","type":1,"pageTitle":"Serving Mistral-7B-Instruct-v0.2 using Inferentia2, Ray Serve, Gradio","url":"/ai-on-eks/docs/blueprints/inference/Neuron/Mistral-7b-inf2#cleanup","content":" Finally, we'll provide instructions for cleaning up and deprovisioning the resources when they are no longer needed.  Step1: Delete Gradio App and mistral Inference deployment  cd ai-on-eks/blueprints/inference/mistral-7b-rayserve-inf2 kubectl delete -f gradio-ui.yaml kubectl delete -f ray-service-mistral.yaml   Step2: Cleanup the EKS Cluster This script will cleanup the environment using -target option to ensure all the resources are deleted in correct order.  cd ai-on-eks/infra/trainium-inferentia/ ./cleanup.sh  ","version":"Next","tagName":"h2"},{"title":"Ray Serve High Availability","type":0,"sectionRef":"#","url":"/ai-on-eks/docs/blueprints/inference/Neuron/rayserve-ha","content":"","keywords":"","version":"Next"},{"title":"Ray Head Node High Availability With Elastic Cache for Redis‚Äã","type":1,"pageTitle":"Ray Serve High Availability","url":"/ai-on-eks/docs/blueprints/inference/Neuron/rayserve-ha#ray-head-node-high-availability-with-elastic-cache-for-redis","content":" A critical component of a Ray cluster is the head node, which orchestrates the entire cluster by managing task scheduling, state synchronization, and node coordination. However, by default, the Ray head Pod represents a single point of failure; if it fails, the entire cluster including the Ray worker Pods need to be restarted.  To address this, High Availability (HA) for the Ray head node is essential. Global Control Service (GCS) manages cluster-level metadata in a RayCluster. By default, the GCS lacks fault tolerance as it stores all data in-memory, and a failure can cause the entire Ray cluster to fail. To avoid this, one must add fault tolerance to Ray‚Äôs Global Control Store (GCS), which allows the Ray Serve application to serve traffic even when the head node crashes. In the event of a GCS restart, it retrieves all the data from the Redis instance and resumes its regular functions.      Following sections provide the steps on how to enable GCS fault tolerance and ensure high availability for the Ray head Pod. We're using the Mistral-7B-Instruct-v0.2 model to demonstrate the Ray head high availability.  ","version":"Next","tagName":"h2"},{"title":"Add an External Redis Server‚Äã","type":1,"pageTitle":"Ray Serve High Availability","url":"/ai-on-eks/docs/blueprints/inference/Neuron/rayserve-ha#add-an-external-redis-server","content":" GCS fault tolerance requires an external Redis database. You can choose to host your own Redis database, or you can use one through a third-party vendor.  For development and testing purposes, you can also host a containerized Redis database on the same EKS cluster as your Ray cluster. However, for production setups, it's recommended to use a highly available external Redis cluster. In this pattern, we've used Amazon ElasticCache for Redis to create an external Redis cluster. You can also choose to use Amazon memoryDB for setting up a Redis cluster.  As part of the current blueprint, we've added a terraform module named elasticache that creates an Elastic Cache Redis cluster in AWS. This uses The Redis cluster has cluster mode disabled and contain one node. This cluster node's endpoint can be used for both reads and writes.  Key things to note in this module are -  The Redis Cluster is in the same VPC as the EKS cluster. If the Redis cluster is created in a separate VPC, then VPC peering needs to be set up between the EKS cluster VPC and the Elastic Cache Redis cluster VPC to enable network connectivity.A cache subnet group needs to be created at the time of creating the Redis cluster. A subnet group is a collection of subnets that you may want to designate for your caches in a VPC. ElastiCache uses that cache subnet group to assign IP addresses within that subnet to each cache node in the cache. The blueprint automatically adds all the subnets used by the EKS cluster in the subnet group for the Elastic cache Redis cluster.Security Group - The Security Group assigned to the Redis cache needs to have an inbound rule that allows TCP traffic from EKS Cluster's worker node security group to the Redis cluster security group over port 6379. This is because the Ray head Pod needs to establish a connection to the Elastic cache Redis cluster over port 6379. The blueprint automatically sets up the security group with the inbound rule.  To create the Redis cluster using Amazon Elastic Cache, please follow the below steps.  info This Mistral7b deployment is using Ray Serve with High availability. If you have already deployed mistral7b in the previous steps, then you can delete the deployment and run the below steps.  Prerequisites:  aws clikubectlterraformenvsubstjq  First, enable the creation of the Redis cluster by setting the enable_rayserve_ha_elastic_cache_redis variable to true by running the below command. By default it's set to false.  export TF_VAR_enable_rayserve_ha_elastic_cache_redis=true   Then, run the install.sh script to install the EKS cluster with KubeRay operator and other add-ons.  cd ai-on-eks/infra/trainium-inferentia ./install.sh   In addition to the EKS cluster, this blueprint creates an AWS Elastic Cache Redis Cluster. Sample output looks like below  Apply complete! Resources: 8 added, 1 changed, 0 destroyed. Outputs: configure_kubectl = &quot;aws eks --region us-west-2 update-kubeconfig --name trainium-inferentia&quot; elastic_cache_redis_cluster_arn = &quot;arn:aws:elasticache:us-west-2:11111111111:cluster:trainium-inferentia&quot;   ","version":"Next","tagName":"h3"},{"title":"Add External Redis Information to RayService‚Äã","type":1,"pageTitle":"Ray Serve High Availability","url":"/ai-on-eks/docs/blueprints/inference/Neuron/rayserve-ha#add-external-redis-information-to-rayservice","content":" Once the elastic cache Redis cluster is created, we need to modify the RayService configuration for mistral-7b model inference.  First we need to obtain the Elastic Cache Redis Cluster endpoint by using AWS CLI and jq like below.  export EXT_REDIS_ENDPOINT=$(aws elasticache describe-cache-clusters \\ --cache-cluster-id &quot;trainium-inferentia&quot; \\ --show-cache-node-info | jq -r '.CacheClusters[0].CacheNodes[0].Endpoint.Address')   Now, add the annotation ray.io/ft-enabled: &quot;true&quot; under RayService CRD. The annotation ray.io/ft-enabled enables GCS fault tolerance when set to true.  apiVersion: ray.io/v1 kind: RayService metadata: name: mistral namespace: mistral annotations: ray.io/ft-enabled: &quot;true&quot;   Add the external Redis cluster information in the headGroupSpec as RAY_REDIS_ADDRESS environment variable.  headGroupSpec: headService: metadata: name: mistral namespace: mistral rayStartParams: dashboard-host: '0.0.0.0' num-cpus: &quot;0&quot; template: spec: containers: - name: head .... env: - name: RAY_REDIS_ADDRESS value: $EXT_REDIS_ENDPOINT:6379   RAY_REDIS_ADDRESS‚Äôs value should be your Redis database‚Äôs address. It should contain the Redis cluster endpoint and the port.  You can find the full RayService configuration with GCS fault tolerance enabled in ai/inference/mistral-7b-rayserve-inf2/ray-service-mistral-ft.yaml file.  With the above RayService configuration, we have enabled GCS fault tolerance for the Ray head Pod and the Ray cluster can recover from head Pod crashes without restarting all the Ray workers.  Let's apply the above RayService configuration and check the behavior.  cd ai-on-eks/blueprints/inference/ envsubst &lt; mistral-7b-rayserve-inf2/ray-service-mistral-ft.yaml| kubectl apply -f -   The output should look like below  namespace/mistral created secret/hf-token created rayservice.ray.io/mistral created ingress.networking.k8s.io/mistral created   Check the status of the Ray Pods in the cluster.  kubectl get po -n mistral   The Ray head and worker Pods should be in Running state as below.  NAME READY STATUS RESTARTS AGE mistral-raycluster-rf6l9-head-hc8ch 2/2 Running 0 31m mistral-raycluster-rf6l9-worker-inf2-tdrs6 1/1 Running 0 31m   ","version":"Next","tagName":"h3"},{"title":"Simulate Ray Head Pod Crash‚Äã","type":1,"pageTitle":"Ray Serve High Availability","url":"/ai-on-eks/docs/blueprints/inference/Neuron/rayserve-ha#simulate-ray-head-pod-crash","content":" Simulate Ray head Pod crashing by deleting the Pod  kubectl -n mistral delete po mistral-raycluster-rf6l9-head-xxxxx pod &quot;mistral-raycluster-rf6l9-head-xxxxx&quot; deleted   We can see that the Ray worker Pod is still running when the Ray head Pod is terminated and auto-restarted. Please see the below screenshots from Lens IDE.      Test the Mistral AI Gradio App‚Äã  Let's also test our Gradio UI App to see whether it's able to answer questions while the Ray head Pod is deleted.  Open the Gradio Mistral AI Chat application by pointing your browser to localhost:7860.  Now repeat the Ray head Pod crash simulation by deleting the Ray head Pod as shown in the above steps.  While the Ray head Pod is terminated and is recovering, submit questions into the Mistral AI Chat interface. We can see from below screenshots that the chat application is indeed able to serve traffic while the Ray head Pod is deleted and is recovering. This is because the RayServe service points to the Ray worker Pod which in this case is never restarted because of the GCS fault tolerance.        For a complete guide on enabling end-to-end fault tolerance to your RayServe application, please refer to Ray Guide.  ","version":"Next","tagName":"h3"},{"title":"Cleanup‚Äã","type":1,"pageTitle":"Ray Serve High Availability","url":"/ai-on-eks/docs/blueprints/inference/Neuron/rayserve-ha#cleanup","content":" Finally, we'll provide instructions for cleaning up and deprovisioning the resources when they are no longer needed.  Step1: Delete Gradio App and mistral Inference deployment  cd ai-on-eks/blueprints/inference/mistral-7b-rayserve-inf2 kubectl delete -f gradio-ui.yaml kubectl delete -f ray-service-mistral-ft.yaml   Step2: Cleanup the EKS Cluster This script will cleanup the environment using -target option to ensure all the resources are deleted in correct order.  cd ai-on-eks/infra/trainium-inferentia/ ./cleanup.sh  ","version":"Next","tagName":"h2"},{"title":"BERT-Large on Trainium","type":0,"sectionRef":"#","url":"/ai-on-eks/docs/blueprints/training/Neuron/BERT-Large","content":"BERT-Large on Trainium info COMING SOON Please note that this section is currently a work in progress and will serve as a comprehensive collection of resources for running data and ML workloads on EKS.","keywords":"","version":"Next"},{"title":"Serving Stable Diffusion XL Base Model with Inferentia, Ray Serve and Gradio","type":0,"sectionRef":"#","url":"/ai-on-eks/docs/blueprints/inference/Neuron/stablediffusion-inf2","content":"","keywords":"","version":"Next"},{"title":"What is Stable Diffusion?‚Äã","type":1,"pageTitle":"Serving Stable Diffusion XL Base Model with Inferentia, Ray Serve and Gradio","url":"/ai-on-eks/docs/blueprints/inference/Neuron/stablediffusion-inf2#what-is-stable-diffusion","content":" Stable Diffusion is a text-to-image model for creating stunning art within seconds. It is one of the largest and most powerful LLMs available today. It is primarily used to generate detailed images conditioned on text descriptions, though it can also be applied to other tasks such as inpainting, outpainting, and generating image-to-image translations guided by a text prompt.  Stable Diffusion XL(SDXL)‚Äã  SDXL is a latent diffusion model for text-to-image synthesis. Compared to previous versions of Stable Diffusion, SDXL uses pipelines for latent diffusion and noise reduction. SDXL also improves the quality of generated images compared to prior Stable Diffusion models by using a times larger UNet. The increase of model parameters is mainly due to more attention blocks and a larger cross-attention context as SDXL uses a second text encoder.  SDXL has been designed with multiple novel conditioning schemes and trained on multiple aspect ratios. It also uses a refinement model which is used to improve the visual fidelity of samples generated by SDXL using a post-hoc image-to-image technique.  This process results in a highly capable and fine-tuned language model that we will guide you to deploy and utilize effectively on Amazon EKS with Ray Serve.  ","version":"Next","tagName":"h3"},{"title":"Inference on Trn1/Inf2 Instances: Unlocking the Full Potential of Stable Diffusion LLMs‚Äã","type":1,"pageTitle":"Serving Stable Diffusion XL Base Model with Inferentia, Ray Serve and Gradio","url":"/ai-on-eks/docs/blueprints/inference/Neuron/stablediffusion-inf2#inference-on-trn1inf2-instances-unlocking-the-full-potential-of-stable-diffusion-llms","content":" Stable Diffusion XL can be deployed on a variety of hardware platforms, each with its own set of advantages. However, when it comes to maximizing the efficiency, scalability, and cost-effectiveness of Stable Diffusion models, AWS Trn1/Inf2 instances shine as the optimal choice.  Scalability and AvailabilityOne of the key challenges in deploying large language models (LLMs) like StableDiffusion XL is the scalability and availability of suitable hardware. Traditional GPU instances often face scarcity due to high demand, making it challenging to provision and scale resources effectively. In contrast, Trn1/Inf2 instances, such as trn1.32xlarge, trn1n.32xlarge, inf2.24xlarge and inf2.48xlarge, are purpose built for high-performance deep learning (DL) training and inference of generative AI models, including LLMs. They offer both scalability and availability, ensuring that you can deploy and scale your Stable-diffusion-xl models as needed, without resource bottlenecks or delays.  Cost Optimization:Running LLMs on traditional GPU instances can be cost-prohibitive, especially given the scarcity of GPUs and their competitive pricing.Trn1/Inf2 instances provide a cost-effective alternative. By offering dedicated hardware optimized for AI and machine learning tasks, Trn1/Inf2 instances allow you to achieve top-notch performance at a fraction of the cost. This cost optimization enables you to allocate your budget efficiently, making LLM deployment accessible and sustainable.  Performance BoostWhile Stable-Diffusion-xl can achieve high-performance inference on GPUs, Neuron accelerators take performance to the next level. Neuron accelerators are purpose-built for machine learning workloads, providing hardware acceleration that significantly enhances Stable-diffusion's inference speeds. This translates to faster response times and improved user experiences when deploying Stable-Diffusion-xl on Trn1/Inf2 instances.  ","version":"Next","tagName":"h2"},{"title":"Example usecase‚Äã","type":1,"pageTitle":"Serving Stable Diffusion XL Base Model with Inferentia, Ray Serve and Gradio","url":"/ai-on-eks/docs/blueprints/inference/Neuron/stablediffusion-inf2#example-usecase","content":" A digital art company wants to deploy Stable-diffusion-xl powered image generator to help generate possible art based on prompts. Using a selection of textual prompts, users can create artwork, graphics and logos in a wide variety of styles. The image generator can be used to predict or fine-tune the art and can result in significant time saving in product iteration cycle. Company has a large customer base and wants the model to be scalable at high load. The company needs to design an infrastructure that can handle the high volume of requests and provide a fast response time.  The company can use Inferentia2 instances to scale its Stable diffusion image generator efficiently. Inferentia2 instances are specialized hardware accelerators for machine learning tasks. They can provide up to 20x better performance and up to 7x lower cost than GPUs for machine learning workloads.  The company can also use Ray Serve to horizontally scale its Stable diffusion image generator. Ray Serve is a distributed framework for serving machine learning models. It can automatically scale your models up or down based on demand.  To scale its Stable diffusion image generator, the company can deploy multiple Inferentia2 instances and use Ray Serve to distribute the traffic across the instances. This will allow the company to handle a high volume of requests and provide a fast response time.  ","version":"Next","tagName":"h3"},{"title":"Solution Architecture‚Äã","type":1,"pageTitle":"Serving Stable Diffusion XL Base Model with Inferentia, Ray Serve and Gradio","url":"/ai-on-eks/docs/blueprints/inference/Neuron/stablediffusion-inf2#solution-architecture","content":" In this section, we will delve into the architecture of our solution, which combines Stable diffusion xl model, Ray Serve and Inferentia2 on Amazon EKS.    ","version":"Next","tagName":"h2"},{"title":"Deploying the Solution‚Äã","type":1,"pageTitle":"Serving Stable Diffusion XL Base Model with Inferentia, Ray Serve and Gradio","url":"/ai-on-eks/docs/blueprints/inference/Neuron/stablediffusion-inf2#deploying-the-solution","content":" To get started with deploying stable-diffusion-xl-base-1-0 on Amazon EKS, we will cover the necessary prerequisites and guide you through the deployment process step by step. This includes setting up the infrastructure, deploying the Ray cluster, and creating the Gradio WebUI app.  Prerequisites üëà  ","version":"Next","tagName":"h2"},{"title":"Deploying the Ray Cluster with Stable Diffusion XL Model‚Äã","type":1,"pageTitle":"Serving Stable Diffusion XL Base Model with Inferentia, Ray Serve and Gradio","url":"/ai-on-eks/docs/blueprints/inference/Neuron/stablediffusion-inf2#deploying-the-ray-cluster-with-stable-diffusion-xl-model","content":" Once the Trainium on EKS Cluster is deployed, you can proceed to use kubectl to deploy the ray-service-stablediffusion.yaml.  In this step, we will deploy the Ray Serve cluster, which comprises one Head Pod on x86 CPU instances using Karpenter autoscaling, as well as Ray workers on Inf2.48xlarge instances, autoscaled by Karpenter.  Let's take a closer look at the key files used in this deployment and understand their functionalities before proceeding with the deployment:  ray_serve_stablediffusion.py:This script uses FastAPI, Ray Serve, and Hugging Face Optimum Neuron library of tools to create an efficient text to image generator using the Neuronx model for stable-diffusion-xl-base-1.0 language model.  For this example blueprint, we are using a precompiled model that's been compiled to run on AWS Neuron. You can use any stable diffusion model of your choice and compile it to run on AWS Neuron before driving inference on it.  ray-service-stablediffusion.yaml:This Ray Serve YAML file serves as a Kubernetes configuration for deploying the Ray Serve service, facilitating efficient text generation using the stable-diffusion-xl-base-1.0 model. It defines a Kubernetes namespace named stablediffusion to isolate resources. Within the configuration, the RayService specification, named stablediffusion-service, is created and hosted within the stablediffusion namespace. The RayService specification leverages the Python script ray_serve_stablediffusion.py (copied into the Dockerfile located within the same folder) to create the Ray Serve service. The Docker image used in this example is publicly available on Amazon Elastic Container Registry (ECR) for ease of deployment. Users can also modify the Dockerfile to suit their specific requirements and push it to their own ECR repository, referencing it in the YAML file.  ","version":"Next","tagName":"h2"},{"title":"Deploy the Stable-Diffusion-xl-base-1-0 Model‚Äã","type":1,"pageTitle":"Serving Stable Diffusion XL Base Model with Inferentia, Ray Serve and Gradio","url":"/ai-on-eks/docs/blueprints/inference/Neuron/stablediffusion-inf2#deploy-the-stable-diffusion-xl-base-1-0-model","content":" Ensure the cluster is configured locally  aws eks --region us-west-2 update-kubeconfig --name trainium-inferentia   Deploy RayServe Cluster  cd ai-on-eks/blueprints/inference/stable-diffusion-xl-base-rayserve-inf2 kubectl apply -f ray-service-stablediffusion.yaml   Verify the deployment by running the following commands  info The deployment process may take up to 10 minutes. The Head Pod is expected to be ready within 2 to 3 minutes, while the Ray Serve worker pod may take up to 10 minutes for image retrieval and Model deployment from Huggingface.  $ kubectl get po -n stablediffusion -w NAME READY STATUS RESTARTS AGE service-raycluster-gc7gb-worker-inf2-worker-group-k2kf2 0/1 Init:0/1 0 7s stablediffusion-service-raycluster-gc7gb-head-6fqvv 1/1 Running 0 7s service-raycluster-gc7gb-worker-inf2-worker-group-k2kf2 0/1 PodInitializing 0 9s service-raycluster-gc7gb-worker-inf2-worker-group-k2kf2 1/1 Running 0 10s stablediffusion-service-raycluster-gc7gb-head-6fqvv 1/1 Running 0 53s service-raycluster-gc7gb-worker-inf2-worker-group-k2kf2 1/1 Running 0 53s   Also check the service and ingress resources that got created  kubectl get svc -n stablediffusion NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE stablediffusion-service NodePort 172.20.175.61 &lt;none&gt; 6379:32190/TCP,8265:32375/TCP,10001:32117/TCP,8000:30770/TCP,52365:30334/TCP,8080:30094/TCP 16h stablediffusion-service-head-svc NodePort 172.20.193.225 &lt;none&gt; 6379:32228/TCP,8265:30215/TCP,10001:30767/TCP,8000:31482/TCP,52365:30170/TCP,8080:31584/TCP 16h stablediffusion-service-serve-svc NodePort 172.20.15.224 &lt;none&gt; 8000:30982/TCP 16h $ kubectl get ingress -n stablediffusion NAME CLASS HOSTS ADDRESS PORTS AGE stablediffusion-ingress nginx * k8s-ingressn-ingressn-7f3f4b475b-1b8966c0b8f4d3da.elb.us-west-2.amazonaws.com 80 16h   Now, you can access the Ray Dashboard from the Load balancer URL below.  http://&lt;NLB_DNS_NAME&gt;/dashboard/#/serve  If you don't have access to a public Load Balancer, you can use port-forwarding and browse the Ray Dashboard using localhost with the following command:  kubectl port-forward svc/stablediffusion-service 8265:8265 -n stablediffusion # Open the link in the browser http://localhost:8265/   From this webpage, you will be able to monitor the progress of Model deployment, as shown in the image below:    ","version":"Next","tagName":"h3"},{"title":"To Test the Stable Diffusion XL Model‚Äã","type":1,"pageTitle":"Serving Stable Diffusion XL Base Model with Inferentia, Ray Serve and Gradio","url":"/ai-on-eks/docs/blueprints/inference/Neuron/stablediffusion-inf2#to-test-the-stable-diffusion-xl-model","content":" Once you've verified that the Stable Diffusion model deployment status has switched to a running state in Ray Dashboard , you're all set to start leveraging the model. This change in status signifies that the Stable Diffusion model is now fully functional and prepared to handle your image generation requests based on textual descriptions.&quot;  You can use the following URL with a query added at the end of the URL.  http://&lt;NLB_DNS_NAME&gt;/serve/imagine?prompt=an astronaut is dancing on green grass, sunlit  You will see an output like this in your browser:    ","version":"Next","tagName":"h3"},{"title":"Deploying the Gradio WebUI App‚Äã","type":1,"pageTitle":"Serving Stable Diffusion XL Base Model with Inferentia, Ray Serve and Gradio","url":"/ai-on-eks/docs/blueprints/inference/Neuron/stablediffusion-inf2#deploying-the-gradio-webui-app","content":" Discover how to create a user-friendly chat interface using Gradio that integrates seamlessly with deployed models.  Let's move forward with setting up the Gradio app as a Docker container running on localhost. This setup will enable interaction with the Stable Diffusion XL model, which is deployed using RayServe.  ","version":"Next","tagName":"h2"},{"title":"Build the Gradio app docker container‚Äã","type":1,"pageTitle":"Serving Stable Diffusion XL Base Model with Inferentia, Ray Serve and Gradio","url":"/ai-on-eks/docs/blueprints/inference/Neuron/stablediffusion-inf2#build-the-gradio-app-docker-container","content":" First, lets build the docker container for the client app.  cd ai-on-eks/blueprints/inference/gradio-ui docker build --platform=linux/amd64 \\ -t gradio-app:sd \\ --build-arg GRADIO_APP=&quot;gradio-app-stable-diffusion.py&quot; \\ .   ","version":"Next","tagName":"h3"},{"title":"Deploy the Gradio container‚Äã","type":1,"pageTitle":"Serving Stable Diffusion XL Base Model with Inferentia, Ray Serve and Gradio","url":"/ai-on-eks/docs/blueprints/inference/Neuron/stablediffusion-inf2#deploy-the-gradio-container","content":" Deploy the Gradio app as a container on localhost using docker:  docker run --rm -it -p 7860:7860 -p 8000:8000 gradio-app:sd   info If you are not running Docker Desktop on your machine and using something like finch instead then you will need to additional flags for a custom host-to-IP mapping inside the container. docker run --rm -it \\ --add-host ray-service:&lt;workstation-ip&gt; \\ -e &quot;SERVICE_NAME=http://ray-service:8000&quot; \\ -p 7860:7860 gradio-app:sd   Invoke the WebUI‚Äã  Open your web browser and access the Gradio WebUI by navigating to the following URL:  Running on local URL: http://localhost:7860  You should now be able to interact with the Gradio application from your local machine.    ","version":"Next","tagName":"h3"},{"title":"Conclusion‚Äã","type":1,"pageTitle":"Serving Stable Diffusion XL Base Model with Inferentia, Ray Serve and Gradio","url":"/ai-on-eks/docs/blueprints/inference/Neuron/stablediffusion-inf2#conclusion","content":" In conclusion, you will have successfully deployed the Stable-diffusion-xl-base model on EKS with Ray Serve and created a prompt based web UI using Gradio. This opens up exciting possibilities for natural language processing and prompt based image generator and image predictor development.  In summary, when it comes to deploying and scaling Stable diffusion models, AWS Trn1/Inf2 instances offer a compelling advantage. They provide the scalability, cost optimization, and performance boost needed to make running large language models efficient and accessible, all while overcoming the challenges associated with the scarcity of GPUs. Whether you're building text-to-image generators, image-to-image generators or any other LLM-driven solution, Trn1/Inf2 instances empower you to harness the full potential of Stable Diffusion LLMs on the AWS cloud.  ","version":"Next","tagName":"h2"},{"title":"Cleanup‚Äã","type":1,"pageTitle":"Serving Stable Diffusion XL Base Model with Inferentia, Ray Serve and Gradio","url":"/ai-on-eks/docs/blueprints/inference/Neuron/stablediffusion-inf2#cleanup","content":" Finally, we'll provide instructions for cleaning up and deprovisioning the resources when they are no longer needed.  Step1: Delete Gradio Container  Ctrl-c on the localhost terminal window where docker run is running to kill the container running the Gradio app. Optionally clean up the docker image  docker rmi gradio-app:sd   Step2: Delete Ray Cluster  cd ai-on-eks/blueprints/inference/stable-diffusion-xl-base-rayserve-inf2 kubectl delete -f ray-service-stablediffusion.yaml   Step3: Cleanup the EKS Cluster This script will cleanup the environment using -target option to ensure all the resources are deleted in correct order.  cd ai-on-eks/infra/trainium-inferentia/ ./cleanup.sh  ","version":"Next","tagName":"h2"},{"title":"BioNeMo on EKS","type":0,"sectionRef":"#","url":"/ai-on-eks/docs/blueprints/training/GPUs/bionemo","content":"","keywords":"","version":"Next"},{"title":"Introduction‚Äã","type":1,"pageTitle":"BioNeMo on EKS","url":"/ai-on-eks/docs/blueprints/training/GPUs/bionemo#introduction","content":" NVIDIA BioNeMo is a generative AI platform for drug discovery that simplifies and accelerates the training of models using your own data and scaling the deployment of models for drug discovery applications. BioNeMo offers the quickest path to both AI model development and deployment, accelerating the journey to AI-powered drug discovery. It has a growing community of users and contributors, and is actively maintained and developed by the NVIDIA.  Given its containerized nature, BioNeMo finds versatility in deployment across various environments such as Amazon Sagemaker, AWS ParallelCluster, Amazon ECS, and Amazon EKS. This solution, however, zeroes in on the specific deployment of BioNeMo on Amazon EKS.  Source: https://blogs.nvidia.com/blog/bionemo-on-aws-generative-ai-drug-discovery/  ","version":"Next","tagName":"h2"},{"title":"Deploying BioNeMo on Kubernetes‚Äã","type":1,"pageTitle":"BioNeMo on EKS","url":"/ai-on-eks/docs/blueprints/training/GPUs/bionemo#deploying-bionemo-on-kubernetes","content":" This blueprint leverages three major components for its functionality. The NVIDIA Device Plugin facilitates GPU usage, FSx stores training data, and the Kubeflow Training Operator manages the actual training process.  Kubeflow Training OperatorNVIDIA Device PluginFSx for Lustre CSI Driver  In this blueprint, we will deploy an Amazon EKS cluster and execute both a data preparation job and a distributed model training job.  Pre-requisites üëà  Deploy the blueprint üëà  Verify Deployment üëà  ","version":"Next","tagName":"h2"},{"title":"Run BioNeMo Training jobs‚Äã","type":1,"pageTitle":"BioNeMo on EKS","url":"/ai-on-eks/docs/blueprints/training/GPUs/bionemo#run-bionemo-training-jobs","content":" Once you've ensured that all components are functioning properly, you can proceed to submit jobs to your clusters.  Step1: Initiate the Uniref50 Data Preparation Task‚Äã  The first task, named the uniref50-job.yaml, involves downloading and partitioning the data to enhance processing efficiency. This task specifically retrieves the uniref50 dataset and organizes it within the FSx for Lustre Filesystem. This structured layout is designed for training, testing, and validation purposes. You can learn more about the uniref dataset here.  To execute this job, navigate to the examples\\esm1nv directory and deploy the uniref50-job.yaml manifest using the following commands:  cd examples/training kubectl apply -f uniref50-job.yaml   info It's important to note that this task requires a significant amount of time, typically ranging from 50 to 60 hours.  Run the below command to look for the pod uniref50-download-*  kubectl get pods   To verify its progress, examine the logs generated by the corresponding pod:  kubectl logs uniref50-download-xnz42 [NeMo I 2024-02-26 23:02:20 preprocess:289] Download and preprocess of UniRef50 data does not currently use GPU. Workstation or CPU-only instance recommended. [NeMo I 2024-02-26 23:02:20 preprocess:115] Data processing can take an hour or more depending on system resources. [NeMo I 2024-02-26 23:02:20 preprocess:117] Downloading file from https://ftp.uniprot.org/pub/databases/uniprot/uniref/uniref50/uniref50.fasta.gz... [NeMo I 2024-02-26 23:02:20 preprocess:75] Downloading file to /fsx/raw/uniref50.fasta.gz... [NeMo I 2024-02-26 23:08:33 preprocess:89] Extracting file to /fsx/raw/uniref50.fasta... [NeMo I 2024-02-26 23:12:46 preprocess:311] UniRef50 data processing complete. [NeMo I 2024-02-26 23:12:46 preprocess:313] Indexing UniRef50 dataset. [NeMo I 2024-02-26 23:16:21 preprocess:319] Writing processed dataset files to /fsx/processed... [NeMo I 2024-02-26 23:16:21 preprocess:255] Creating train split...   After finishing this task, the processed dataset will be saved in the /fsx/processed directory. Once this is done, we can move forward and start the pre-training job by running the following command:  Following this, we can proceed to execute the pre-training job by running:  In this PyTorchJob YAML, the command python3 -m torch.distributed.run plays a crucial role in orchestrating distributed training across multiple worker pods in your Kubernetes cluster.  It handles the following tasks:  Initializes a distributed backend (e.g., c10d, NCCL) for communication between worker processes.In our example it's using c10d. This is a commonly used distributed backend in PyTorch that can leverage different communication mechanisms like TCP or Infiniband depending on your environment.Sets up environment variables to enable distributed training within your training script.Launches your training script on all worker pods, ensuring each process participates in the distributed training.  cd examples/training kubectl apply -f esm1nv_pretrain-job.yaml   Run the below command to look for the pods esm1nv-pretraining-worker-*  kubectl get pods   NAME READY STATUS RESTARTS AGE esm1nv-pretraining-worker-0 1/1 Running 0 11m esm1nv-pretraining-worker-1 1/1 Running 0 11m esm1nv-pretraining-worker-2 1/1 Running 0 11m esm1nv-pretraining-worker-3 1/1 Running 0 11m esm1nv-pretraining-worker-4 1/1 Running 0 11m esm1nv-pretraining-worker-5 1/1 Running 0 11m esm1nv-pretraining-worker-6 1/1 Running 0 11m esm1nv-pretraining-worker-7 1/1 Running 0 11m   We should see 8 pods running. In the pod definition we have specified 8 worker replicas with 1 gpu limit for each. Karpenter provisioned 2 g5.12xlarge instance with 4 GPUs each. Since we set up &quot;nprocPerNode&quot; to &quot;4&quot;, each node will be responsible for 4 jobs. For more details around distributed pytorch training see pytorch docs.  info This training job can run for at least 3-4 days with g5.12xlarge nodes.  This configuration utilizes Kubeflow's PyTorch training Custom Resource Definition (CRD). Within this manifest, various parameters are available for customization. For detailed insights into each parameter and guidance on fine-tuning, you can refer to BioNeMo's documentation.  info Based on the Kubeflow training operator documentation, if you do not specify the master replica pod explicitly, the first worker replica pod(worker-0) will be treated as the master pod.  To track the progress of this process, follow these steps:  kubectl logs esm1nv-pretraining-worker-0 Epoch 0: 7%|‚ñã | 73017/1017679 [00:38&lt;08:12, 1918.0%   Additionally, get a snapshot of the GPU status for a specific worker node by running nvidia-smi command inside a Kubernetes pod running in that node. If you want to have a more robust observability, you can refer to the DCGM Exporter.  kubectl exec esm1nv-pretraining-worker-0 -- nvidia-smi Mon Feb 24 18:51:35 2025 +---------------------------------------------------------------------------------------+ | NVIDIA-SMI 535.230.02 Driver Version: 535.230.02 CUDA Version: 12.2 | |-----------------------------------------+----------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+======================+======================| | 0 NVIDIA A10G On | 00000000:00:1E.0 Off | 0 | | 0% 33C P0 112W / 300W | 3032MiB / 23028MiB | 95% Default | | | | N/A | +-----------------------------------------+----------------------+----------------------+ +---------------------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=======================================================================================| +---------------------------------------------------------------------------------------+   Benefits of Distributed Training:‚Äã  By distributing the training workload across multiple GPUs in your worker pods, you can train large models faster by leveraging the combined computational power of all GPUs. Handle larger datasets that might not fit on a single GPU's memory.  Conclusion‚Äã  BioNeMo stands as a formidable generative AI tool tailored for the realm of drug discovery. In this illustrative example, we took the initiative to pretrain a custom model entirely from scratch, utilizing the extensive uniref50 dataset. However, it's worth noting that BioNeMo offers the flexibility to expedite the process by employing pretrained models directly provided by NVidia. This alternative approach can significantly streamline your workflow while maintaining the robust capabilities of the BioNeMo framework.  Cleanup üëà ","version":"Next","tagName":"h3"},{"title":"Llama 3 fine-tuning on Trn1 with HuggingFace Optimum Neuron","type":0,"sectionRef":"#","url":"/ai-on-eks/docs/blueprints/training/Neuron/Llama-LoRA-Finetuning","content":"","keywords":"","version":"Next"},{"title":"What is Llama 3?‚Äã","type":1,"pageTitle":"Llama 3 fine-tuning on Trn1 with HuggingFace Optimum Neuron","url":"/ai-on-eks/docs/blueprints/training/Neuron/Llama-LoRA-Finetuning#what-is-llama-3","content":" Llama 3 is a large language model (LLM) for tasks like text generation, summarization, translation, and question answering. You can fine-tune it for your specific needs.  ","version":"Next","tagName":"h3"},{"title":"AWS Trainium‚Äã","type":1,"pageTitle":"Llama 3 fine-tuning on Trn1 with HuggingFace Optimum Neuron","url":"/ai-on-eks/docs/blueprints/training/Neuron/Llama-LoRA-Finetuning#aws-trainium","content":" AWS Trainium (Trn1) instances are designed for high-throughput, low-latency deep learning, ideal for training large models like Llama 3. The AWS Neuron SDK enhances Trainium's performance by optimizing models with advanced compiler techniques and mixed precision training for faster, accurate results.  ","version":"Next","tagName":"h3"},{"title":"1. Deploying the Solution‚Äã","type":1,"pageTitle":"Llama 3 fine-tuning on Trn1 with HuggingFace Optimum Neuron","url":"/ai-on-eks/docs/blueprints/training/Neuron/Llama-LoRA-Finetuning#1-deploying-the-solution","content":" Prerequisites üëà  ","version":"Next","tagName":"h2"},{"title":"2. Launch the Llama training job‚Äã","type":1,"pageTitle":"Llama 3 fine-tuning on Trn1 with HuggingFace Optimum Neuron","url":"/ai-on-eks/docs/blueprints/training/Neuron/Llama-LoRA-Finetuning#2-launch-the-llama-training-job","content":" Before we launch the training script, let's first deploy a utility pod. You will get on an interactive shell of this pod to monitor the progress of the fine-tuning job, access the fined tuned model weights, and view the output generated by the fine-tuned model for sample prompts.  kubectl apply -f training-artifact-access-pod.yaml   Create the ConfigMap for the training script:  kubectl apply -f llama3-finetuning-script-configmap.yaml   In order for the training script to be able to download the Llama 3 model from HuggingFace, it needs your HuggingFace Hub access token for authentication and accessing the model. For guidance on how to create and manage your HuggingFace tokens, please visit Hugging Face Token Management.  We'll set your HuggingFace Hub token as an environment variable. Replace your_huggingface_hub_access_token with your actual HuggingFace Hub access token.  export HUGGINGFACE_HUB_ACCESS_TOKEN=$(echo -n &quot;your_huggingface_hub_access_token&quot; | base64)   Deploy the Secret and fine-tuning Job resources by running the following command, which automatically substitutes HUGGINGFACE_HUB_ACCESS_TOKEN environment variable in the yaml before applying the resources to your Kubernetes cluster.  Note: The fine-tuning container image is fetched from the us-west-2 ECR repository. Review the HuggingFace website for additional guidance to check if it provides another region that you prefer based on the region you selected above for running this particular fine-tuning example. If you choose a different supported region, update the AWS account ID and region in the container image URL in the lora-finetune-resources.yaml file before running the below command.  envsubst &lt; lora-finetune-resources.yaml | kubectl apply -f -   ","version":"Next","tagName":"h2"},{"title":"3. Verify fine-tuned Llama3 model‚Äã","type":1,"pageTitle":"Llama 3 fine-tuning on Trn1 with HuggingFace Optimum Neuron","url":"/ai-on-eks/docs/blueprints/training/Neuron/Llama-LoRA-Finetuning#3-verify-fine-tuned-llama3-model","content":" Check the job status:  kubectl get jobs   Note: If the container isn't scheduled, check Karpenter logs for errors. This might happen if the chosen availability zones (AZs) or subnets lack an available trn1.32xlarge EC2 instance. To fix this, update the local.azs field in the main.tf file, located at ai-on-eks/infra/base/terraform. Ensure the trainium-trn1 EC2NodeClass in the addons.tf file, also at ai-on-eks/infra/base/terraform, references the correct subnets for these AZs. Then, rerun install.sh from ai-on-eks/infra/trainium-inferentia to apply the changes via Terraform.  To monitor the log for the fine-tuning job, access the tuned model, or check the generated text-to-SQL outputs from the test run with the fine-tuned model, open a shell in the utility pod and navigate to the /shared folder where these can be found. The fine-tuned model will be saved in a folder named llama3_tuned_model_&lt;timestamp&gt; and the generated SQL queries from sample prompts can be found in a log file named llama3_finetuning.out alongside the model folder.  kubectl exec -it training-artifact-access-pod -- /bin/bash cd /shared ls -l llama3_tuned_model* llama3_finetuning*   ","version":"Next","tagName":"h2"},{"title":"4. Cleaning up‚Äã","type":1,"pageTitle":"Llama 3 fine-tuning on Trn1 with HuggingFace Optimum Neuron","url":"/ai-on-eks/docs/blueprints/training/Neuron/Llama-LoRA-Finetuning#4-cleaning-up","content":" Note: Always run the cleanup steps to avoid extra AWS costs.  To remove the resources created by this solution, run these commands from the root of the ai-on-eks repository:  # Delete the Kubernetes resources: cd blueprints/training/llama-lora-finetuning-trn1 envsubst &lt; lora-finetune-resources.yaml | kubectl delete -f - kubectl delete -f llama3-finetuning-script-configmap.yaml kubectl delete -f training-artifact-access-pod.yaml   Clean up the EKS cluster and related resources:  cd ../../../infra/trainium-inferentia/terraform ./cleanup.sh  ","version":"Next","tagName":"h2"},{"title":"Serving LLMs with RayServe and vLLM on AWS Neuron","type":0,"sectionRef":"#","url":"/ai-on-eks/docs/blueprints/inference/Neuron/vllm-ray-inf2","content":"","keywords":"","version":"Next"},{"title":"What is AWS Neuron?‚Äã","type":1,"pageTitle":"Serving LLMs with RayServe and vLLM on AWS Neuron","url":"/ai-on-eks/docs/blueprints/inference/Neuron/vllm-ray-inf2#what-is-aws-neuron","content":" In this tutorial, you'll leverage AWS Neuron, a powerful SDK that optimizes deep learning performance on AWS Inferentia and Trainium accelerators. Neuron seamlessly integrates with frameworks like PyTorch and TensorFlow, providing a comprehensive toolkit for developing, profiling, and deploying high-performance machine learning models on specialized EC2 instances such as Inf1, Inf2, Trn1, and Trn1n.  ","version":"Next","tagName":"h3"},{"title":"What is vLLM?‚Äã","type":1,"pageTitle":"Serving LLMs with RayServe and vLLM on AWS Neuron","url":"/ai-on-eks/docs/blueprints/inference/Neuron/vllm-ray-inf2#what-is-vllm","content":" vLLM is a high-performance library for LLM inference and serving, designed to maximize throughput and minimize latency. At its core, vLLM utilizes PagedAttention, an innovative attention algorithm that dramatically improves memory efficiency, allowing for optimal utilization of GPU resources. This open-source solution offers seamless integration through its Python API and OpenAI-compatible server, enabling developers to deploy and scale large language models like Llama 3 with unprecedented efficiency in production environments.  ","version":"Next","tagName":"h3"},{"title":"What is RayServe?‚Äã","type":1,"pageTitle":"Serving LLMs with RayServe and vLLM on AWS Neuron","url":"/ai-on-eks/docs/blueprints/inference/Neuron/vllm-ray-inf2#what-is-rayserve","content":" Ray Serve is a scalable model serving library built on top of Ray, designed for deploying machine learning models and AI applications with features like framework-agnostic deployment, model composition, and built-in scaling. You will also encounter RayService, which is a Kubernetes custom resource that's part of the KubeRay project, used to deploy and manage Ray Serve applications on Kubernetes clusters.  ","version":"Next","tagName":"h3"},{"title":"What is Llama-3-8B Instruct?‚Äã","type":1,"pageTitle":"Serving LLMs with RayServe and vLLM on AWS Neuron","url":"/ai-on-eks/docs/blueprints/inference/Neuron/vllm-ray-inf2#what-is-llama-3-8b-instruct","content":" Meta developed and released the Meta Llama 3 family of large language models (LLMs), a collection of pretrained and instruction tuned generative text models in 8 and 70B sizes. The Llama 3 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks. Further, in developing these models, we took great care to optimize helpfulness and safety.  More information on Llama3 sizes and model architecture can be found here.  ","version":"Next","tagName":"h3"},{"title":"Why AWS Accelerators?‚Äã","type":1,"pageTitle":"Serving LLMs with RayServe and vLLM on AWS Neuron","url":"/ai-on-eks/docs/blueprints/inference/Neuron/vllm-ray-inf2#why-aws-accelerators","content":" Scalability and Availability  One of the key challenges in deploying large language models (LLMs) like Llama-3 is the scalability and availability of suitable hardware. Traditional GPU instances often face scarcity due to high demand, making it challenging to provision and scale resources effectively.  In contrast, Trn1/Inf2 instances, such as trn1.32xlarge, trn1n.32xlarge, inf2.24xlarge and inf2.48xlarge, are purpose built for high-performance deep learning (DL) training and inference of generative AI models, including LLMs. They offer both scalability and availability, ensuring that you can deploy and scale your Llama-3 models as needed, without resource bottlenecks or delays.  Cost Optimization  Running LLMs on traditional GPU instances can be cost-prohibitive, especially given the scarcity of GPUs and their competitive pricing. Trn1/Inf2 instances provide a cost-effective alternative. By offering dedicated hardware optimized for AI and machine learning tasks, Trn1/Inf2 instances allow you to achieve top-notch performance at a fraction of the cost. This cost optimization enables you to allocate your budget efficiently, making LLM deployment accessible and sustainable.  Performance Boost  While Llama-3 can achieve high-performance inference on GPUs, Neuron accelerators take performance to the next level. Neuron accelerators are purpose-built for machine learning workloads, providing hardware acceleration that significantly enhances Llama-3's inference speeds. This translates to faster response times and improved user experiences when deploying Llama-3 on Trn1/Inf2 instances.  ","version":"Next","tagName":"h3"},{"title":"Solution Architecture‚Äã","type":1,"pageTitle":"Serving LLMs with RayServe and vLLM on AWS Neuron","url":"/ai-on-eks/docs/blueprints/inference/Neuron/vllm-ray-inf2#solution-architecture","content":" In this section, we will delve into the architecture of our solution, which combines Llama-3 model, Ray Serve and Inferentia2 on Amazon EKS.    ","version":"Next","tagName":"h2"},{"title":"Deploying the Solution‚Äã","type":1,"pageTitle":"Serving LLMs with RayServe and vLLM on AWS Neuron","url":"/ai-on-eks/docs/blueprints/inference/Neuron/vllm-ray-inf2#deploying-the-solution","content":" To get started with deploying Llama-3-8B-instruct on Amazon EKS, we will cover the necessary prerequisites and guide you through the deployment process step by step.  This includes setting up the infrastructure using AWS Inferentia instances and deploying the Ray cluster.  Prerequisites üëà  ","version":"Next","tagName":"h2"},{"title":"Deploying the Ray Cluster with Llama3 Model‚Äã","type":1,"pageTitle":"Serving LLMs with RayServe and vLLM on AWS Neuron","url":"/ai-on-eks/docs/blueprints/inference/Neuron/vllm-ray-inf2#deploying-the-ray-cluster-with-llama3-model","content":" In this tutorial, we leverage the KubeRay operator, which extends Kubernetes with custom resource definitions for Ray-specific constructs like RayCluster, RayJob, and RayService. The operator watches for user events related to these resources, automatically creates necessary Kubernetes artifacts to form Ray clusters, and continuously monitors cluster state to ensure the desired configuration matches the actual state. It handles lifecycle management including setup, dynamic scaling of worker groups, and teardown, abstracting away the complexity of managing Ray applications on Kubernetes.  Each Ray cluster consists of a head node pod and a collection of worker node pods, with optional autoscaling support to size clusters according to workload requirements. KubeRay supports heterogeneous compute nodes (including GPUs) and running multiple Ray clusters with different Ray versions in the same Kubernetes cluster. Additionally, KubeRay can integrate with AWS Inferentia accelerators, enabling efficient deployment of large language models like Llama 3 on specialized hardware, potentially improving performance and cost-effectiveness for machine learning inference tasks.  Having deployed the EKS cluster with all the necessary components, we can now proceed with the steps to deploy NousResearch/Meta-Llama-3-8B-Instruct using RayServe and vLLM on AWS Accelerators.  Step 1: To deploy the RayService cluster, navigate to the directory containing the vllm-rayserve-deployment.yaml file and execute the kubectl apply command in your terminal. This will apply the RayService configuration and deploy the cluster on your EKS setup.  cd ai-on-eks/blueprints/inference/vllm-rayserve-inf2 kubectl apply -f vllm-rayserve-deployment.yaml   Optional Configuration  By default, an inf2.8xlarge instance will be provisioned. If you would like to use inf2.48xlarge, modify the file vllm-rayserve-deployment.yaml to change resources section under worker container.  limits: cpu: &quot;30&quot; memory: &quot;110G&quot; aws.amazon.com/neuron: &quot;1&quot; requests: cpu: &quot;30&quot; memory: &quot;110G&quot; aws.amazon.com/neuron: &quot;1&quot;   to the following:  limits: cpu: &quot;90&quot; memory: &quot;360G&quot; aws.amazon.com/neuron: &quot;12&quot; requests: cpu: &quot;90&quot; memory: &quot;360G&quot; aws.amazon.com/neuron: &quot;12&quot;   Optional: Deploy 70B ModelFor deploying a llama-70B model on inf2.48xlarge, refer to ai-on-eks/blueprints/inference/vllm-rayserve-inf2/vllm-rayserve-deployment-70B.yaml. This deployment will take about 60 minutes for downloading the large model and compile it for running on Neuron cores.  Step 2: Verify the deployment by running the following commands  To ensure that the deployment has been successfully completed, run the following commands:  info Deployment process may take up to 10 minutes. The Head Pod is expected to be ready within 5 to 6 minutes, while the Ray Serve worker pod may take up to 10 minutes for image retrieval and Model deployment from Huggingface.  According to the RayServe configuration, you will have one Ray head pod running on an x86 instance and one worker pod running on a inf2 instance. You can modify the RayServe YAML file to run multiple replicas; however, be aware that each additional replica can potentially create new instances.  kubectl get pods -n vllm   NAME READY STATUS RESTARTS AGE lm-llama3-inf2-raycluster-ksh7w-worker-inf2-group-dcs5n 1/1 Running 0 2d4h vllm-llama3-inf2-raycluster-ksh7w-head-4ck8f 2/2 Running 0 2d4h   This deployment also configures a service with multiple ports. Port 8265 is designated for the Ray dashboard, and port 8000 is for the vLLM inference server endpoint.  Run the following command to verify the services:  kubectl get svc -n vllm NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE vllm ClusterIP 172.20.23.54 &lt;none&gt; 8080/TCP,6379/TCP,8265/TCP,10001/TCP,8000/TCP 2d4h vllm-llama3-inf2-head-svc ClusterIP 172.20.18.130 &lt;none&gt; 6379/TCP,8265/TCP,10001/TCP,8000/TCP,8080/TCP 2d4h vllm-llama3-inf2-serve-svc ClusterIP 172.20.153.10 &lt;none&gt; 8000/TCP 2d4h   To access the Ray dashboard, you can port-forward the relevant port to your local machine:  kubectl -n vllm port-forward svc/vllm 8265:8265   You can then access the web UI at http://localhost:8265, which displays the deployment of jobs and actors within the Ray ecosystem.    Once the deployment is complete, the Controller and Proxy status should be HEALTHY and Application status should be RUNNING    ","version":"Next","tagName":"h2"},{"title":"To Test the Llama3 Model‚Äã","type":1,"pageTitle":"Serving LLMs with RayServe and vLLM on AWS Neuron","url":"/ai-on-eks/docs/blueprints/inference/Neuron/vllm-ray-inf2#to-test-the-llama3-model","content":" Now it's time to test the Meta-Llama-3-8B-Instruct chat model. We'll use a Python client script to send prompts to the RayServe inference endpoint and verify the outputs generated by the model.  First, execute a port forward to the vllm-llama3-inf2-serve-svc Service using kubectl:  kubectl -n vllm port-forward svc/vllm-llama3-inf2-serve-svc 8000:8000   openai-client.py uses the HTTP POST method to send a list of prompts to the inference endpoint for text completion and Q&amp;A, targeting the vllm server.  To run the Python client application in a virtual environment, follow these steps:  cd ai-on-eks/blueprints/inference/vllm-rayserve-inf2 python3 -m venv .venv source .venv/bin/activate pip3 install openai python3 openai-client.py   You will see an output something like below in the terminal:  Click to expand Python Client Terminal output Example 1 - Simple chat completion: Handling connection for 8000 The capital of India is New Delhi. Example 2 - Chat completion with different parameters: The twin suns of Tatooine set slowly in the horizon, casting a warm orange glow over the bustling spaceport of Anchorhead. Amidst the hustle and bustle, a young farm boy named Anakin Skywalker sat atop a dusty speeder, his eyes fixed on the horizon as he dreamed of adventure beyond the desert planet. As the suns dipped below the dunes, Anakin's uncle, Owen Lars, called out to him from the doorway of their humble moisture farm. &quot;Anakin, it's time to head back! Your aunt and I have prepared a special dinner in your honor.&quot; But Anakin was torn. He had received a strange message from an unknown sender, hinting at a great destiny waiting for him. Against his uncle's warnings, Anakin decided to investigate further, sneaking away into the night to follow the mysterious clues. As he rode his speeder through the desert, the darkness seemed to grow thicker, and the silence was broken only by the distant Example 3 - Streaming chat completion: I'd be happy to help you with that. Here we go: 1... (Pause) 2... (Pause) 3... (Pause) 4... (Pause) 5... (Pause) 6... (Pause) 7... (Pause) 8... (Pause) 9... (Pause) 10! Let me know if you have any other requests!   ","version":"Next","tagName":"h3"},{"title":"Observability‚Äã","type":1,"pageTitle":"Serving LLMs with RayServe and vLLM on AWS Neuron","url":"/ai-on-eks/docs/blueprints/inference/Neuron/vllm-ray-inf2#observability","content":" ","version":"Next","tagName":"h2"},{"title":"Observability with AWS CloudWatch and Neuron Monitor‚Äã","type":1,"pageTitle":"Serving LLMs with RayServe and vLLM on AWS Neuron","url":"/ai-on-eks/docs/blueprints/inference/Neuron/vllm-ray-inf2#observability-with-aws-cloudwatch-and-neuron-monitor","content":" This blueprint deploys the CloudWatch Observability Agent as a managed add-on, providing comprehensive monitoring for containerized workloads. It includes container insights for tracking key performance metrics such as CPU and memory utilization. Additionally, the addon utilizes the Neuron Monitor plugin to capture and report Neuron-specific metrics.  All metrics, including container insights and Neuron metrics such as Neuron Core utilization, NeuronCore memory usage are sent to Amazon CloudWatch, where you can monitor and analyze them in real-time. After the deployment is complete, you should be able to access these metrics directly from the CloudWatch console, allowing you to manage and optimize your workloads effectively.    ","version":"Next","tagName":"h3"},{"title":"Open WebUI Deployment‚Äã","type":1,"pageTitle":"Serving LLMs with RayServe and vLLM on AWS Neuron","url":"/ai-on-eks/docs/blueprints/inference/Neuron/vllm-ray-inf2#open-webui-deployment","content":" info Open WebUI is compatible only with models that work with the OpenAI API server and Ollama.  1. Deploy the WebUI  Deploy the Open WebUI by running the following command:  kubectl apply -f openai-webui-deployment.yaml   2. Port Forward to Access WebUI  Note If you're running a port forward already to test the inference with python client, then press ctrl+c to interrupt that.  Use kubectl port-forward to access the WebUI locally:  kubectl port-forward svc/open-webui 8081:80 -n openai-webui   3. Access the WebUI  Open your browser and go to http://localhost:8081  4. Sign Up  Sign up using your name, email, and a dummy password.  5. Start a New Chat  Click on New Chat and select the model from the dropdown menu, as shown in the screenshot below:    6. Enter Test Prompt  Enter your prompt, and you will see the streaming results, as shown below:    ","version":"Next","tagName":"h2"},{"title":"Performance Benchmarking with LLMPerf Tool‚Äã","type":1,"pageTitle":"Serving LLMs with RayServe and vLLM on AWS Neuron","url":"/ai-on-eks/docs/blueprints/inference/Neuron/vllm-ray-inf2#performance-benchmarking-with-llmperf-tool","content":" LLMPerf is an open-source tool designed for benchmarking the performance of large language models (LLMs).  LLMPerf tool connects to the vllm service via port 8000 using the port forwarding setup above done using the command kubectl -n vllm port-forward svc/vllm-llama3-inf2-serve-svc 8000:8000.  Execute the commands below in your terminal.  Clone the LLMPerf repository:  git clone https://github.com/ray-project/llmperf.git cd llmperf pip install -e . pip install pandas pip install ray   Create the vllm_benchmark.sh file using the command below:  cat &lt;&lt; 'EOF' &gt; vllm_benchmark.sh #!/bin/bash model=${1:-NousResearch/Meta-Llama-3-8B-Instruct} vu=${2:-1} export OPENAI_API_KEY=EMPTY export OPENAI_API_BASE=&quot;http://localhost:8000/v1&quot; export TOKENIZERS_PARALLELISM=true #if you have more vllm servers, append the below line to the above #;http://localhost:8001/v1;http://localhost:8002/v1&quot; max_requests=$(expr ${vu} \\* 8 ) date_str=$(date '+%Y-%m-%d-%H-%M-%S') python ./token_benchmark_ray.py \\ --model ${model} \\ --mean-input-tokens 512 \\ --stddev-input-tokens 20 \\ --mean-output-tokens 245 \\ --stddev-output-tokens 20 \\ --max-num-completed-requests ${max_requests} \\ --timeout 7200 \\ --num-concurrent-requests ${vu} \\ --results-dir &quot;vllm_bench_results/${date_str}&quot; \\ --llm-api openai \\ --additional-sampling-params '{}' EOF   --mean-input-tokens: specifies the average number of tokens in the input prompts  --stddev-input-tokens: specifies the variability in input token lengths for creating a more realistic testing environment  --mean-output-tokens: specifies the average number of tokens expected in the model's output to simulate realistic response lengths  --stddev-output-tokens: specifies the variability in output token lengths introducing diversity in response sizes  --max-num-completed-requests: sets the maximum number of requests to process  --num-concurrent-requests: specifies the number of simultaneous requests to simulate parallel workload  The command below executes the benchmarking script with the specified model, NousResearch/Meta-Llama-3-8B-Instruct, and sets the number of virtual users to 2. This results in the benchmark testing the model's performance with 2 concurrent requests, calculating a maximum of 16 requests to be processed.  Execute the command below:  ./vllm_benchmark.sh NousResearch/Meta-Llama-3-8B-Instruct 2   You should see similar output like the following:  ./vllm_benchmark.sh NousResearch/Meta-Llama-3-8B-Instruct 2 None of PyTorch, TensorFlow &gt;= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used. You are using the default legacy behaviour of the &lt;class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'&gt;. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message. 2024-09-03 09:54:45,976\tINFO worker.py:1783 -- Started a local Ray instance. 0%| | 0/16 [00:00&lt;?, ?it/s]Handling connection for 8000 Handling connection for 8000 12%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2/16 [00:17&lt;02:00, 8.58s/it]Handling connection for 8000 Handling connection for 8000 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 4/16 [00:33&lt;01:38, 8.20s/it]Handling connection for 8000 Handling connection for 8000 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 6/16 [00:47&lt;01:17, 7.75s/it]Handling connection for 8000 Handling connection for 8000 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 8/16 [01:00&lt;00:58, 7.36s/it]Handling connection for 8000 Handling connection for 8000 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 10/16 [01:15&lt;00:43, 7.31s/it]Handling connection for 8000 Handling connection for 8000 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 12/16 [01:29&lt;00:28, 7.20s/it]Handling connection for 8000 Handling connection for 8000 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 14/16 [01:45&lt;00:15, 7.52s/it]Handling connection for 8000 Handling connection for 8000 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [02:01&lt;00:00, 7.58s/it] \\Results for token benchmark for NousResearch/Meta-Llama-3-8B-Instruct queried with the openai api. inter_token_latency_s p25 = 0.051964785839225695 p50 = 0.053331799814278796 p75 = 0.05520852723583741 p90 = 0.05562424625711179 p95 = 0.05629651696856784 p99 = 0.057518213120178636 mean = 0.053548951905597324 min = 0.0499955879607504 max = 0.05782363715808134 stddev = 0.002070751885022901 ttft_s p25 = 1.5284210312238429 p50 = 1.7579061459982768 p75 = 1.8209733433031943 p90 = 1.842437624989543 p95 = 1.852818323241081 p99 = 1.8528624982456676 mean = 1.5821313202395686 min = 0.928935999982059 max = 1.8528735419968143 stddev = 0.37523908630204694 end_to_end_latency_s p25 = 13.74749460403109 p50 = 14.441407957987394 p75 = 15.53337344751344 p90 = 16.104882833489683 p95 = 16.366086292022374 p99 = 16.395070491998922 mean = 14.528114874927269 min = 10.75658329098951 max = 16.40231654199306 stddev = 1.4182672949824733 request_output_throughput_token_per_s p25 = 18.111220396798153 p50 = 18.703139371912407 p75 = 19.243016652511997 p90 = 19.37836414194298 p95 = 19.571455249271224 p99 = 19.915057038539217 mean = 18.682678715983627 min = 17.198769813363445 max = 20.000957485856215 stddev = 0.725563381521316 number_input_tokens p25 = 502.5 p50 = 509.5 p75 = 516.5 p90 = 546.5 p95 = 569.25 p99 = 574.65 mean = 515.25 min = 485 max = 576 stddev = 24.054105678657024 number_output_tokens p25 = 259.75 p50 = 279.5 p75 = 291.75 p90 = 297.0 p95 = 300.5 p99 = 301.7 mean = 271.625 min = 185 max = 302 stddev = 29.257192847799555 Number Of Errored Requests: 0 Overall Output Throughput: 35.827933968528434 Number Of Completed Requests: 16 Completed Requests Per Minute: 7.914131755588426   You can try generating benchmarking results with multiple concurrent requests to understand how performance varies with increasing number of concurrent requests:  ./vllm_benchmark.sh NousResearch/Meta-Llama-3-8B-Instruct 2 ./vllm_benchmark.sh NousResearch/Meta-Llama-3-8B-Instruct 4 ./vllm_benchmark.sh NousResearch/Meta-Llama-3-8B-Instruct 8 ./vllm_benchmark.sh NousResearch/Meta-Llama-3-8B-Instruct 16 . .   ","version":"Next","tagName":"h2"},{"title":"Performance Benchmarking Metrics‚Äã","type":1,"pageTitle":"Serving LLMs with RayServe and vLLM on AWS Neuron","url":"/ai-on-eks/docs/blueprints/inference/Neuron/vllm-ray-inf2#performance-benchmarking-metrics","content":" You can find the results of the benchmarking script under vllm_bench_results directory in the llmperf directory. The results are stored in folders following a date-time naming convention. New folders are created every time the benchmarking script is executed.  You will find that the results for every execution of the benchmarking script comprise of 2 files in the format below:  NousResearch-Meta-Llama-3-8B-Instruct_512_245_summary_32.json - contains the summary of performance metrics across all the request/response pairs.  NousResearch-Meta-Llama-3-8B-Instruct_512_245_individual_responses.json - contains performance metrics for each and every request / response pair.  Each of these files contain the following Performance Benchmarking Metrics:  results_inter_token_latency_s_*: Also referred to as Token generation latency (TPOT).Inter-Token latency refers to the average time elapsed between generating consecutive output tokens by a large language model (LLM) during the decoding or generation phase  results_ttft_s_*: Time taken to generate the first token (TTFT)  results_end_to_end_s_*: End-to-End latency - total time taken from when a user submits an input prompt to when the complete output response is generated by the LLM  results_request_output_throughput_token_per_s_*: Number of output tokens generated by a large language model (LLM) per second across all user requests or queries  results_number_input_tokens_*: Number of input tokens in the requests (Input length)  results_number_output_tokens_*: Number of output tokens in the requests (Output length)  ","version":"Next","tagName":"h3"},{"title":"Conclusion‚Äã","type":1,"pageTitle":"Serving LLMs with RayServe and vLLM on AWS Neuron","url":"/ai-on-eks/docs/blueprints/inference/Neuron/vllm-ray-inf2#conclusion","content":" In summary, when it comes to deploying and scaling Llama-3, AWS Trn1/Inf2 instances offer a compelling advantage. They provide the scalability, cost optimization, and performance boost needed to make running large language models efficient and accessible, all while overcoming the challenges associated with the scarcity of GPUs. Whether you're building chatbots, natural language processing applications, or any other LLM-driven solution, Trn1/Inf2 instances empower you to harness the full potential of Llama-3 on the AWS cloud.  ","version":"Next","tagName":"h2"},{"title":"Cleanup‚Äã","type":1,"pageTitle":"Serving LLMs with RayServe and vLLM on AWS Neuron","url":"/ai-on-eks/docs/blueprints/inference/Neuron/vllm-ray-inf2#cleanup","content":" Finally, we'll provide instructions for cleaning up and deprovisioning the resources when they are no longer needed.  Delete the RayCluster  cd ai-on-eks/blueprints/inference/vllm-rayserve-inf2 kubectl delete -f vllm-rayserve-deployment.yaml   Destroy the EKS Cluster and resources  cd ai-on-eks/infra/trainium-inferentia/terraform/_LOCAL ./cleanup.sh  ","version":"Next","tagName":"h2"},{"title":"Training a Llama-2 Model using Trainium, Neuronx-Nemo-Megatron and MPI operator","type":0,"sectionRef":"#","url":"/ai-on-eks/docs/blueprints/training/Neuron/Llama2","content":"","keywords":"","version":"Next"},{"title":"What is Llama-2?‚Äã","type":1,"pageTitle":"Training a Llama-2 Model using Trainium, Neuronx-Nemo-Megatron and MPI operator","url":"/ai-on-eks/docs/blueprints/training/Neuron/Llama2#what-is-llama-2","content":" Llama-2 is a large language model (LLM) trained on 2 trillion tokens of text and code. It is one of the largest and most powerful LLMs available today. Llama-2 can be used for a variety of tasks, including natural language processing, text generation, and translation.  Although Llama-2 is available as a pretrained model, in this tutorial we will show how to pretrain the model from scratch.  Llama-2-chat‚Äã  Llama-2 is a remarkable language model that has undergone a rigorous training process. It starts with pretraining using publicly available online data.  Llama-2 is available in three different model sizes:  Llama-2-70b: This is the largest Llama-2 model, with 70 billion parameters. It is the most powerful Llama-2 model and can be used for the most demanding tasks.Llama-2-13b: This is a medium-sized Llama-2 model, with 13 billion parameters. It is a good balance between performance and efficiency, and can be used for a variety of tasks.Llama-2-7b: This is the smallest Llama-2 model, with 7 billion parameters. It is the most efficient Llama-2 model and can be used for tasks that do not require the highest level of performance.  ","version":"Next","tagName":"h3"},{"title":"Which Llama-2 model size should I use?‚Äã","type":1,"pageTitle":"Training a Llama-2 Model using Trainium, Neuronx-Nemo-Megatron and MPI operator","url":"/ai-on-eks/docs/blueprints/training/Neuron/Llama2#which-llama-2-model-size-should-i-use","content":" The best Llama-2 model size for you will depend on your specific needs. and it may not always be the largest model for achieving the highest performance. It's advisable to evaluate your needs and consider factors such as computational resources, response time, and cost-efficiency when selecting the appropriate Llama-2 model size. The decision should be based on a comprehensive assessment of your application's goals and constraints.  Performance BoostWhile Llama-2 can achieve high-performance inference on GPUs, Neuron accelerators take performance to the next level. Neuron accelerators are purpose-built for machine learning workloads, providing hardware acceleration that significantly enhances Llama-2's inference speeds. This translates to faster response times and improved user experiences when deploying Llama-2 on Trn1/Inf2 instances.  ","version":"Next","tagName":"h3"},{"title":"Solution Architecture‚Äã","type":1,"pageTitle":"Training a Llama-2 Model using Trainium, Neuronx-Nemo-Megatron and MPI operator","url":"/ai-on-eks/docs/blueprints/training/Neuron/Llama2#solution-architecture","content":" In this section, we will delve into the architecture of our solution.  Trn1.32xl Instance: This is an EC2 accelerated instance type that is part of the EC2 Trn1 (Trainium) instance family, optimized for machine learning training workloads  MPI Worker Pods: These are Kubernetes pods configured for running MPI (Message Passing Interface) tasks. MPI is a standard for distributed memory parallel computing. Each worker pod runs on a trn1.32xlarge instance which is equipped with 16 Trainium accelerators and 8 Elastic Fabric Adapters (EFAs). EFAs are network devices that support high-performance computing applications running on Amazon EC2 instances.  MPI Launcher Pod: This pod is responsible for coordinating the MPI job across the worker pods. When a training job is first submitted to the cluster, an MPI launcher pod is created which waits for the workers to come online, connects to each worker, and invokes the training script.  MPI Operator: An operator in Kubernetes is a method of packaging, deploying, and managing a Kubernetes application. The MPI Operator automates the deployment and management of MPI workloads.  FSx for Lustre: A shared, high-performance filesystem which is well suited for workloads such as machine learning, high performance computing (HPC), video processing, and financial modeling. The FSx for Lustre filesystem will be shared across worker pods in the training job, providing a central repository to access the training data and to store model artifacts and logs.    ","version":"Next","tagName":"h2"},{"title":"Deploying the Solution‚Äã","type":1,"pageTitle":"Training a Llama-2 Model using Trainium, Neuronx-Nemo-Megatron and MPI operator","url":"/ai-on-eks/docs/blueprints/training/Neuron/Llama2#deploying-the-solution","content":" Steps to train Llama-2 using AWS Trainium on Amazon EKS  Note: This post makes use of Meta‚Äôs Llama tokenizer, which is protected by a user license that must be accepted before the tokenizer files can be downloaded. Please ensure that you have access to the Llama files by requesting access here.  Prerequisites üëà  ","version":"Next","tagName":"h2"},{"title":"Distributed training‚Äã","type":1,"pageTitle":"Training a Llama-2 Model using Trainium, Neuronx-Nemo-Megatron and MPI operator","url":"/ai-on-eks/docs/blueprints/training/Neuron/Llama2#distributed-training","content":" Once the EKS Cluster is deployed, you can proceed with the next steps of building neuronx-nemo-megatron container image and pushing the image to ECR.  ","version":"Next","tagName":"h2"},{"title":"Build the neuronx-nemo-megatron container image‚Äã","type":1,"pageTitle":"Training a Llama-2 Model using Trainium, Neuronx-Nemo-Megatron and MPI operator","url":"/ai-on-eks/docs/blueprints/training/Neuron/Llama2#build-the-neuronx-nemo-megatron-container-image","content":" Navigate to examples/llama2 directory  cd examples/llama2/   Run the 1-llama2-neuronx-pretrain-build-image.sh script to build the neuronx-nemo-megatron container image and push the image into ECR.  When prompted for a region, enter the region in which you launched your EKS cluster, above.  ./1-llama2-neuronx-pretrain-build-image.sh   Note: The image building and pushing to ECR will take ~10 minutes  ","version":"Next","tagName":"h3"},{"title":"Launch and connect to a CLI pod‚Äã","type":1,"pageTitle":"Training a Llama-2 Model using Trainium, Neuronx-Nemo-Megatron and MPI operator","url":"/ai-on-eks/docs/blueprints/training/Neuron/Llama2#launch-and-connect-to-a-cli-pod","content":" In this step we need access to the shared FSx storage. To copy files to this storage, we‚Äôll first launch and connect to a CLI pod running the neuronx-nemo-megatron docker image that you created above.  Run the following script to launch the CLI pod:  ./2-launch-cmd-shell-pod.sh   Next, periodically run the following command until you see the CLI pod go into ‚ÄòRunning‚Äô state:  kubectl get pod -w   Once the CLI pod is ‚ÄòRunning‚Äô, connect to it using the following command:  kubectl exec -it cli-cmd-shell -- /bin/bash   ","version":"Next","tagName":"h3"},{"title":"Download the Llama tokenizer and Redpajama dataset to FSx‚Äã","type":1,"pageTitle":"Training a Llama-2 Model using Trainium, Neuronx-Nemo-Megatron and MPI operator","url":"/ai-on-eks/docs/blueprints/training/Neuron/Llama2#download-the-llama-tokenizer-and-redpajama-dataset-to-fsx","content":" From within the CLI pod, we‚Äôll download the Llama tokenizer files. These files are protected by Meta's Llama license, so you will need to run the huggingface-cli login command to login to Hugging Face using your access token. The access token is found under Settings ‚Üí Access Tokens on the Hugging Face website.  huggingface-cli login   When prompted for your token, paste-in the access token and hit ENTER.  Next, you download the llama7-7b tokenizer files to /shared/llama7b_tokenizer by running the following python code:  python3 &lt;&lt;EOF import transformers tok = transformers.AutoTokenizer.from_pretrained(&quot;meta-llama/Llama-2-7b-hf&quot;) tok.save_pretrained(&quot;/shared/llama7b_tokenizer&quot;) EOF   Next, download the RedPajama-Data-1T-Sample dataset (a small subset of the full RedPajama dataset that contains 1B tokens).  While still connected to the CLI pod, use git to download the dataset  cd /shared git clone https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T-Sample \\ data/RedPajama-Data-1T-Sample   ","version":"Next","tagName":"h3"},{"title":"Tokenize the dataset‚Äã","type":1,"pageTitle":"Training a Llama-2 Model using Trainium, Neuronx-Nemo-Megatron and MPI operator","url":"/ai-on-eks/docs/blueprints/training/Neuron/Llama2#tokenize-the-dataset","content":" Tokenize the dataset using the preprocessing script included with neuronx-nemo-megatron. This preprocessing step will take ~60 minutes to run on a trn1.32xl instance.  cd /shared # Clone the neuronx-nemo-megatron repo, which includes the required scripts git clone https://github.com/aws-neuron/neuronx-nemo-megatron.git # Combine the separate redpajama files to a single jsonl file cat /shared/data/RedPajama-Data-1T-Sample/*.jsonl &gt; /shared/redpajama_sample.jsonl # Run preprocessing script using llama tokenizer python3 neuronx-nemo-megatron/nemo/scripts/nlp_language_modeling/preprocess_data_for_megatron.py \\ --input=/shared/redpajama_sample.jsonl \\ --json-keys=text \\ --tokenizer-library=huggingface \\ --tokenizer-type=/shared/llama7b_tokenizer \\ --dataset-impl=mmap \\ --output-prefix=/shared/data/redpajama_sample \\ --append-eod \\ --need-pad-id \\ --workers=32   ","version":"Next","tagName":"h3"},{"title":"Modify dataset and tokenizer paths in the training script‚Äã","type":1,"pageTitle":"Training a Llama-2 Model using Trainium, Neuronx-Nemo-Megatron and MPI operator","url":"/ai-on-eks/docs/blueprints/training/Neuron/Llama2#modify-dataset-and-tokenizer-paths-in-the-training-script","content":" Note: When we later launch our training jobs in EKS, the training pods will run the training script from within neuronx-nemo-megatron/nemo/examples directory on FSx. This is convenient, because it will let you modify your training script directly on FSx without requiring that you rebuild the neuronx-nemo-megatron container for every change.  Modify the test_llama.sh script /shared/neuronx-nemo-megatron/nemo/examples/nlp/language_modeling/test_llama.sh to update the following two lines. These lines tell the training pod workers where to find the Llama tokenizer and the dataset on the FSx filesystem.  Run:  sed -i 's#^\\(: ${TOKENIZER_PATH=\\).*#\\1/shared/llama7b_tokenizer}#' /shared/neuronx-nemo-megatron/nemo/examples/nlp/language_modeling/test_llama.sh sed -i 's#^\\(: ${DATASET_PATH=\\).*#\\1/shared/data/redpajama_sample_text_document}#' /shared/neuronx-nemo-megatron/nemo/examples/nlp/language_modeling/test_llama.sh   Before changes:  : ${TOKENIZER_PATH=$HOME/llamav2_weights/7b-hf} : ${DATASET_PATH=$HOME/examples_datasets/llama_7b/book.jsonl-processed_text_document}   After changes:  : ${TOKENIZER_PATH=/shared/llama7b_tokenizer} : ${DATASET_PATH=/shared/data/redpajama_sample_text_document}   You can save your changes in nano by pressing CTRL-X, then y, then ENTER.  When you are finished, type exit or press CTRL-d to exit the CLI pod.  If you no longer need the CLI pod you can remove it by running:  kubectl delete pod cli-cmd-shell   We are finally ready to launch our pre-compilation and training jobs!  First, let's check to make sure the MPI operator is functional by running this command:  kubectl get all -n mpi-operator   If the MPI Operator is not installed, please follow the MPI Operator installation instructions before proceeding.  Before we can run the training job, we first run a pre-compilation job in order to prepare the model artifacts. This step extracts and compiles the underlying compute graphs for the Llama-2-7b model and generates Neuron executable files (NEFFs) that can run on the Trainium accelerators. These NEFFs are stored in a persistent Neuron cache on FSx so that the training job can later access them.  ","version":"Next","tagName":"h3"},{"title":"Run pre-compilation job‚Äã","type":1,"pageTitle":"Training a Llama-2 Model using Trainium, Neuronx-Nemo-Megatron and MPI operator","url":"/ai-on-eks/docs/blueprints/training/Neuron/Llama2#run-pre-compilation-job","content":" Run the pre-compilation script  ./3-llama2-neuronx-mpi-compile.sh   Pre-compilation will take ~10 minutes when using 4 trn1.32xlarge nodes.  Periodically run kubectl get pods | grep compile and wait until you see that the compile job shows ‚ÄòCompleted‚Äô.  When pre-compilation is complete, you can then launch the pre-training job on 4 trn1.32xl nodes by running the following script:  ","version":"Next","tagName":"h3"},{"title":"Run training job‚Äã","type":1,"pageTitle":"Training a Llama-2 Model using Trainium, Neuronx-Nemo-Megatron and MPI operator","url":"/ai-on-eks/docs/blueprints/training/Neuron/Llama2#run-training-job","content":" ./4-llama2-neuronx-mpi-train.sh   ","version":"Next","tagName":"h3"},{"title":"View training job output‚Äã","type":1,"pageTitle":"Training a Llama-2 Model using Trainium, Neuronx-Nemo-Megatron and MPI operator","url":"/ai-on-eks/docs/blueprints/training/Neuron/Llama2#view-training-job-output","content":" To monitor the training job output - first, find the name of the launcher pod associated with your training job:  kubectl get pods | grep launcher   Once you have identified the name of the launcher pod and see that it is ‚ÄòRunning‚Äô, the next step is to determine its UID. Replace test-mpi-train-launcher-xxx with your launcher pod name in the following command and it will output the UID:  kubectl get pod test-mpi-train-launcher-xxx -o json | jq -r &quot;.metadata.uid&quot;   Use the UID to determine the log path so you can tail the training logs. Replace UID in the following command with the above value.  kubectl exec -it test-mpi-train-worker-0 -- tail -f /shared/nemo_experiments/UID/0/log   When you are done viewing the logs, you can press CTRL-C to quit the tail command.  ","version":"Next","tagName":"h3"},{"title":"Monitor Trainium accelerator utilization‚Äã","type":1,"pageTitle":"Training a Llama-2 Model using Trainium, Neuronx-Nemo-Megatron and MPI operator","url":"/ai-on-eks/docs/blueprints/training/Neuron/Llama2#monitor-trainium-accelerator-utilization","content":" To monitor Trainium accelerator utilization you can use the neuron-top command. Neuron-top is a console-based tool for monitoring Neuron and system-related performance metrics on trn1/inf2/inf1 instances. You can launch neuron-top on one of the worker pods as follows:  kubectl exec -it test-mpi-train-worker-0 -- /bin/bash -l neuron-top   ","version":"Next","tagName":"h3"},{"title":"View training job metrics in TensorBoard‚Äã","type":1,"pageTitle":"Training a Llama-2 Model using Trainium, Neuronx-Nemo-Megatron and MPI operator","url":"/ai-on-eks/docs/blueprints/training/Neuron/Llama2#view-training-job-metrics-in-tensorboard","content":" TensorBoard is a web-based visualization tool that is commonly used to monitor and explore training jobs. It allows you to quickly monitor training metrics, and you can also easily compare metrics across different training runs.  TensorBoard logs available in the /shared/nemo_experiments/ directory on the FSx for Lustre filesystem.  Run the following script to create a TensorBoard deployment so you can visualize your Llama-2 training job progress:  ./5-deploy-tensorboard.sh   Once the deployment is ready the script will output a password-protected URL for your new TensorBoard deployment.  Launch the URL to view your training progress.  When you have opened the TensorBoard interface, choose your training job UID from the left-hand menu, and then explore the various training metrics (ex: reduced-train-loss, throughput, and grad-norm) from the main application window.  ","version":"Next","tagName":"h3"},{"title":"Stopping the training job‚Äã","type":1,"pageTitle":"Training a Llama-2 Model using Trainium, Neuronx-Nemo-Megatron and MPI operator","url":"/ai-on-eks/docs/blueprints/training/Neuron/Llama2#stopping-the-training-job","content":" To stop your training job and remove the launcher/worker pods, run the following command:  kubectl delete mpijob test-mpi-train   You can then run kubectl get pods to confirm that the launcher/worker pods have been removed.  ","version":"Next","tagName":"h3"},{"title":"Cleaning up‚Äã","type":1,"pageTitle":"Training a Llama-2 Model using Trainium, Neuronx-Nemo-Megatron and MPI operator","url":"/ai-on-eks/docs/blueprints/training/Neuron/Llama2#cleaning-up","content":" To remove the resources created using this solution, run the cleanup script:  cd ai-on-eks/infra/trainium-inferentia ./cleanup.sh  ","version":"Next","tagName":"h3"},{"title":"Llama2 Distributed Pre-training on Trn1 with RayTrain and KubeRay","type":0,"sectionRef":"#","url":"/ai-on-eks/docs/blueprints/training/Neuron/RayTrain-Llama2","content":"","keywords":"","version":"Next"},{"title":"What is Llama-2?‚Äã","type":1,"pageTitle":"Llama2 Distributed Pre-training on Trn1 with RayTrain and KubeRay","url":"/ai-on-eks/docs/blueprints/training/Neuron/RayTrain-Llama2#what-is-llama-2","content":" Llama-2 is a state-of-the-art large language model (LLM) designed for various natural language processing (NLP) tasks, including text generation, summarization, translation, question answering, and more. It's a powerful tool that can be fine-tuned for specific use cases.  ","version":"Next","tagName":"h3"},{"title":"Why RayTrain and KubeRay for Distributed Training?‚Äã","type":1,"pageTitle":"Llama2 Distributed Pre-training on Trn1 with RayTrain and KubeRay","url":"/ai-on-eks/docs/blueprints/training/Neuron/RayTrain-Llama2#why-raytrain-and-kuberay-for-distributed-training","content":" Distributed training is essential for large models like Llama-2 due to their extensive computational and memory requirements. The combination of RayTrain and KubeRay, especially when leveraged with AWS Trainium, provides a robust framework for handling these demands efficiently and effectively:  RayTrain:‚Äã  Simplified Distributed Training: RayTrain is a high-level library built on the Ray framework that abstracts the complexities of distributed training. It allows you to scale your Llama-2 training across multiple nodes with minimal code changes. Ray's actor-based architecture and task-based parallelism enable efficient execution of distributed workloads.Flexible Strategies: RayTrain supports various distributed training strategies such as data parallelism and model parallelism. Data parallelism splits the dataset across multiple nodes, while model parallelism splits the model itself. This flexibility allows you to optimize training based on the specific needs of your model and the architecture of your training environment.Fault Tolerance: RayTrain includes built-in fault tolerance mechanisms. If a node fails, Ray can reschedule the tasks on other available nodes, ensuring that the training job continues without interruption. This feature is crucial for maintaining robustness in large-scale distributed training environments.Ease of Use: RayTrain offers intuitive APIs that simplify the setup and execution of distributed training jobs. Integrations with popular machine learning libraries like Hugging Face Transformers make it easier to incorporate RayTrain into your existing workflows without extensive modifications.  KubeRay:‚Äã  Integration with Kubernetes: KubeRay leverages Kubernetes' native capabilities to deploy, manage, and scale Ray clusters. This integration allows you to use Kubernetes' robust orchestration features to handle Ray workloads effectively.Dynamic Scaling: KubeRay supports dynamic scaling of Ray clusters. Ray's built-in autoscaler can request additional actor replicas based on workload demands, while Kubernetes tools like Karpenter or Cluster Autoscaler manage the creation of new nodes to meet these demands.Horizontal Scaling: Scale your Ray clusters horizontally by adding more worker nodes as the computational load increases. This allows efficient handling of large-scale distributed training and inference tasks.Custom Resource Definitions (CRDs): KubeRay utilizes Kubernetes CRDs to define and manage Ray clusters and jobs. This provides a standardized way to handle Ray workloads within the Kubernetes ecosystem.Fault Tolerance: KubeRay takes advantage of Kubernetes' self-healing capabilities. If a Ray head node or worker node fails, Kubernetes automatically restarts the failed components, ensuring minimal downtime and continuous operation.Distributed Scheduling: Ray's actor-based model and distributed task scheduling, combined with Kubernetes' orchestration, ensure high availability and efficient task execution even in the event of node failures.Declarative Configuration: KubeRay allows you to define Ray clusters and jobs using declarative YAML configurations. This simplifies the deployment and management process, making it easier to set up and maintain Ray clusters.Integrated Logging and Monitoring: KubeRay integrates with Kubernetes' logging and monitoring tools, such as Prometheus and Grafana. This provides comprehensive insights into the performance and health of Ray clusters, facilitating easier debugging and optimization.Spot Instances: Use Kubernetes' support for spot instances to run Ray clusters cost-effectively. This allows you to take advantage of lower-cost compute resources while maintaining the ability to scale as needed.  AWS Trainium:‚Äã  Optimized for Deep Learning: AWS Trainium-based Trn1 instances are specifically designed for deep learning workloads. They offer high throughput and low latency, making them ideal for training large-scale models like Llama-2. Trainium chips provide significant performance improvements over traditional processors, accelerating training times.Neuron SDK: The AWS Neuron SDK is tailored to optimize your deep learning models for Trainium. It includes features like advanced compiler optimizations and support for mixed precision training, which can further accelerate your training workloads while maintaining accuracy.  ","version":"Next","tagName":"h3"},{"title":"Why This Combination is Powerful‚Äã","type":1,"pageTitle":"Llama2 Distributed Pre-training on Trn1 with RayTrain and KubeRay","url":"/ai-on-eks/docs/blueprints/training/Neuron/RayTrain-Llama2#why-this-combination-is-powerful","content":" Simplified Scaling: RayTrain and KubeRay simplify the process of scaling Llama-2 training across multiple nodes. With Ray's efficient distributed execution and KubeRay's Kubernetes-native orchestration, you can easily scale your training workloads to leverage the full power of AWS Trainium on Trn1 instances.Optimized Performance: The Neuron SDK enhances the performance of your training jobs by optimizing them specifically for Trainium's architecture. Combined with Ray's ability to efficiently manage distributed tasks and KubeRay's resource orchestration, this setup ensures optimal training performance.Cost-Effective: Ray's autoscaling capabilities and Kubernetes' resource management help you optimize costs by efficiently allocating resources and scaling your cluster based on demand. This ensures you only use the resources you need, reducing unnecessary expenditure.  By using this combination of technologies, you can take advantage of the latest advancements in distributed training and hardware to pre-train Llama-2 efficiently and effectively.  ","version":"Next","tagName":"h3"},{"title":"What is Volcano?‚Äã","type":1,"pageTitle":"Llama2 Distributed Pre-training on Trn1 with RayTrain and KubeRay","url":"/ai-on-eks/docs/blueprints/training/Neuron/RayTrain-Llama2#what-is-volcano","content":" Volcano is an open-source batch scheduling system built on Kubernetes, specifically designed to manage high-performance computing (HPC) and machine learning workloads. It provides advanced scheduling capabilities such as gang scheduling, fair sharing, and preemption, which are essential for efficiently running large-scale, distributed training jobs in a Kubernetes environment.  ","version":"Next","tagName":"h3"},{"title":"How Volcano Works with Gang Scheduling‚Äã","type":1,"pageTitle":"Llama2 Distributed Pre-training on Trn1 with RayTrain and KubeRay","url":"/ai-on-eks/docs/blueprints/training/Neuron/RayTrain-Llama2#how-volcano-works-with-gang-scheduling","content":" Volcano's gang scheduling ensures that all pods in a job (or &quot;gang&quot;) are scheduled simultaneously. This is critical for distributed training workloads where multiple pods need to start together to function correctly. If even one pod in the gang cannot be scheduled due to resource constraints, none of the pods in the gang will start. This prevents partial execution and ensures that all resources required for the job are available before execution begins.  ","version":"Next","tagName":"h3"},{"title":"1. Deploying the Solution‚Äã","type":1,"pageTitle":"Llama2 Distributed Pre-training on Trn1 with RayTrain and KubeRay","url":"/ai-on-eks/docs/blueprints/training/Neuron/RayTrain-Llama2#1-deploying-the-solution","content":" Prerequisites üëà  ","version":"Next","tagName":"h2"},{"title":"2. Build the Docker Image (Optional Step)‚Äã","type":1,"pageTitle":"Llama2 Distributed Pre-training on Trn1 with RayTrain and KubeRay","url":"/ai-on-eks/docs/blueprints/training/Neuron/RayTrain-Llama2#2-build-the-docker-image-optional-step","content":" To simplify the blueprint deployment, we have already built the Docker image and made it available under the public ECR. If you want to customize the Docker image, you can update the Dockerfile and follow the optional step to build the Docker image. Please note that you will also need to modify the RayCluster YAML file, llama2-pretrain-trn1-raycluster.yaml, with the newly created image using your own private ECR.  cd ai/training/raytrain-llama2-pretrain-trn1 ./kuberay-trn1-llama2-pretrain-build-image.sh   After running this script, note the Docker image URL and tag that are produced. You will need this information for the next step.  ","version":"Next","tagName":"h2"},{"title":"3. Launch the Ray Cluster with KubeRay Operator‚Äã","type":1,"pageTitle":"Llama2 Distributed Pre-training on Trn1 with RayTrain and KubeRay","url":"/ai-on-eks/docs/blueprints/training/Neuron/RayTrain-Llama2#3-launch-the-ray-cluster-with-kuberay-operator","content":" If you skip step 2, you don't need to modify the YAML file. You can simply run the kubectl apply command on the file, and it will use the public ECR image that we published.  If you built a custom Docker image in Step 2, update the ai/training/raytrain-llama2-pretrain-trn1/llama2-pretrain-trn1-raycluster.yaml file with the Docker image URL and tag obtained from the previous step.  Once you have updated the YAML file (if needed), run the following command to launch the KubeRay cluster pods in your EKS cluster:  kubectl apply -f llama2-pretrain-trn1-raycluster.yaml   Verify the Pod Status:  kubectl get pods -l &quot;ray.io/cluster=kuberay-trn1&quot;   ","version":"Next","tagName":"h2"},{"title":"Gang Scheduling of Ray Head and Worker Pods with Volcano‚Äã","type":1,"pageTitle":"Llama2 Distributed Pre-training on Trn1 with RayTrain and KubeRay","url":"/ai-on-eks/docs/blueprints/training/Neuron/RayTrain-Llama2#gang-scheduling-of-ray-head-and-worker-pods-with-volcano","content":" In the context of deploying a Ray cluster for training Llama2, Volcano is crucial for ensuring that the Ray head and worker pods are scheduled together efficiently. The Ray head pod, typically running on an x86 instance, coordinates the distributed training, while the worker pods, running on AWS Trainium (Trn1) instances, perform the computationally intensive tasks. By leveraging Volcano's gang scheduling, we can ensure that the head and all worker pods are allocated the necessary resources simultaneously, enabling the distributed training job to start without delays.  Here's an example configuration for integrating Volcano with a RayCluster for Llama2 training:  info We are using the default namespace for this deployment because the fsx-claim PVC is created under the default namespace by the Terraform blueprint. If you want to deploy the cluster in a dedicated namespace, ensure that the FSX for Lustre file system is also created in the same namespace since PVCs are namespace-bound.  # Docs for Volcano with KubeRay: https://docs.ray.io/en/master/cluster/kubernetes/k8s-ecosystem/volcano.html --- apiVersion: scheduling.volcano.sh/v1beta1 kind: Queue metadata: name: llama2-training-queue namespace: default spec: weight: 1 capability: cpu: '500' memory: 1500Gi --- apiVersion: ray.io/v1 kind: RayCluster metadata: name: kuberay-trn1 namespace: default labels: ray.io/scheduler-name: volcano volcano.sh/queue-name: llama2-training-queue spec: rayVersion: 2.22.0 headGroupSpec: ... ...   You should see one ray-head pod and two ray-worker pods in the Running state:  warning Please be aware that it may take up to 10 minutes for the image to be pulled and the pod to become ready.  NAME READY STATUS RESTARTS AGE kuberay-trn1-head-67t46 0/1 Pending 0 2m50s kuberay-trn1-worker-workergroup-fz8bs 0/1 Pending 0 2m50s kuberay-trn1-worker-workergroup-gpnxh 0/1 Pending 0 2m50s   Check the logs of the head pod:  Look for messages indicating that the Ray head has started and the cluster is operational.  kubectl logs kuberay-trn1-head-xxxxx   ","version":"Next","tagName":"h3"},{"title":"Accessing the Ray Dashboard (Port Forwarding):‚Äã","type":1,"pageTitle":"Llama2 Distributed Pre-training on Trn1 with RayTrain and KubeRay","url":"/ai-on-eks/docs/blueprints/training/Neuron/RayTrain-Llama2#accessing-the-ray-dashboard-port-forwarding","content":" The Ray dashboard provides valuable insights into your cluster's status and job progress. To access it:  Forward the Port:  This forwards the Ray dashboard port (8265) from your local machine to the head pod within the cluster.  kubectl port-forward service/kuberay-trn1-head-svc 8265:8265   Open Browser and navigate to http://localhost:8265 in your web browser to view the dashboard.  ","version":"Next","tagName":"h3"},{"title":"4. Generate Pre-training Data on FSx Shared Filesystem‚Äã","type":1,"pageTitle":"Llama2 Distributed Pre-training on Trn1 with RayTrain and KubeRay","url":"/ai-on-eks/docs/blueprints/training/Neuron/RayTrain-Llama2#4-generate-pre-training-data-on-fsx-shared-filesystem","content":" warning Data generation step can take up to 20 minutes to create all the data in FSx for Lustre.  In this step, we'll leverage KubeRay's Job specification to kickstart the data generation process. We'll submit a job directly to the Ray head pod. This job plays a key role in preparing your model for training.  Check out the RayJob definition spec below to leverage the existing RayCluster using clusterSelector to submit the jobs to RayCluster.  # ---------------------------------------------------------------------------- # RayJob: llama2-generate-pretraining-test-data # # Description: # This RayJob is responsible for generating pre-training test data required for # the Llama2 model training. It sources data from the specified dataset, processes # it, and prepares it for use in subsequent training stages. The job runs a Python # script (`get_dataset.py`) that performs these data preparation steps. # Usage: # Apply this configuration to your Kubernetes cluster using `kubectl apply -f 1-llama2-pretrain-trn1-rayjob-create-test-data.yaml`. # Ensure that the Ray cluster (`kuberay-trn1`) is running and accessible in the specified namespace. # ---------------------------------------------------------------------------- apiVersion: ray.io/v1 kind: RayJob metadata: name: llama2-generate-pretraining-test-data namespace: default spec: submissionMode: K8sJobMode entrypoint: &quot;python3 get_dataset.py&quot; runtimeEnvYAML: | working_dir: /llama2_pretrain env_vars: PYTHONUNBUFFERED: '0' resources: requests: cpu: &quot;6&quot; memory: &quot;30Gi&quot; clusterSelector: ray.io/cluster: kuberay-trn1 rayClusterNamespace: default # Replace with the namespace where your RayCluster is deployed ttlSecondsAfterFinished: 60 # Time to live for the pod after completion (in seconds)   Execute the following command to run the Test Data creation Ray job:  kubectl apply -f 1-llama2-pretrain-trn1-rayjob-create-test-data.yaml   What Happens Behind the Scenes:  Job Launch: You'll use kubectl to submit the KubeRay job specification. The Ray head pod in your kuberay-trn1 cluster receives and executes this job.  Data Generation: The job runs the ai/training/raytrain-llama2-pretrain-trn1/llama2_pretrain/get_dataset.py script, which harnesses the power of the Hugging Face datasets library to fetch and process the raw English Wikipedia dataset (&quot;wikicorpus&quot;).  Tokenization: The script tokenizes the text using a pre-trained tokenizer from Hugging Face transformers. Tokenization breaks down the text into smaller units (words or subwords) for the model to understand.  Data Storage: The tokenized data is neatly organized and saved to a specific directory (/shared/wikicorpus_llama2_7B_tokenized_4k/) within your FSx for Lustre shared filesystem. This ensures all worker nodes in your cluster can readily access this standardized data during pre-training.  ","version":"Next","tagName":"h2"},{"title":"Monitoring the Job:‚Äã","type":1,"pageTitle":"Llama2 Distributed Pre-training on Trn1 with RayTrain and KubeRay","url":"/ai-on-eks/docs/blueprints/training/Neuron/RayTrain-Llama2#monitoring-the-job","content":" To keep tabs on the job's progress:  Ray Dashboard: Head over to the Ray dashboard, accessible via your Ray head pod's IP address and port 8265. You'll see real-time updates on the job's status.        Alternatively, you can use the following command in your terminal:  kubectl get pods | grep llama2   Output:  llama2-generate-pretraining-test-data-g6ccl 1/1 Running 0 5m5s   The following screenshot taken from Lens K8s IDE to show the logs of the pod.    ","version":"Next","tagName":"h3"},{"title":"5. Run Pre-compilation Job (Optimization Step)‚Äã","type":1,"pageTitle":"Llama2 Distributed Pre-training on Trn1 with RayTrain and KubeRay","url":"/ai-on-eks/docs/blueprints/training/Neuron/RayTrain-Llama2#5-run-pre-compilation-job-optimization-step","content":" info Pre-compilation job can take upto 6 min  Before starting the actual training, we'll perform a pre-compilation step to optimize the model for the Neuron SDK. This helps the model run more efficiently on the Trn1 instances. This script will use the Neuron SDK to compile and optimize the model's computational graph, making it ready for efficient training on the Trn1 processors.  In this step, you will run a pre-compilation job where the Neuron SDK will identify, compile, and cache the compute graphs associated with Llama2 pretraining.  Check out the RayJob definition spec below to run the pre-compilation job:  # ---------------------------------------------------------------------------- # RayJob: llama2-precompilation-job # # Description: # This RayJob is responsible for the pre-compilation step required for the Llama2 model # training. It runs a Python script (`ray_train_llama2.py`) with the `--neuron_parallel_compile` # option to compile the model in parallel using AWS Neuron devices. This step is crucial for # optimizing the model for efficient training on AWS infrastructure. # Usage: # Apply this configuration to your Kubernetes cluster using `kubectl apply -f 2-llama2-pretrain-trn1-rayjob-precompilation.yaml`. # Ensure that the Ray cluster (`kuberay-trn1`) is running and accessible in the specified namespace. # ---------------------------------------------------------------------------- --- apiVersion: ray.io/v1 kind: RayJob metadata: name: llama2-precompilation-job namespace: default spec: submissionMode: K8sJobMode entrypoint: &quot;NEURON_NUM_DEVICES=32 python3 /llama2_pretrain/ray_train_llama2.py --neuron_parallel_compile&quot; runtimeEnvYAML: | working_dir: /llama2_pretrain env_vars: PYTHONUNBUFFERED: '0' clusterSelector: ray.io/cluster: kuberay-trn1 rayClusterNamespace: default # Replace with the namespace where your RayCluster is deployed ttlSecondsAfterFinished: 60 # Time to live for the pod after completion (in seconds)   Execute the following command to run the pre-compilation job:  kubectl apply -f 2-llama2-pretrain-trn1-rayjob-precompilation.yaml   Verification Steps:  To monitor the job's progress and verify that it is running correctly, use the following commands and tools:  Ray Dashboard: Access the Ray dashboard via your Ray head pod's IP address and port 8265 to see real-time updates on the job's status.      The following screenshot taken from Lens K8s IDE to show the logs of the pod.    ","version":"Next","tagName":"h2"},{"title":"6. Run Distributed Pre-training Job‚Äã","type":1,"pageTitle":"Llama2 Distributed Pre-training on Trn1 with RayTrain and KubeRay","url":"/ai-on-eks/docs/blueprints/training/Neuron/RayTrain-Llama2#6-run-distributed-pre-training-job","content":" warning This job can run for many hours so feel free to cancel the job using Ctrl+C once you are convinced that the loss is decreasing and the model is learning.  Now, you're ready to begin the actual training of the Llama 2 model! This step involves running the distributed pre-training job using a RayJob. The job will utilize AWS Neuron devices to efficiently train the model with the prepared dataset.  Check out the RayJob definition spec below to run the pretraining job:  # ---------------------------------------------------------------------------- # RayJob: llama2-pretraining-job # # Description: # This RayJob is responsible for the main pretraining step of the Llama2 model. # It runs a Python script (`ray_train_llama2.py`) to perform the pretraining using AWS Neuron devices. # This step is critical for training the language model with the prepared dataset. # Usage: # Apply this configuration to your Kubernetes cluster using `kubectl apply -f 3-llama2-pretrain-trn1-rayjob.yaml`. # Ensure that the Ray cluster (`kuberay-trn1`) is running and accessible in the specified namespace. # ---------------------------------------------------------------------------- --- apiVersion: ray.io/v1 kind: RayJob metadata: name: llama2-pretraining-job namespace: default spec: submissionMode: K8sJobMode entrypoint: &quot;NEURON_NUM_DEVICES=32 python3 ray_train_llama2.py&quot; runtimeEnvYAML: | working_dir: /llama2_pretrain env_vars: PYTHONUNBUFFERED: '0' clusterSelector: ray.io/cluster: kuberay-trn1 rayClusterNamespace: default # Replace with the namespace where your RayCluster is deployed shutdownAfterJobFinishes: true activeDeadlineSeconds: 600 # The job will be terminated if it runs longer than 600 seconds (10 minutes) ttlSecondsAfterFinished: 60 # Time to live for the pod after completion (in seconds)   Execute the following command to run the pretraining job:  kubectl apply -f 3-llama2-pretrain-trn1-rayjob.yaml   Monitor Progress:  You can monitor the progress of the training job using the Ray Dashboard or by observing the logs output to your terminal. Look for information like the training loss, learning rate, and other metrics to assess how well the model is learning.    Ray Dashboard: Access the Ray dashboard via your Ray head pod's IP address and port 8265 to see real-time updates on the job's status.      ","version":"Next","tagName":"h2"},{"title":"Cleaning up‚Äã","type":1,"pageTitle":"Llama2 Distributed Pre-training on Trn1 with RayTrain and KubeRay","url":"/ai-on-eks/docs/blueprints/training/Neuron/RayTrain-Llama2#cleaning-up","content":" To remove the resources created using this solution, run the cleanup script:  # Delete the RayCluster Resources: cd ai/training/raytrain-llama2-pretrain-trn1 kubectl delete -f llama2-pretrain-trn1-raycluster.yaml # Clean Up the EKS Cluster and Associated Resources: cd ai-on-eks/infra/trainium-inferentia ./cleanup.sh  ","version":"Next","tagName":"h3"},{"title":"Guidance for AI workloads","type":0,"sectionRef":"#","url":"/ai-on-eks/docs/guidance","content":"Guidance for AI workloads This section is a collection of guidance on different topics around building and deploying AI workloads on Amazon EKS. If you have an idea about a topic you'd like to see, feel free to open an issue and start a discussion about it. info Keep in mind that this is an evolving content that we plan to update regularly based on the evolution of patterns in this space","keywords":"","version":"Next"},{"title":"Solving cold start challenges for AI/ML inference applications on Amazon EKS","type":0,"sectionRef":"#","url":"/ai-on-eks/docs/guidance/container-startup-time","content":"","keywords":"","version":"Next"},{"title":"The cold start challenge‚Äã","type":1,"pageTitle":"Solving cold start challenges for AI/ML inference applications on Amazon EKS","url":"/ai-on-eks/docs/guidance/container-startup-time#the-cold-start-challenge","content":" Containerizing AI/ML inference applications allows organizations to leverage Kubernetes's automated scaling to handle fluctuating demand, unified resource orchestration to manage expensive GPU hardware, and declarative configuration to simplify their deployment pipelines and operations.  While Kubernetes offers a lot of benefits to make running applications easier, application cold start is still a challenge.  Although acceptable startup times differ between AI/ML inference applications, delays of tens of seconds to several minutes create cascading effects throughout the system, impacting user experience, application performance, operational efficiency, infrastructure costs, and time-to-market.  Key impacts include:  Increased request latency and degraded user experienceIdle expensive GPU resourcesReduced responsiveness of auto-scaling processes during traffic spikesLong feedback loops during deployment, experimentation, testing and debugging  To effectively evaluate and implement the solutions in this guide, it is important to outline, and later explore further, several compounding factors that contribute to application container startup time:  Provision and bootstrapping of the compute capacity (e.g., Amazon EC2 instances) to host the applicationDownloading of the typically large container images and model artifacts  The rest of the guide explores recommended patterns and solutions for these factors for applications deployed on Amazon Elastic Kubernetes Service (Amazon EKS).  Each solution provides an implementation guide covering the following aspects:  In-depth discussion of the recommended AWS and Kubernetes architectureImplementation details, along with code examplesWays in which it benefits the main purposeAny potential additional benefits and integrations with other solutionsTrade-offs to take into account  ","version":"Next","tagName":"h2"},{"title":"Improving container startup time‚Äã","type":1,"pageTitle":"Solving cold start challenges for AI/ML inference applications on Amazon EKS","url":"/ai-on-eks/docs/guidance/container-startup-time#improving-container-startup-time","content":" Container image pull time is a primary contributor to the startup latency of AI/ML inference applications. These applications may reach sizes of multiple gigabytes due to inclusion of framework dependencies (e.g., PyTorch or TensorFlow), runtimes (e.g., TorchServe, Triton or Ray Serve), and bundled model files and associated artifacts.  In this section we will focus on the following solutions:  reducing the overall size of the imagemaking the container image pull process more efficient  As we explore the solutions and their tradeoffs, we will refer to various layers and components that go into creating an AI/ML inference application container as depicted in the diagram in Figure 1.  Figure 1: An AI/ML inference application container image layers  Note that lower layers in the diagram may already contain artifacts from higher layers (e.g., PyTorch AI/ML framework and inference runtime components bundled in the pytorch/pytorch base OS image and not installed separately), which can either simplify or complicate optimization. ","version":"Next","tagName":"h2"},{"title":"Accelerating pull process","type":0,"sectionRef":"#","url":"/ai-on-eks/docs/guidance/container-startup-time/accelerate-pull-process","content":"Accelerating pull process The solutions in this section reduce the container startup time by improving the image pull process. They do it by either relying on the images‚Äô layered internal structure and changing how their layers are retrieved over the network from a container registry or by skipping the registry altogether. Container images are composed of layers, implemented by content-addressable blobs, stored in registries like Amazon ECR. When a pod is scheduled onto a worker node, the container runtime, typically containerd, retrieves and mounts these read-only layers using a union file-system, such as OverlayFS, then attaches a writable, ephemeral layer on top to complete the container filesystem. containerd is implemented as a modular container runtime with several pluggable components. A snapshotter, which is the focus of the first two solutions, is a pluggable component that is responsible for the assembly of the image layers. The default OverlayFS snapshotter fully unpacks image layers to disk before container start and does not support lazy or partial layer extraction. Only after all layers are unpacked and mounted via OverlayFS can the container‚Äôs unified filesystem be presented. The last step of the process described above is blocking and sequential, and thus has a very significant impact on the container startup time. Instead of fully extracting each layer to disk, advanced snapshotters, such as SOCI (Seekable OCI) or Nydus, create virtual, mountable snapshots, lazy-loading files from a registry or a remote storage as they are accessed, which has a lower I/O overhead and drastically improves container startup time. Instead of improving the pull process by optimizing container image layers retrieval and storage, the last solution prefetches all the image layers onto a data volume, into the container runtime cache, of a Bottlerocket EC2 machine during a CI/CD process. The process then takes a snapshot of the volume to be mounted onto EKS Bottlerocket worker nodes, creating a warmed-up cached for the container runtime on them.","keywords":"","version":"Next"},{"title":"Preload container images into Bottlerocket data volumes with EBS Snapshots","type":0,"sectionRef":"#","url":"/ai-on-eks/docs/guidance/container-startup-time/accelerate-pull-process/prefecthing-images-on-br","content":"","keywords":"","version":"Next"},{"title":"Overview of this script‚Äã","type":1,"pageTitle":"Preload container images into Bottlerocket data volumes with EBS Snapshots","url":"/ai-on-eks/docs/guidance/container-startup-time/accelerate-pull-process/prefecthing-images-on-br#overview-of-this-script","content":"   Launch an EC2 instance with Bottlerocket for EKS AMI.Access to instance via Amazon System ManagerPull images to be cached in this EC2 using Amazon System Manager Run Command.Shut down the instance, build the EBS snapshot for the data volume.Terminate the instance.  ","version":"Next","tagName":"h2"},{"title":"Usage Example‚Äã","type":1,"pageTitle":"Preload container images into Bottlerocket data volumes with EBS Snapshots","url":"/ai-on-eks/docs/guidance/container-startup-time/accelerate-pull-process/prefecthing-images-on-br#usage-example","content":" git clone https://github.com/aws-samples/bottlerocket-images-cache/ cd bottlerocket-images-cache/ # Using nohup in terminals to avoid disconnections ‚ùØ nohup ./snapshot.sh --snapshot-size 150 -r us-west-2 \\ docker.io/rayproject/ray-ml:2.10.0-py310-gpu,public.ecr.aws/data-on-eks/ray2.11.0-py310-gpu-stablediffusion:latest &amp; ‚ùØ tail -f nohup.out 2024-07-15 17:18:53 I - [1/8] Deploying EC2 CFN stack ... 2024-07-15 17:22:07 I - [2/8] Launching SSM . 2024-07-15 17:22:08 I - SSM launched in instance i-07d10182abc8a86e1. 2024-07-15 17:22:08 I - [3/8] Stopping kubelet.service .. 2024-07-15 17:22:10 I - Kubelet service stopped. 2024-07-15 17:22:10 I - [4/8] Cleanup existing images .. 2024-07-15 17:22:12 I - Existing images cleaned 2024-07-15 17:22:12 I - [5/8] Pulling images: 2024-07-15 17:22:12 I - Pulling docker.io/rayproject/ray-ml:2.10.0-py310-gpu - amd64 ... 2024-07-15 17:27:50 I - docker.io/rayproject/ray-ml:2.10.0-py310-gpu - amd64 pulled. 2024-07-15 17:27:50 I - Pulling docker.io/rayproject/ray-ml:2.10.0-py310-gpu - arm64 ... 2024-07-15 17:27:58 I - docker.io/rayproject/ray-ml:2.10.0-py310-gpu - arm64 pulled. 2024-07-15 17:27:58 I - Pulling public.ecr.aws/data-on-eks/ray2.11.0-py310-gpu-stablediffusion:latest - amd64 ... 2024-07-15 17:31:34 I - public.ecr.aws/data-on-eks/ray2.11.0-py310-gpu-stablediffusion:latest - amd64 pulled. 2024-07-15 17:31:34 I - Pulling public.ecr.aws/data-on-eks/ray2.11.0-py310-gpu-stablediffusion:latest - arm64 ... 2024-07-15 17:31:36 I - public.ecr.aws/data-on-eks/ray2.11.0-py310-gpu-stablediffusion:latest - arm64 pulled. 2024-07-15 17:31:36 I - [6/8] Stopping instance ... 2024-07-15 17:32:25 I - Instance i-07d10182abc8a86e1 stopped 2024-07-15 17:32:25 I - [7/8] Creating snapshot ... 2024-07-15 17:38:36 I - Snapshot snap-0c6d965cf431785ed generated. 2024-07-15 17:38:36 I - [8/8] Cleanup. 2024-07-15 17:38:37 I - Stack deleted. 2024-07-15 17:38:37 I - -------------------------------------------------- 2024-07-15 17:38:37 I - All done! Created snapshot in us-west-2: snap-0c6d965cf431785ed   You can copy the snapshot ID snap-0c6d965cf431785ed and configure it as a snapshot for worker nodes.  Using Snapshot with Amazon EKS and Karpenter  You can specify snapshotID in a Karpenter node class (another option is to use the Infrastructure Blueprints in this repo, and pass the snapshotID as part of the IaC configuration). Add the content on EC2NodeClass:  apiVersion: karpenter.k8s.aws/v1beta1 kind: EC2NodeClass metadata: name: default spec: amiFamily: Bottlerocket # Ensure OS is BottleRocket blockDeviceMappings: - deviceName: /dev/xvdb ebs: volumeSize: 150Gi volumeType: gp3 kmsKeyID: &quot;arn:aws:kms:&lt;REGION&gt;:&lt;ACCOUNT_ID&gt;:key/1234abcd-12ab-34cd-56ef-1234567890ab&quot; # Specify KMS ID if you use custom KMS key snapshotID: snap-0123456789 # Specify your snapshot ID here   End-to-End deployment example  An end-to-end deployment example can be found in Stable Diffusion on GPU. ","version":"Next","tagName":"h2"},{"title":"Reducing container image size","type":0,"sectionRef":"#","url":"/ai-on-eks/docs/guidance/container-startup-time/reduce-container-image-size","content":"Reducing container image size Image compressed size ‚Äì the size it occupies in the container image registry, directly correlates with pull time, while image uncompressed size ‚Äì the size of the image once downloaded, impacts the time it takes to bootstrap the container ‚Äì the larger the size the longer it takes to decompress and extract the image layers, mount them, and combine them into the container file system. While there are other contributing factors, like number of image layers, their structure, deduplication and container runtime cache efficiency, or registry proximity, paying special attention to size optimization is an important step on the road to reducing startup latency. As described in AWS Container Build lens, traditional optimization techniques, such as multi-stage builds and layer structuring that benefits loading and caching, provide foundational improvements to container image size and overall performance and cost efficiency, regardless of application type. For AI/ML inference application container images, decoupling large components often yields an additional significant size reduction, since these components, particularly model artifacts and serving frameworks, often exceed several gigabytes (GB) in size. The solutions in this section focus on both traditional optimization techniques and architectural patterns that extract these large components from the container image to a different delivery system, like Amazon S3 or Amazon FSx. Note that the relocated components must still be made available to the application, requiring any new delivery system to be faster than the image pull process. The subsequent Accelerating pull process section addresses how to optimize the retrieval of container images.","keywords":"","version":"Next"},{"title":"Using containerd snapshotter","type":0,"sectionRef":"#","url":"/ai-on-eks/docs/guidance/container-startup-time/accelerate-pull-process/containerd-snapshotter","content":"","keywords":"","version":"Next"},{"title":"Using SOCI snapshotter‚Äã","type":1,"pageTitle":"Using containerd snapshotter","url":"/ai-on-eks/docs/guidance/container-startup-time/accelerate-pull-process/containerd-snapshotter#using-soci-snapshotter","content":" This solution simply relies on the organic improvements of the image pull process by plugging SOCI snapshotter (v0.11.0+) into containerd. This is currently isn‚Äôt the default in EKS AMIs, but should eventually become one.  Architecture overview  There are no architectural changes, as this requires to bootstrap your worker nodes with the snapshotter via userData in the relevant Karpenter node classes or launch templates for the non-Karpenter instance provision schemes.  The new SOCI snapshotter implementation introduces a non-lazy-loading pull mode that that pulls large layers in chunks, allowing them to be pulled faster, similar in idea to the multipart layer fetch introduced in containerd 2.1.0. By using a temporary file buffer instead of in-memory one, SOCI is able to parallelize the layers store and decompression operations, which results in a much faster image pulls (as permitted by hardware limitations).  Implementation guide  Below is a schematic implementation of the above changes:  info For a complete example on how to use SOCI snapshotter, please refer to this guide.  apiVersion: karpenter.k8s.aws/v1 kind: EC2NodeClass metadata: name: soci-snapshotter spec: role: KarpenterNodeRole-my-cluster instanceStorePolicy: RAID0 subnetSelectorTerms: - tags: karpenter.sh/discovery: my-cluster-private securityGroupSelectorTerms: - tags: karpenter.sh/discovery: my-cluster-private amiSelectorTerms: - alias: al2023@latest userData: | MIME-Version: 1.0 Content-Type: multipart/mixed; boundary=&quot;//&quot; --// Content-Type: text/x-shellscript; charset=&quot;us-ascii&quot; # 1. Detect the architecture # 2. Download the v0.11.0+ SOCI snapshotter version # at https://github.com/awslabs/soci-snapshotter/releases/download/... # 3. Configure the snapshotter by creating a config.toml file # 4. Configure the snapshotter service by creating a systemd config file # 5. Enable the snapshotter --// Content-Type: application/node.eks.aws apiVersion: node.eks.aws/v1alpha1 kind: NodeConfig spec: kubelet: config: imageServiceEndpoint: unix:///run/soci-snapshotter-grpc/soci-snapshotter-grpc.sock containerd: config: | [proxy_plugins.soci] type = &quot;snapshot&quot; address = &quot;/run/soci-snapshotter-grpc/soci-snapshotter-grpc.sock&quot; [proxy_plugins.soci.exports] root = &quot;/var/lib/containerd/io.containerd.snapshotter.v1.soci&quot; [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd] snapshotter = &quot;soci&quot; disable_snapshot_annotations = false discard_unpacked_layers = false --//   Main benefits  The solution provides a direct improvement to the image pull process by plugging a more performant snapshotter into containerd on a worker node.  Additional benefits  This solution requires no changes to the development process, no additional infrastructure and once enabled by default, no changes in code or configuration.  Trade-offs  Before the snapshotter becomes the default, this requires to implement and maintain a userData bootstrapping of the worker node described above. Once it becomes the default ‚Üí none. ","version":"Next","tagName":"h2"},{"title":"AIBrix on EKS","type":0,"sectionRef":"#","url":"/ai-on-eks/docs/infra/ai-ml/aibrix","content":"","keywords":"","version":"Next"},{"title":"What is AIBrix?‚Äã","type":1,"pageTitle":"AIBrix on EKS","url":"/ai-on-eks/docs/infra/ai-ml/aibrix#what-is-aibrix","content":" AIBrix is an open source initiative designed to provide essential building blocks to construct scalable GenAI inference infrastructure. AIBrix delivers a cloud-native solution optimized for deploying, managing, and scaling large language model (LLM) inference, tailored specifically to enterprise needs.  ","version":"Next","tagName":"h3"},{"title":"Key Features and Benefits‚Äã","type":1,"pageTitle":"AIBrix on EKS","url":"/ai-on-eks/docs/infra/ai-ml/aibrix#key-features-and-benefits","content":" LLM Gateway and Routing: Efficiently manage and direct traffic across multiple models and replicas.High-Density LoRA Management: Streamlined support for lightweight, low-rank adaptations of models.Distributed Inference: Scalable architecture to handle large workloads across multiple nodes.LLM App-Tailored Autoscaler: Dynamically scale inference resources based on real-time demand.Unified AI Runtime: A versatile sidecar enabling metric standardization, model downloading, and management.Heterogeneous-GPU Inference: Cost-effective SLO-driven LLM inference using heterogeneous GPUs.GPU Hardware Failure Detection: Proactive detection of GPU hardware issues.  Deploying the Solution üëà  Verify Deployment üëà  Clean Up üëà ","version":"Next","tagName":"h3"},{"title":"Introduction","type":0,"sectionRef":"#","url":"/ai-on-eks/docs/infra/ai-ml","content":"","keywords":"","version":"Next"},{"title":"Resources‚Äã","type":1,"pageTitle":"Introduction","url":"/ai-on-eks/docs/infra/ai-ml#resources","content":" Each stack inherits the base stack's components. These components include:  VPC with subnets in 2 availability zonesEKS cluster with 1 core nodegroup with 2 nodes to run the minimum infrastructureKarpenter for autoscaling with CPU, GPU, and AWS Neuron NodePoolsGPU/Neuron device driversGPU/Neuron monitoring agents  ","version":"Next","tagName":"h2"},{"title":"Variables‚Äã","type":1,"pageTitle":"Introduction","url":"/ai-on-eks/docs/infra/ai-ml#variables","content":" ","version":"Next","tagName":"h2"},{"title":"Deployment‚Äã","type":1,"pageTitle":"Introduction","url":"/ai-on-eks/docs/infra/ai-ml#deployment","content":" Variable Name\tDescription\tDefaultname\tThe name of the Kubernetes cluster\tai-stack region\tThe region for the cluster\tus-east-1 eks_cluster_version\tThe version of EKS to use\t1.32 vpc_cidr\tThe CIDR used for the VPC\t10.1.0.0/21 secondary_cidr_blocks\tSecondary CIDR for the VPC\t100.64.0.0/16 enable_database_subnets\tWhether or not to enable the database subnets\tfalse enable_aws_cloudwatch_metrics\tEnable the AWS Cloudwatch Metrics addon\tfalse bottlerocket_data_disk_snapshot_id\tAttach a snapshot ID to the deployed nodes\t&quot;&quot; enable_aws_efs_csi_driver\tEnable the AWS EFS CSI driver\tfalse enable_aws_efa_k8s_device_plugin\tEnable the AWS EFA device plugin\tfalse enable_aws_fsx_csi_driver\tEnable the FSx device plugin\tfalse deploy_fsx_volume\tDeploy a simple FSx volume\tfalse enable_amazon_prometheus\tEnable Amazon Managed Prometheus\tfalse enable_amazon_emr\tSet up Amazon EMR\tfalse enable_kube_prometheus_stack\tEnable the Kube Prometheus addon\tfalse enable_kubecost\tEnable Kubecost\tfalse enable_ai_ml_observability_stack\tEnable AI/ML observability addon\tfalse enable_argo_workflows\tEnable Argo Workflow\tfalse enable_argo_events\tEnable Argo Events\tfalse enable_argocd\tEnable ArgoCD addon\tfalse enable_mlflow_tracking\tEnable MLFlow Tracking\tfalse enable_jupyterhub\tEnable JupyterHub\tfalse enable_volcano\tEnable Volcano\tfalse enable_kuberay_operator\tEnable KubeRay\tfalse huggingface_token\tHugging Face token to use in environment\tDUMMY_TOKEN_REPLACE_ME enable_rayserve_ha_elastic_cache_redis\tEnable Rayserve high availability using ElastiCache\tfalse enable_torchx_etcd\tEnable etcd for torchx\tfalse enable_mpi_operator\tEnable the MPI Operator\tfalse enable_aibrix_stack\tEnable the AIBrix stack\tfalse aibrix_stack_version\tAIBrix Stack version\tv0.2.1  ","version":"Next","tagName":"h3"},{"title":"JupyterHub‚Äã","type":1,"pageTitle":"Introduction","url":"/ai-on-eks/docs/infra/ai-ml#jupyterhub","content":" Variable Name\tDescription\tDefaultjupyter_hub_auth_mechanism\tWhich authorization mechanism to use for JupyterHub [dummy | cognito | oauth]\tdummy cognito_custom_domain\tCognito domain prefix for Hosted UI authentication endpoints\teks acm_certificate_domain\tDomain name used for the ACM certificate\t&quot;&quot; jupyterhub_domain\tDomain name for JupyterHub (only used for cognito or oauth)\t&quot;&quot; oauth_jupyter_client_id\toauth clientid for JupyterHub. Only used for oauth\t&quot;&quot; oauth_jupyter_client_secret\toauth client secret. Only used for oauth\t&quot;&quot; oauth_username_key\toauth field for username (e.g. preferred_username). Only needed for oauth\t&quot;&quot;  ","version":"Next","tagName":"h3"},{"title":"Custom Stacks‚Äã","type":1,"pageTitle":"Introduction","url":"/ai-on-eks/docs/infra/ai-ml#custom-stacks","content":" With the variables above, it's very easy to compose a new environment tailored to your own needs. A custom folder is available in the infra folder with a simple blueprint.tfvars. By adding the variables above with the appropriate value, you are able to customize which addons you would like deployed to create an environment to support your preferences. Once the variables are added, run the install.sh at the root of infra/custom ","version":"Next","tagName":"h2"},{"title":"Optimizing container images size","type":0,"sectionRef":"#","url":"/ai-on-eks/docs/guidance/container-startup-time/reduce-container-image-size/optimize-image-size","content":"","keywords":"","version":"Next"},{"title":"Selecting appropriate base images‚Äã","type":1,"pageTitle":"Optimizing container images size","url":"/ai-on-eks/docs/guidance/container-startup-time/reduce-container-image-size/optimize-image-size#selecting-appropriate-base-images","content":" Different AI/ML frameworks and platforms offer ready-to-use container images, providing convenience and enabling experimentation. However, these images often try to address as wide a feature set as possible and thus may include different runtimes, frameworks, or supported APIs, leading to bloat that is unsuitable for production container images.  For example, different PyTorch image variants have very different sizes: from 2.7.1-cuda11.8-cudnn9-devel (6.66 GB) that includes development tools, compilers etc. to 2.7.1-cuda11.8-cudnn9-runtime (3.03 GB) that contains runtime only. The vLLM project provides several container image variants, each with different capabilities packaged in, such as the OpenAI spec, Sagemaker integration, and more.  Selecting a smaller base image that satisfies the needs of the application can make a big difference, with a caveat ‚Äì smaller runtime-only images may not include JIT compilation or dynamic optimization and thus fall on the slower code paths, reducing startup time.  A production-oriented comprehensive approach would include:  benchmarking workloads with different base imagesconsidering custom builds that include only required optimization librariestesting overall cold start performance in addition to image pull time improvements  ","version":"Next","tagName":"h2"},{"title":"Using multi-stage builds‚Äã","type":1,"pageTitle":"Optimizing container images size","url":"/ai-on-eks/docs/guidance/container-startup-time/reduce-container-image-size/optimize-image-size#using-multi-stage-builds","content":" Multi-stage container image builds, supported by multiple platforms such as Docker/BuildKit, Podman, Finch/Buildkit, allow use of multiple FROM statements in a single container image file to separate build process and artifacts from the runtime concerns.  A multi-stage build container image file may look similar to the following:  # Build stage that also uses an external image to copy artifacts FROM python:3.12-slim-bookworm AS builder COPY --from=ghcr.io/astral-sh/uv:0.7.11 /uv /uvx /bin/ ... # Runtime stage FROM python:3.12-slim-bookworm ... COPY --from=models some-model /app/models/some-model/configs COPY --from=builder --chown=app:app /app/.venv ./.venv COPY --from=builder --chown=app:app /app/main.py ./main.py ... CMD [&quot;sh&quot;, &quot;-c&quot;, &quot;exec fastapi run --host 0.0.0.0 --port 80 /app/main.py&quot;]   info Unlike typical application dependencies baked into container images, copying large model files (ranging from a few GBs to tens of GBs) is generally discouraged. This is due to increased container image size which affect its pull time, separate release lifecycle for the app and model, and potential storage duplication when sharing models across multiple apps.  Copying only the required artifacts allows fine-grained control over which components of the build result are to be included in the final runtime image, reducing its size (along with other benefits, like security or workflow simplicity).  In the example above we also employed two other variations of COPY --from (supported via BuildKit by the majority of popular image building platforms) :  COPY --from=&lt;path/to/image/in/registry and parts to be taken&gt; allows to extract only the specific files and folders from another container image stored in a registryCOPY --from=&lt;name of the build context&gt; allows to copy only the specific files and folders from a local folder, provided as a parameter to the build command using --build-context models=/path/to/local/folder  Note that while using .dockerignore is a good practice, in general, and should be used alongside the above process, it doesn‚Äôt impact the COPY --from=... commands.  Additionally this technique can be further (if sometimes marginally) improved by using the following one, taking caveats into consideration.  ","version":"Next","tagName":"h2"},{"title":"Employing layer optimization techniques‚Äã","type":1,"pageTitle":"Optimizing container images size","url":"/ai-on-eks/docs/guidance/container-startup-time/reduce-container-image-size/optimize-image-size#employing-layer-optimization-techniques","content":" As image layers (now smaller in total size) are downloaded during the pull process, they are decompressed and unpacked to assemble the container‚Äôs file system. The amount and size of the image layers have an impact of the duration of that process, serving as another candidate for optimization.  One commonly mentioned optimization involves combining the RUN or COPY commands to create smaller amount of larger layers, with the following typical example:  FROM ... RUN apt-get update &amp;&amp; \\ apt-get install -y --no-install-recommends \\ build-essential \\ libssl-dev \\ pkg-config &amp;&amp; \\ rm -rf /var/lib/apt/lists/* &amp;&amp; \\ apt-get clean   In context of the recommendation to employ multi-stage builds, RUN isn‚Äôt often a part of the last ‚Äì runtime stage. This is because, ideally, all execution is done during the previous build stages and the processed artifacts are only copied into their intended locations in the runtime stage.  The COPY command, in the runtime stage, can be optimized using this process, with one challenge ‚Äì it doesn‚Äôt support multiple destinations or multiple source stages, so combining these commands into one isn‚Äôt possible:  COPY --from=models some-model /app/models/some-model/configs COPY --from=builder1 --chown=app:app /app/.venv ./.venv COPY --from=builder1 --chown=app:app /app/main.py ./main.py COPY --from=builder2 --chown=app:app /app/config.json ./config.json   To overcome this, an additional copying stage may be introduced, where the final folder structure is created and then copied via a single command:  FROM python:3.12-bookworm AS builder1 WORKDIR /app ... FROM pytorch/pytorch:2.7.1-cuda11.8-cudnn9-devel AS builder2 WORKDIR /app ... FROM scratch AS assembly ... COPY --from=models some-model/weights /app/models/some-model/weights COPY --from=builder1 /app/.venv /app/venv COPY --from=builder1 /app/main.py /app/main.py COPY --from=builder2 /dist/config.json /app/config.json FROM python:3.12-slim-bookworm COPY --from=assembly --chown=app:app /app /app CMD [&quot;python&quot;, &quot;main.py&quot;]   Take into consideration that even when the above steps have a positive effect on the overall container startup time, it is often negligible, relative to other solutions in this guide and should be assessed before investing time into the technique.  The improvement should be weighted against the trade-offs that include:  reduction in cache efficiency, due to its lower granularity with a smaller amount of larger layers if layers are not ordered correctlybuild time, due to more shuffling for the sake of optimization of the runtime layer ","version":"Next","tagName":"h2"},{"title":"JARK on EKS","type":0,"sectionRef":"#","url":"/ai-on-eks/docs/infra/ai-ml/jark","content":"","keywords":"","version":"Next"},{"title":"What is JARK?‚Äã","type":1,"pageTitle":"JARK on EKS","url":"/ai-on-eks/docs/infra/ai-ml/jark#what-is-jark","content":" JARK is a powerful stack composed of JupyterHub, Argo Workflows, Ray, and Kubernetes, designed to streamline the deployment and management of Generative AI models on Amazon EKS. This stack brings together some of the most effective tools in the AI and Kubernetes ecosystem, offering a robust solution for training, fine-tuning, and inference large AI models.  JARK comes enabled with AI/ML Observability. Please see the observability section for details on the observability architecture.  ","version":"Next","tagName":"h3"},{"title":"Key Features and Benefits‚Äã","type":1,"pageTitle":"JARK on EKS","url":"/ai-on-eks/docs/infra/ai-ml/jark#key-features-and-benefits","content":" JupyterHub: Provides a collaborative environment for running notebooks, crucial for model development and prompt engineering.  Argo Workflows: Automates the entire AI model pipeline‚Äîfrom data preparation to model deployment‚Äîensuring a consistent and efficient process.  Ray: Scales AI model training and inference across multiple nodes, making it easier to handle large datasets and reduce training time.  Kubernetes: Powers the stack by providing the necessary orchestration to run, scale, and manage containerized AI models with high availability and resource efficiency.  ","version":"Next","tagName":"h3"},{"title":"Why Use JARK?‚Äã","type":1,"pageTitle":"JARK on EKS","url":"/ai-on-eks/docs/infra/ai-ml/jark#why-use-jark","content":" The JARK stack is ideal for teams and organizations looking to simplify the complex process of deploying and managing AI models. Whether you're working on cutting-edge generative models or scaling existing AI workloads, JARK on Amazon EKS offers the flexibility, scalability, and control you need to succeed.    ","version":"Next","tagName":"h3"},{"title":"Ray on Kubernetes‚Äã","type":1,"pageTitle":"JARK on EKS","url":"/ai-on-eks/docs/infra/ai-ml/jark#ray-on-kubernetes","content":" Ray is an open-source framework for building scalable and distributed applications. It is designed to make it easy to write parallel and distributed Python applications by providing a simple and intuitive API for distributed computing. It has a growing community of users and contributors, and is actively maintained and developed by the Ray team at Anyscale, Inc.    Source: https://docs.ray.io/en/latest/cluster/key-concepts.html  To deploy Ray in production across multiple machines users must first deploy Ray Cluster. A Ray Cluster consists of head nodes and worker nodes which can be autoscaled using the built-in Ray Autoscaler.  Deploying Ray Cluster on Kubernetes including on Amazon EKS is supported via the KubeRay Operator. The operator provides a Kubernetes-native way to manage Ray clusters. The installation of KubeRay Operator involves deploying the operator and the CRDs for RayCluster, RayJob and RayService as documented here.  Deploying Ray on Kubernetes can provide several benefits:  Scalability: Kubernetes allows you to scale your Ray cluster up or down based on your workload requirements, making it easy to manage large-scale distributed applications. Fault tolerance: Kubernetes provides built-in mechanisms for handling node failures and ensuring high availability of your Ray cluster. Resource allocation: With Kubernetes, you can easily allocate and manage resources for your Ray workloads, ensuring that they have access to the necessary resources for optimal performance. Portability: By deploying Ray on Kubernetes, you can run your workloads across multiple clouds and on-premises data centers, making it easy to move your applications as needed. Monitoring: Kubernetes provides rich monitoring capabilities, including metrics and logging, making it easy to troubleshoot issues and optimize performance.  Overall, deploying Ray on Kubernetes can simplify the deployment and management of distributed applications, making it a popular choice for many organizations that need to run large-scale machine learning workloads.  Before moving forward with the deployment please make sure you have read the pertinent sections of the official documentation.    Source: https://docs.ray.io/en/latest/cluster/kubernetes/index.html  Deploying the Solution üëà  Verify Deployment üëà  Clean Up üëà ","version":"Next","tagName":"h3"},{"title":"Decoupling model artifacts from container image","type":0,"sectionRef":"#","url":"/ai-on-eks/docs/guidance/container-startup-time/reduce-container-image-size/decoupling-model-artifacts","content":"","keywords":"","version":"Next"},{"title":"Using init containers to download model artifacts from Amazon S3‚Äã","type":1,"pageTitle":"Decoupling model artifacts from container image","url":"/ai-on-eks/docs/guidance/container-startup-time/reduce-container-image-size/decoupling-model-artifacts#using-init-containers-to-download-model-artifacts-from-amazon-s3","content":" This solution provides a straightforward approach to extracting any model files or associated artifacts to an external storage. The artifacts are tagged, versioned and placed in an Amazon S3 bucket during the development or CI/CD phases, along with the appropriate lifecycle policy to control their retention. Once referenced by an application, they are downloaded to shared volumes during the application‚Äôs pod initialization using the Kubernetes native init containers, which are executed sequentially before the main application container.  Architecture overview  The diagram in Figure 1 shows the architecture for the solution, which includes the personas, AWS services, Kubernetes components, and artifacts that are being created, stored and retrieved as part of the data flow.  Figure 1: Init containers architecture to retrieve model artifacts from Amazon S3  Implementation guide  Following the architecture diagram above, these are the main high-level steps for each of the teams.  The DevOps/MLOps/Platform team:  alters the Kubernetes deployment manifest (e.g., YAML files, Helm charts) to include init containers,implements, using the init containers that are executed first, the steps to download model artifacts to an EC2 volumealters the Kubernetes deployment manifests to: define a volume shared between init containers and the application containermount the volume under the expected paths in the application container adjusts pod IAM permissions to allow init containers to access the appropriate prefix in the bucketcreates an Amazon S3 bucket to store model artifactsalters the container image definition to exclude the model artifacts during the build stage  ML or application teams:  upload model artifacts to an Amazon S3 bucket (as part of their SDLC)continue to push their container image changes to Amazon ECR, as beforecontinue to use the Kubernetes deployment manifests to define the application Kubernetes deployment, as before  The following code shows an already-adjusted Kubernetes deployment manifest:  apiVersion: apps/v1 kind: Deployment metadata: name: my-inference-app namespace: apps spec: selector: matchLabels: app.kubernetes.io/name: my-inference-app replicas: 1 template: metadata: labels: app.kubernetes.io/name: my-inference-app spec: serviceAccountName: my-inference-app initContainers: - name: download image: peakcom/s5cmd command: - /bin/sh - -c - '/s5cmd sync s3://my-ml-bucket/model-artifacts/my-model-1.2.3/* /model-artifacts/my-model-1.2.3' resources: ... volumeMounts: - mountPath: /model-artifacts name: model-artifacts containers: - name: app image: &lt;account-id&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com/my-inference-app:3.5.0 ports: - name: app-port containerPort: 6060 resources: ... volumeMounts: - mountPath: /app/model-artifacts name: model-artifacts volumes: - emptyDir: {} name: model-artifacts   The example above follows the steps outlined above and uses s5cmd, which is an open-source tool that provides excellent performance for downloading files from Amazon S3. The implementation relies on the fact that the current, everything-in-image solution bundles the my-model-1.2.3 model artifacts in the container image under the /app/model-artifacts and mimics that behavior by placing them in the same location.  Note that the my-inference-app service account listed in the manifest above needs to have the appropriate IAM permissions to read from the corresponding prefix in the my-ml-bucket bucket. On Amazon EKS, the recommended way to achieve that is to use Amazon EKS Pod Identity add-on. Amazon EKS Pod Identity provides a way to create, via the AWS API, an association between a Kubernetes service account and an AWS IAM role. When deployed, Pod Identity add-on places an agent (via DaemonSet) on every node in the cluster. and then allows pods with that service account to extract the required credentials, at runtime, from that agent.  Main benefits  This solution implements the high-level approach of improving AI/ML inference application containers startup performance by reducing the container image size. Depending on networking conditions, and s5cmd superior performance, it may also improve the image pull directly.  Additional benefits  In addition to the main benefit, the solution introduces the following potential additional benefits:  Ability to update model versions separately from the application, without rebuilding the container imageAbility to A/B test, hot-swap or rollback models, without rebuilding or increasing the container image sizeAbility to share models between different applications without packaging them into all container imagesReduction in storage cost and ability to utilize Amazon S3 storage classes, include Intelligent-TieringAbility to have a fine-grained control over access to models with Amazon EKS Pod Identity session tagsFaster container builds, aiding experimentation and testingMinimal changes in the ML or application teams workflowSimple cross-region replication  Trade-offs  The trade-offs include:  Additional operational complexity of making the required CI/CD changes and handling the download process, including retries, error handling, backoff etc.Additional storage and cleanup management, which essentially replicates the ECR functionalityIn cases where additional replicas of the application often land on the same EC2 instances, having the image stored in on-host container runtime cache may be more beneficial to their containers startup time  Variants and hybrid solutions  This solution integrates well with the Accelerating pull process group of solutions, improving both the container image size and its pull time.  For AI/ML inference application that read the artifacts into memory, skipping the intermediate step of storing them on the local disk, the ‚ÄúUsing Mountpoint CSI Driver to read model artifacts into memory directly from Amazon S3‚Äù solution may replace or be used in addition to improve the download performance further.  ","version":"Next","tagName":"h2"},{"title":"Using Amazon File Storage services to host model artifacts‚Äã","type":1,"pageTitle":"Decoupling model artifacts from container image","url":"/ai-on-eks/docs/guidance/container-startup-time/reduce-container-image-size/decoupling-model-artifacts#using-amazon-file-storage-services-to-host-model-artifacts","content":" This section covers different Amazon FSx services that can be used to decouple the model artifacts from a container image. Amazon FSx offers a range of services like FSx for OpenZFS, FSx for Lustre, or FSx for NetApp ONTAP. These services differs by the technology that back them, and have different performance and scale characteristics. To learn more about these services, refer to the documentation.  Choosing the appropriate Amazon FSx service depends on the characteristics of your use case. Factors like the size of the model, the number of models, update frequency, and the number of clients that pull the models, may impact the chosen service.  info This section is WIP, and will get more FSx service specific guides in the future.  ","version":"Next","tagName":"h2"},{"title":"Using Trident CSI driver to provide access to model artifacts on Amazon FSx for NetApp ONTAP‚Äã","type":1,"pageTitle":"Decoupling model artifacts from container image","url":"/ai-on-eks/docs/guidance/container-startup-time/reduce-container-image-size/decoupling-model-artifacts#using-trident-csi-driver-to-provide-access-to-model-artifacts-on-amazon-fsx-for-netapp-ontap","content":" Architecture overview  The diagram in Figure 2 shows the architecture for the solution, which includes the personas, AWS services, Kubernetes components, and artifacts that are being created, stored and retrieved as part of the data flow.  Figure 2: Using Trident CSI driver to store and access model artifacts on FSx for NetApp ONTAP  Implementation guide  Following the architecture diagram above, these are the main high-level steps for each of the teams.  The DevOps/MLOps/Platform team:  creates the FSx for NetApp ONTAP file system, the storage virtual machines (SVM) and the credentials secretsinstalls the Trident CSI driver, provides the required IAM permissions and defines a Trident CSI storage classdefines a Trident NAS backend using the file system, SVM and the credentialsdefines and deploys a dynamic provision PVC, with the above storage class, shared to the application namespaceimplements a Kubernetes job that mounts the PVC and downloads the model onto the corresponding volumetriggers the job for each model that is uploaded to S3 and ‚Äúmarked‚Äù as published (e.g., via tags)alters the container image definition to exclude the model artifacts during the build stagealters the application manifest to: include a PVC that imports the shared PVCdefine a volume for the application podmount the volume under the expected paths in the application container  ML or application teams:  upload model artifacts to an Amazon S3 bucket (as part of their SDLC) or Git LFScontinue to push their container image changes to Amazon ECR, as beforecontinue to use the Kubernetes deployment manifests to define the application Kubernetes deployment, as before  To further illustrate the above steps, what follows is a collection of the relevant code examples for the most important parts of the implementation.  This is the storage class that to be handled by the Trident CSI driver:  apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: ontap-nas provisioner: csi.trident.netapp.io volumeBindingMode: WaitForFirstConsumer parameters: backendType: ontap-nas fsType: ext4 allowVolumeExpansion: True reclaimPolicy: Delete   This is the Trident backend configuration for the backend integration with FSx for NetApp ONTAP:  apiVersion: trident.netapp.io/v1 kind: TridentBackendConfig metadata: name: svm1-nas namespace: kube-system spec: version: 1 backendName: svm1-nas storageDriverName: ontap-nas managementLIF: ${SVM_MGMT_DNS} svm: svm1 aws: fsxFilesystemID: ${FSXN_ID} apiRegion: ${AWS_REGION} credentials: name: ${SVM_ADMIN_CREDS_SECRET_ARN} type: awsarn   This is an example of a PVC that is handled by Trident and is allowed to be shared across namespaces:  kind: PersistentVolumeClaim apiVersion: v1 metadata: name: some-model namespace: kube-system annotations: trident.netapp.io/shareToNamespace: apps spec: storageClassName: ontap-nas accessModes: - ReadWriteMany resources: requests: storage: 20Gi   We can the use a Job that mounts the PVC and populates the FSx for NetApp ONTAP volume behind its dynamic PV with data from S3:  apiVersion: batch/v1 kind: Job metadata: name: fsxn-loader-some-model namespace: kube-system labels: job-type: fsxn-loader pvc: some-model spec: backoffLimit: 1 ttlSecondsAfterFinished: 30 template: metadata: labels: job-type: fsxn-loader pvc: some-model spec: restartPolicy: Never containers: - name: loader image: peakcom/s5cmd command: - /bin/sh - -c - '/s5cmd sync --delete s3://${MODELS_BUCKET}/${MODELS_FOLDER}/some-model/* /model' resources: ... volumeMounts: - name: model mountPath: /model volumes: - name: model persistentVolumeClaim: claimName: some-model   Now the PVC exists in the cluster and can be consumed, via Trident PVC sharing defined by the TridentVolumeReference CR, by the application in the apps namespace:  apiVersion: trident.netapp.io/v1 kind: TridentVolumeReference metadata: name: some-model namespace:apps spec: pvcName: some-model pvcNamespace: kube-system --- kind: PersistentVolumeClaim apiVersion: v1 metadata: name: some-model namespace: apps annotations: trident.netapp.io/shareFromPVC: kube-system/some-model spec: storageClassName: ontap-nas accessModes: - ReadOnlyMany resources: requests: storage: 20Gi --- apiVersion: apps/v1 kind: Deployment metadata: name: some-app namespace: apps spec: selector: matchLabels: app.kubernetes.io/name: some-app replicas: 1 template: metadata: labels: app.kubernetes.io/name: some-app spec: containers: - name: app image: some-app:1.2.3 ... volumeMounts: - name: data-volume mountPath: /app/models/some-model volumes: - name: data-volume persistentVolumeClaim: claimName: some-model   Main benefits  This solution implements the high-level approach of improving AI/ML inference application containers startup performance by reducing the container image size. It improves the time it takes for the model artifacts to become available for the application (since it no longer needs to download them).  Additional benefits  In addition to the main benefit, the solution introduces the following potential additional benefits:  Ability to update model versions separately from the application, without rebuilding the container imageAbility to A/B test, hot-swap or rollback models, without rebuilding or increasing the container image sizeAbility to share models between different applications without packaging them into all container imagesKubernetes-driven provision and access control via Trident clone, share and snapshot-related features, which reduce the need to copy models, create new operationAbility to have a POSIX-based access control to modelsFaster container builds, aiding experimentation and testingMinimal changes in the ML or application teams workflow  Trade-offs  The trade-offs include:  Additional operational complexity of making the required CI/CD changes and maintaining the loader processAdditional software (Trident) to operate and maintainThe need to implement a custom S3/FSx for NetApp ONTAP TTL/retention-related mechanism to reduce storage costRead performance for the model artifacts needs to be measured against container image download timeMore complex cross-region replication  Variants and hybrid solutions  This solution integrates well with the Accelerating pull process group of solutions, improving both the container image size and its pull time.  For AI/ML inference application that read the artifacts into memory this also allows to skip the intermediate step of storing them on the local disk, similar to the ‚ÄúUsing Mountpoint CSI Driver to read model artifacts into memory directly from Amazon S3‚Äù solution, and can be used instead (or in addition) to improve the download performance further.  ","version":"Next","tagName":"h3"},{"title":"Baking model artifacts into a custom Amazon AMI‚Äã","type":1,"pageTitle":"Decoupling model artifacts from container image","url":"/ai-on-eks/docs/guidance/container-startup-time/reduce-container-image-size/decoupling-model-artifacts#baking-model-artifacts-into-a-custom-amazon-ami","content":" Architecture overview    Figure 3: Baking model artifacts into a custom Amazon AMI  This solution may be suitable in environments with very infrequently changing models that are extremely sensitive to startup time latency and with limited network connectivity.  Implementation guide  Following the architecture diagram above, these are the main high-level steps for each of the teams.  The DevOps/MLOps/Platform team:  creates the EC2 Image Builder recipe ro Packer template and pushes it to Gitupdates the CI/CD process to include AMI backing stepscreates the corresponding Karpenter node pools that use the AMIalters the application manifest to include node selector for a node pool to be provided as a parameter  ML or application teams:  upload model artifacts to an Amazon S3 bucket (as part of their SDLC) or Git LFScontinue to push their container image changes to Amazon ECR, as beforeprovides the appropriate model-specific node pool labels as parameters to the Kubernetes manifest deployment  Main benefits  The solution provides the following main benefits:  no download latency as the models are available immediately upon container startno network dependency  Additional benefits  In addition to the main benefit, the solution introduces the following potential additional benefits:  no changes to Kubernetes artifactsstreamlined rollout for new model versions via Karpenter drift detectionno need to depend on additional services like S3 of FSx  Trade-offs  The trade-offs include:  Significant additional operational complexity of integrating Image Builder or Packer into the CI/CD processSlower build times and longer feedback loop for experimentation, debugging and testing due a more complex setup that requires access to the AMI on a running instanceStorage cost if co-locating all models or extreme cluster segmentation with a AMI-per-model approach, since it requires to manage scheduling of the applications to their corresponding AMIs.  Variants and hybrid solutions  This solution integrates well with the Accelerating pull process group of solutions, improving both the container image size and its pull time.  Note that this can introduce regression in accuracy or compatibility issues with serving frameworks. ","version":"Next","tagName":"h2"},{"title":"EMR on EKS NVIDIA RAPIDS Accelerator for Apache Spark","type":0,"sectionRef":"#","url":"/ai-on-eks/docs/infra/ai-ml/emr-spark-rapids","content":"","keywords":"","version":"Next"},{"title":"EMR support for NVIDIA RAPIDS Accelerator for Apache Spark‚Äã","type":1,"pageTitle":"EMR on EKS NVIDIA RAPIDS Accelerator for Apache Spark","url":"/ai-on-eks/docs/infra/ai-ml/emr-spark-rapids#emr-support-for-nvidia-rapids-accelerator-for-apache-spark","content":" Integration of Amazon EMR with NVIDIA RAPIDS Accelerator for Apache Spark‚Äã Amazon EMR on EKS now extends its support to include the use of GPU instance types with the NVIDIA RAPIDS Accelerator for Apache Spark. As the use of artificial intelligence (AI) and machine learning (ML) continues to expand in the realm of data analytics, there's an increasing demand for rapid and cost-efficient data processing, which GPUs can provide. The NVIDIA RAPIDS Accelerator for Apache Spark enables users to harness the superior performance of GPUs, leading to substantial infrastructure cost savings.  ","version":"Next","tagName":"h3"},{"title":"Features‚Äã","type":1,"pageTitle":"EMR on EKS NVIDIA RAPIDS Accelerator for Apache Spark","url":"/ai-on-eks/docs/infra/ai-ml/emr-spark-rapids#features","content":" Highlighted Features‚Äã Experience a performance boost in data preparation tasks, allowing you to transition quickly to the subsequent stages of your pipeline. This not only accelerates model training but also liberates data scientists and engineers to concentrate on priority tasks. Spark 3 ensures seamless coordination of end-to-end pipelines - from data ingestion, through model training, to visualization. The same GPU-accelerated setup can serve both Spark and machine learning or deep learning frameworks. This obviates the need for discrete clusters and provides GPU acceleration to the entire pipeline. Spark 3 extends support for columnar processing in the Catalyst query optimizer. The RAPIDS Accelerator can plug into this system to speed up SQL and DataFrame operators. When the query plan is actioned, these operators can then utilize the GPUs within the Spark cluster for improved performance. NVIDIA has introduced an innovative Spark shuffle implementation designed to optimize data exchange between Spark tasks. This shuffle system is built on GPU-boosted communication libraries, including UCX, RDMA, and NCCL, which significantly enhance data transfer rates and overall performance.-  Deploying the Solution üëà  ","version":"Next","tagName":"h3"},{"title":"Launching XGBoost Spark Job‚Äã","type":1,"pageTitle":"EMR on EKS NVIDIA RAPIDS Accelerator for Apache Spark","url":"/ai-on-eks/docs/infra/ai-ml/emr-spark-rapids#launching-xgboost-spark-job","content":" Training Dataset‚Äã  Fannie Mae‚Äôs Single-Family Loan Performance Data has a comprehensive dataset starting from 2013. It provides valuable insights into the credit performance of a portion of Fannie Mae‚Äôs single-family book of business. This dataset is designed to assist investors in better understanding the credit performance of single-family loans owned or guaranteed by Fannie Mae.  Step 1: Building a Custom Docker Image‚Äã  To pull the Spark Rapids base image from the EMR on EKS ECR repository located in us-west-2, log in:  aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin 895885662937.dkr.ecr.us-west-2.amazonaws.com   If you're located in a different region, please refer to: this guide.  To build your Docker image locally, use the following command:  Build the custom Docker image using the provided Dockerfile. Choose a tag for the image, such as 0.10.  info Please note that the build process may take some time, depending on your network speed. Keep in mind that the resulting image size will be approximately 23.5GB.  cd infra/emr-spark-rapids/examples/xgboost docker build -t emr-6.10.0-spark-rapids-custom:0.10 -f Dockerfile .   Replace &lt;ACCOUNTID&gt; with your AWS account ID. Log in to your ECR repository with the following command:  aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin &lt;ACCOUNTID&gt;.dkr.ecr.us-west-2.amazonaws.com   To push your Docker image to your ECR, use:  $ docker tag emr-6.10.0-spark-rapids-custom:0.10 &lt;ACCOUNT_ID&gt;.dkr.ecr.us-west-2.amazonaws.com/emr-6.10.0-spark-rapids-custom:0.10 $ docker push &lt;ACCOUNT_ID&gt;.dkr.ecr.us-west-2.amazonaws.com/emr-6.10.0-spark-rapids-custom:0.10   You can use this image during the job execution in Step3 .  ","version":"Next","tagName":"h3"},{"title":"Step2: Acquire the Input Data (Fannie Mae‚Äôs Single-Family Loan Performance Data)‚Äã","type":1,"pageTitle":"EMR on EKS NVIDIA RAPIDS Accelerator for Apache Spark","url":"/ai-on-eks/docs/infra/ai-ml/emr-spark-rapids#step2-acquire-the-input-data-fannie-maes-single-family-loan-performance-data","content":" This dataset is sourced from Fannie Mae‚Äôs Single-Family Loan Performance Data. All rights are held by Fannie Mae.  Go to the Fannie Mae websiteClick on Single-Family Loan Performance Data Register as a new user if you are using the website for the first timeUse the credentials to login Select HPClick on Download Data and choose Single-Family Loan Performance DataYou will find a tabular list of Acquisition and Performance` files sorted based on year and quarter. Click on the file to download. You can download three years(2020, 2021 and 2022 - 4 files for each year and one for each quarter) worth of data that will be used in our example job. e.g.,: 2017Q1.zipUnzip the download file to extract the csv file to your local machine. e.g.,: 2017Q1.csvCopy only the CSV files to an S3 bucket under ${S3_BUCKET}/${EMR_VIRTUAL_CLUSTER_ID}/spark-rapids-emr/input/fannie-mae-single-family-loan-performance/. The example below uses three years of data (one file for each quarter, 12 files in total). Note: ${S3_BUCKET} and ${EMR_VIRTUAL_CLUSTER_ID} values can be extracted from Terraform outputs.   aws s3 ls s3://emr-spark-rapids-&lt;aws-account-id&gt;-us-west-2/949wt7zuphox1beiv0i30v65i/spark-rapids-emr/input/fannie-mae-single-family-loan-performance/ 2023-06-24 21:38:25 2301641519 2000Q1.csv 2023-06-24 21:38:25 9739847213 2020Q2.csv 2023-06-24 21:38:25 10985541111 2020Q3.csv 2023-06-24 21:38:25 11372073671 2020Q4.csv 2023-06-23 16:38:36 9603950656 2021Q1.csv 2023-06-23 16:38:36 7955614945 2021Q2.csv 2023-06-23 16:38:36 5365827884 2021Q3.csv 2023-06-23 16:38:36 4390166275 2021Q4.csv 2023-06-22 19:20:08 2723499898 2022Q1.csv 2023-06-22 19:20:08 1426204690 2022Q2.csv 2023-06-22 19:20:08 595639825 2022Q3.csv 2023-06-22 19:20:08 180159771 2022Q4.csv   ","version":"Next","tagName":"h3"},{"title":"Step3: Run the EMR Spark XGBoost Job‚Äã","type":1,"pageTitle":"EMR on EKS NVIDIA RAPIDS Accelerator for Apache Spark","url":"/ai-on-eks/docs/infra/ai-ml/emr-spark-rapids#step3-run-the-emr-spark-xgboost-job","content":" Here, we will utilize a helper shell script to execute the job. This script requires user input.  This script will ask for certain inputs that you can obtain from Terraform outputs. See the example below.  cd infra/emr-spark-rapids/examples/xgboost chmod +x execute_spark_rapids_xgboost.sh ./execute_spark_rapids_xgboost.sh # Example inputs shown below Did you copy the fannie-mae-single-family-loan-performance data to S3 bucket(y/n): y Enter the customized Docker image URI: public.ecr.aws/o7d8v7g9/emr-6.10.0-spark-rapids:0.11 Enter EMR Virtual Cluster AWS Region: us-west-2 Enter the EMR Virtual Cluster ID: 949wt7zuphox1beiv0i30v65i Enter the EMR Execution Role ARN: arn:aws:iam::&lt;ACCOUNTID&gt;:role/emr-spark-rapids-emr-eks-data-team-a Enter the CloudWatch Log Group name: /emr-on-eks-logs/emr-spark-rapids/emr-ml-team-a Enter the S3 Bucket for storing PySpark Scripts, Pod Templates, Input data and Output data.&lt;bucket-name&gt;: emr-spark-rapids-&lt;ACCOUNTID&gt;-us-west-2 Enter the number of executor instances (4 to 8): 8   Verify the pod status    info Note that the first execution might take longer as it needs to download the image for the EMR Job Pod, Driver, and Executor pods. Each pod may take up to 8 minutes to download the Docker image. Subsequent runs should be faster (usually under 30 seconds), thanks to image caching  ","version":"Next","tagName":"h3"},{"title":"Step4: Verify the Job results‚Äã","type":1,"pageTitle":"EMR on EKS NVIDIA RAPIDS Accelerator for Apache Spark","url":"/ai-on-eks/docs/infra/ai-ml/emr-spark-rapids#step4-verify-the-job-results","content":" Log in to check the Spark driver pod logs from either CloudWatch logs or your S3 bucket.  Here's a sample output from the log file:  /emr-on-eks-logs/emr-spark-rapids/emr-ml-team-a spark-rapids-emr/949wt7zuphox1beiv0i30v65i/jobs/0000000327fe50tosa4/containers/spark-0000000327fe50tosa4/spark-0000000327fe50tosa4-driver/stdout   The following is a sample output from the above log file:  Raw Dataframe CSV Rows count : 215386024 Raw Dataframe Parquet Rows count : 215386024 ETL takes 222.34674382209778  Training takes 95.90932035446167 seconds If features_cols param set, then features_col param is ignored.  Transformation takes 63.999391317367554 seconds +--------------+--------------------+--------------------+----------+ |delinquency_12| rawPrediction| probability|prediction| +--------------+--------------------+--------------------+----------+ | 0|[10.4500541687011...|[0.99997103214263...| 0.0| | 0|[10.3076572418212...|[0.99996662139892...| 0.0| | 0|[9.81707763671875...|[0.99994546175003...| 0.0| | 0|[9.10498714447021...|[0.99988889694213...| 0.0| | 0|[8.81903457641601...|[0.99985212087631...| 0.0| +--------------+--------------------+--------------------+----------+ only showing top 5 rows  Evaluation takes 3.8372223377227783 seconds Accuracy is 0.996563056111921  ","version":"Next","tagName":"h3"},{"title":"ML Pipeline for Fannie Mae Single Loan Performance Dataset‚Äã","type":1,"pageTitle":"EMR on EKS NVIDIA RAPIDS Accelerator for Apache Spark","url":"/ai-on-eks/docs/infra/ai-ml/emr-spark-rapids#ml-pipeline-for-fannie-mae-single-loan-performance-dataset","content":" Step1: Preprocess and clean the dataset to handle missing values, categorical variables, and other data inconsistencies.This may involve techniques like data imputation,one-hot encoding, and data normalization.  Step2: Create additional features from the existing ones that might provide more useful information for predicting loan performance. For example, you could extract features like loan-to-value ratio, borrower's credit score range, or loan origination year.  Step3: Divide the dataset into two parts: one for training the XGBoost model and one for evaluating its performance. This allows you to assess how well the model generalizes to unseen data.  Step4: Feed the training dataset into XGBoost to train the model. XGBoost will analyze the loan attributes and their corresponding loan performance labels to learn the patterns and relationships between them. The objective is to predict whether a loan is likely to default or perform well based on the given features.  Step5: Once the model is trained, use the evaluation dataset to assess its performance. This involves analyzing metrics such as accuracy, precision, recall, or area under the receiver operating characteristic curve (AUC-ROC) to measure how well the model predicts loan performance  Step6: If the performance is not satisfactory, you can tune the XGBoost hyperparameters, such as the learning rate, tree depth, or regularization parameters, to improve the model's accuracy or address issues like overfitting.  Step7: Finally, with a trained and validated XGBoost model, you can use it to make predictions on new, unseen loan data. These predictions can help in identifying potential risks associated with loan default or evaluating loan performance.    ","version":"Next","tagName":"h3"},{"title":"GPU Monitoring with DCGM Exporter, Prometheus and Grafana‚Äã","type":1,"pageTitle":"EMR on EKS NVIDIA RAPIDS Accelerator for Apache Spark","url":"/ai-on-eks/docs/infra/ai-ml/emr-spark-rapids#gpu-monitoring-with-dcgm-exporter-prometheus-and-grafana","content":" Observability plays a crucial role in managing and optimizing hardware resources such as GPUs, particularly in machine learning workloads where the GPU utilization is high. The ability to monitor GPU usage in real-time, identify trends, and detect anomalies can significantly impact performance tuning, troubleshooting, and efficient resource utilization.  NVIDIA GPU Operator plays a key role in GPU observability. It automates the deployment of the necessary components to run GPU workloads on Kubernetes. One of its components, the DCGM (Data Center GPU Manager) Exporter, is an open-source project that exports GPU metrics in a format that can be ingested by Prometheus, a leading open-source monitoring solution. These metrics include GPU temperature, memory usage, GPU utilization, and more. The DCGM Exporter allows you to monitor these metrics on a per-GPU basis, providing granular visibility into your GPU resources.  The NVIDIA GPU Operator, in combination with the DCGM Exporter, exports GPU metrics to a Prometheus server. With its flexible query language, Prometheus allows you to slice and dice data to generate insights into resource usage patterns.  However, Prometheus is not designed for long-term data storage. This is where the Amazon Managed Service for Prometheus (AMP) comes into play. It provides a fully-managed, secure, and scalable service for Prometheus that makes it easy to analyze operational data at scale without having to manage the underlying infrastructure.  Visualizing these metrics and creating informative dashboards is where Grafana excels. Grafana is an open-source platform for monitoring and observability, offering rich visualizations to represent collected metrics intuitively. When combined with Prometheus, Grafana can display the GPU metrics collected by the DCGM Exporter in a user-friendly manner.  The NVIDIA GPU Operator is configured to export metrics to the Prometheus server, which then remote-writes these metrics to Amazon Managed Prometheus (AMP). As a user, you can log into the Grafana WebUI, deployed as part of the blueprint, and add AMP as a data source. Following this, you can import the open-source GPU monitoring dashboard that presents GPU metrics in an easily digestible format, facilitating real-time performance monitoring and resource optimization.  NVIDIA GPU Operator: Installed on your Kubernetes cluster, the NVIDIA GPU Operator is responsible for managing the lifecycle of GPU resources. It deploys NVIDIA drivers and the DCGM Exporter on each GPU-equipped node.DCGM Exporter: The DCGM Exporter runs on each node, collecting GPU metrics and exposing them to Prometheus.Prometheus: Prometheus is a time-series database that collects metrics from various sources, including the DCGM Exporter. It pulls metrics from the exporter at regular intervals and stores them. In this setup, you would configure Prometheus to remote-write the collected metrics to AMP.Amazon Managed Service for Prometheus (AMP): AMP is a fully-managed Prometheus service provided by AWS. It takes care of long-term storage, scalability, and security of your Prometheus data.Grafana: Grafana is a visualization tool that can query AMP for the collected metrics, and display them on informative dashboards.  In this blueprint, we leverage DCGM to write GPU metrics to both Prometheus and Amazon Managed Prometheus (AMP). To verify the GPU metrics, you can use Grafana by running the following command:  kubectl port-forward svc/grafana 3000:80 -n grafana `` Login to Grafana using `admin` as the username, and retrieve the password from Secrets Manager using the following AWS CLI command: ```bash aws secretsmanager get-secret-value --secret-id emr-spark-rapids-grafana --region us-west-2   Once logged in, add the AMP datasource to Grafana and import the Open Source GPU monitoring dashboard. You can then explore the metrics and visualize them using the Grafana dashboard, as shown in the screenshot below.    Cleanup üëà  caution To avoid unwanted charges to your AWS account, delete all the AWS resources created during this deployment ","version":"Next","tagName":"h3"},{"title":"JupyterHub on EKS","type":0,"sectionRef":"#","url":"/ai-on-eks/docs/infra/ai-ml/jupyterhub","content":"","keywords":"","version":"Next"},{"title":"JupyterHub on EKS‚Äã","type":1,"pageTitle":"JupyterHub on EKS","url":"/ai-on-eks/docs/infra/ai-ml/jupyterhub#jupyterhub-on-eks-1","content":" Deploying JupyterHub on Amazon Elastic Kubernetes Service (EKS) combines the versatility of JupyterHub with the scalability and flexibility of Kubernetes. This blueprint enables users to build a multi-tenant JupyterHub platform on EKS with the help of JupyterHub profiles. By leveraging EFS shared filesystems for each user, it facilitates easy sharing of notebooks and provides individual EFS storage so that user pods can securely store data, even if the user pod is deleted or expired. When users log in, they can access all their scripts and data under the existing EFS volume.  By leveraging the capabilities of EKS, you can seamlessly scale your JupyterHub environment to meet the needs of your users, ensuring efficient resource utilization and optimal performance. With EKS, you can take advantage of Kubernetes features such as automated scaling, high availability, and easy deployment of updates and upgrades. This enables you to provide a reliable and robust JupyterHub experience for your users, empowering them to collaborate, explore, and analyze data effectively.  To get started with JupyterHub on EKS, follow the instructions in this guide to set up and configure your JupyterHub environment.  Deploying the Solution üëà  Verify the resources üëà  ","version":"Next","tagName":"h2"},{"title":"Type1 deployment: Login to JupyterHub‚Äã","type":1,"pageTitle":"JupyterHub on EKS","url":"/ai-on-eks/docs/infra/ai-ml/jupyterhub#type1-deployment-login-to-jupyterhub","content":" Exposing JupyterHub with port-forward:  Execute the command below to make the JupyterHub service accessible for viewing the Web User Interface locally. It‚Äôs important to note that our current dummy deployment only establishes a Web UI service with a ClusterIP. Should you wish to customize this to an internal or internet-facing load balancer, you can make the necessary adjustments in the JupyterHub Helm chart values file.  kubectl port-forward svc/proxy-public 8080:80 -n jupyterhub   Sign-in: Navigate to http://localhost:8080/ in your web browser. Input user-1 as the username and choose any password.  Select server options: Upon sign-in, you‚Äôll be presented with a variety of Notebook instance profiles to choose from. The Data Engineering (CPU) server is for traditional, CPU based notebook work. The Elyra server provides Elyra functionality, allowing you to quickly develop pipelines: . Trainium and Inferentia servers will deploy the notebook server onto Trainium and Inferentia nodes, allowing accelerated workloads. Time Slicing and MIG are two different strategies for GPU sharing. Finally, the Data Science (GPU) server is a traditional server running on an NVIDIA GPU.  For this time-slicing feature demonstration, we‚Äôll be using the Data Science (GPU + Time-Slicing ‚Äì G5) profile. Go ahead and select this option and choose the Start button.    The new node created by Karpenter with the g5.2xlarge instance type has been configured to leverage the timeslicing feature provided by the NVIDIA device plugin. This feature allows for efficient GPU utilization by dividing a single GPU into multiple allocatable units. In this case, we have defined 4 allocatable GPUs in the NVIDIA device plugin Helm chart config map. Below is the status of the node:  GPUs: The node is configured with 4 GPUs through the NVIDIA device plugin's timeslicing feature. This allows the node to allocate GPU resources more flexibly to different workloads.  status: capacity: cpu: '8' # The node has 8 CPUs available ephemeral-storage: 439107072Ki # The node has a total ephemeral storage capacity of 439107072 KiB hugepages-1Gi: '0' # The node has 0 1Gi hugepages hugepages-2Mi: '0' # The node has 0 2Mi hugepages memory: 32499160Ki # The node has a total memory capacity of 32499160 KiB nvidia.com/gpu: '4' # The node has a total of 4 GPUs, configured through timeslicing pods: '58' # The node can accommodate up to 58 pods allocatable: cpu: 7910m # 7910 millicores of CPU are allocatable ephemeral-storage: '403607335062' # 403607335062 KiB of ephemeral storage is allocatable hugepages-1Gi: '0' # 0 1Gi hugepages are allocatable hugepages-2Mi: '0' # 0 2Mi hugepages are allocatable memory: 31482328Ki # 31482328 KiB of memory is allocatable nvidia.com/gpu: '4' # 4 GPUs are allocatable pods: '58' # 58 pods are allocatable   Setting up second user (user-2) environment:  To demonstrate GPU time-slicing in action, we‚Äôll provision another Jupyter Notebook instance. This time, we‚Äôll validate that the second user‚Äôs pod is scheduled on the same node as the first user‚Äôs, taking advantage of the GPU time-slicing configuration we set up earlier. Follow the steps below to achieve this:  Open JupyterHub in an Incognito browser window: Navigate to http://localhost:8080/ in the new incognito window in web browser. Input user-2 as the username and choose any password.  Choose server options: After logging in, you‚Äôll see the server options page. Ensure that you select the Data Science (GPU + Time-Slicing ‚Äì G5) radio button and select Start.    Verify pod placement: Notice that this pod placement takes only few seconds unlike the user-1. It‚Äôs because the Kubernetes scheduler is able to place the pod on the existing g5.2xlarge node created by the user-1 pod. user-2 is also using the same docker image so there is no delay in pulling the docker image and it leveraged local cache.  Open a terminal and execute the following command to check where the new Jupyter Notebook pod has been scheduled:  kubectl get pods -n jupyterhub -owide | grep -i user   Observe that both the user-1 and user-2 pods are running on the same node. This confirms that our GPU time-slicing configuration is functioning as expected.  info Checkout the AWS blog: Building multi-tenant JupyterHub Platforms on Amazon EKS for more details  ","version":"Next","tagName":"h3"},{"title":"Type2 deployment(Optional): Login into JupyterHub via Amazon Cognito‚Äã","type":1,"pageTitle":"JupyterHub on EKS","url":"/ai-on-eks/docs/infra/ai-ml/jupyterhub#type2-deploymentoptional-login-into-jupyterhub-via-amazon-cognito","content":" Add the CNAME DNS record in ChangeIP for the JupyterHub domain with the load balancer DNS name.    info When adding the load balancer DNS name in the value field of CNAME in ChangeIP make sure to add a dot(.) at the end of the load-balancer DNS name.  Now typing the domain url in the browser should redirect to the Jupyterhub login page.    Follow the Cognito sign-up and sign-in process to login.    Successful sign-in will open up the JupyterHub environment for the logged in user.    To test the setup of the shared and personal directories in JupyterHub, you can follow these steps:  Open a terminal window from the launcher dashboard.    execute the command  df -h   Verify EFS mounts created. Each user's private home directory is available at /home/jovyan. The shared directory is available at /home/shared  ","version":"Next","tagName":"h3"},{"title":"Type3 deployment(Optional): Login into JupyterHub via OAuth (Keycloak)‚Äã","type":1,"pageTitle":"JupyterHub on EKS","url":"/ai-on-eks/docs/infra/ai-ml/jupyterhub#type3-deploymentoptional-login-into-jupyterhub-via-oauth-keycloak","content":" Note: This will look a little different depending on your OAuth provider.  Add the CNAME DNS record in ChangeIP for the JupyterHub domain with the load balancer DNS name.    info When adding the load balancer DNS name in the value field of CNAME in ChangeIP make sure to add a dot(.) at the end of the load-balancer DNS name.  Now typing the domain url in the browser should redirect to the Jupyterhub login page.    Follow the Keycloak sign-up and sign-in process to login.    Successful sign-in will open up the JupyterHub environment for the logged in user.    Cleanup üëà ","version":"Next","tagName":"h3"},{"title":"AWS Trainium on EKS","type":0,"sectionRef":"#","url":"/ai-on-eks/docs/infra/ai-ml/trainium","content":"","keywords":"","version":"Next"},{"title":"Solution Architecture‚Äã","type":1,"pageTitle":"AWS Trainium on EKS","url":"/ai-on-eks/docs/infra/ai-ml/trainium#solution-architecture","content":"   Deploying the Solution üëà  ","version":"Next","tagName":"h3"},{"title":"Observability with AWS CloudWatch and Neuron Monitor‚Äã","type":1,"pageTitle":"AWS Trainium on EKS","url":"/ai-on-eks/docs/infra/ai-ml/trainium#observability-with-aws-cloudwatch-and-neuron-monitor","content":" This blueprint deploys the CloudWatch Observability Agent as a managed add-on, providing comprehensive monitoring for containerized workloads. It includes container insights for tracking key performance metrics such as CPU and memory utilization. Additionally, the blueprint integrates GPU metrics using NVIDIA's DCGM plugin, which is essential for monitoring high-performance GPU workloads. For machine learning models running on AWS Inferentia or Trainium, the Neuron Monitor plugin is added to capture and report Neuron-specific metrics.  All metrics, including container insights, GPU performance, and Neuron metrics, are sent to Amazon CloudWatch, where you can monitor and analyze them in real-time. After the deployment is complete, you should be able to access these metrics directly from the CloudWatch console, allowing you to manage and optimize your workloads effectively.  ","version":"Next","tagName":"h3"},{"title":"Distributed PyTorch Training on Trainium with TorchX and EKS‚Äã","type":1,"pageTitle":"AWS Trainium on EKS","url":"/ai-on-eks/docs/infra/ai-ml/trainium#distributed-pytorch-training-on-trainium-with-torchx-and-eks","content":" In this example, we will perform DataParallel-based phase1 pretraining on the BERT-large model using the WikiCorpus dataset. To execute the task, we will use TorchX to initiate the job on two trn1.32xlarge instances, with 32 workers per instance. You can also run the same job on trn1n.32xlarge node group.  We have created three Shell scripts to automate the job execution as much as possible.  Step1: Create a Docker image for PyTorch Neuron container for BERT-large model pre-training‚Äã  This step creates a new Docker image and push this image to ECR repo. The Dockerfile handles the installation of necessary software packages, such as AWS Neuron repos, Python dependencies, and other essential tools for PyTorch and BERT pre-training. It configures various environment variables to ensure smooth execution and optimal performance. The image contains crucial components like a BERT pretraining script and requirements.txt file sourced from GitHub, both vital for the BERT pretraining process. Furthermore, it includes a basic environment test script for validation purposes. Together, this Docker image provides a comprehensive environment for efficient BERT pre-training with PyTorch while incorporating AWS Neuron optimizations.  caution This step generates an AMD64 (x86-64) Docker image with a size of 7GB or more. Therefore, it is strongly advised to utilize an AWS Cloud9/EC2 AMD64 (x86-64) instance with Docker client installed, ensuring sufficient storage capacity for this process.  caution If you are executing this script on a Cloud9 IDE/EC2 instance different from the one where the EKS Cluster is deployed, it is essential to ensure that the same IAM role is used or attached to the Cloud9 IDE/EC2 instance. Should you prefer a distinct IAM role for Cloud9 IDE/EC2, it must be added to the EKS Cluster's aws-auth config map to grant the role authorization for authenticating with the EKS Cluster. Taking these precautions will enable smooth communication between the instances and the EKS Cluster, ensuring the script functions as intended.  cd infra/trainium-inferentia/examples/dp-bert-large-pretrain chmod +x 1-bert-pretrain-build-image.sh ./1-bert-pretrain-build-image.sh   Admin:~/environment/data-on-eks/infra/trainium-inferentia/examples/dp-bert-large-pretrain (trainium-part2) $ ./1-bert-pretrain-build-image.sh Did you install docker on AMD64(x86-64) machine (y/n): y Enter the ECR region: us-west-2 ECR repository 'eks_torchx_test' already exists. Repository URL: &lt;YOUR_ACCOUNT_ID&gt;.dkr.ecr.us-west-2.amazonaws.com/eks_torchx_test Building and Tagging Docker image... &lt;YOUR_ACCOUNT_ID&gt;.dkr.ecr.us-west-2.amazonaws.com/eks_torchx_test:bert_pretrain [+] Building 2.4s (26/26) FINISHED =&gt; [internal] load build definition from Dockerfile.bert_pretrain 0.0s =&gt; =&gt; transferring dockerfile: 5.15kB 0.0s =&gt; [internal] load .dockerignore 0.0s =&gt; =&gt; transferring context: 2B 0.0s =&gt; [internal] load metadata for docker.io/library/ubuntu:20.04 0.7s =&gt; [ 1/22] FROM docker.io/library/ubuntu:20.04@sha256:c9820a44b950956a790c354700c1166a7ec648bc0d215fa438d3a339812f1d01 0.0s ... bert_pretrain: digest: sha256:1bacd5233d1a87ca1d88273c5a7cb131073c6f390f03198a91dc563158485941 size: 4729   Login to AWS Console and verify the ECR repo(&lt;YOUR_ACCOUNT_ID&gt;.dkr.ecr.&lt;REGION&gt;.amazonaws.com/eks_torchx_test) and the image tag(bert_pretrain) in ECR.  Step2: Copy WikiCorpus pre-training dataset for BERT model to FSx for Lustre filesystem‚Äã  In this step, we make it easy to transfer the WikiCorpus pre-training dataset, which is crucial for training the BERT model in distributed mode by multiple Trainium instances, to the FSx for Lustre filesystem. To achieve this, we will login to cmd-shell pod which includes an AWS CLI container, providing access to the filesystem.  Once you're inside the container, Copy the WikiCorpus dataset from S3 bucket (s3://neuron-s3/training_datasets/bert_pretrain_wikicorpus_tokenized_hdf5/bert_pretrain_wikicorpus_tokenized_hdf5_seqlen128.tar). The dataset is then unpacked, giving you access to its contents, ready for use in the subsequent BERT model pre-training process.  kubectl exec -i -t -n default cmd-shell -c app -- sh -c &quot;clear; (bash || ash || sh)&quot; # Once logged into the container yum install tar cd /data aws s3 cp s3://neuron-s3/training_datasets/bert_pretrain_wikicorpus_tokenized_hdf5/bert_pretrain_wikicorpus_tokenized_hdf5_seqlen128.tar . --no-sign-request chmod 744 bert_pretrain_wikicorpus_tokenized_hdf5_seqlen128.tar tar xvf bert_pretrain_wikicorpus_tokenized_hdf5_seqlen128.tar   Step3: Precompile the BERT graphs using neuron_parallel_compile‚Äã  PyTorch Neuron introduces a valuable tool known as neuron_parallel_compile, which significantly reduces graph compilation time by extracting model graphs and compiling them in parallel. This optimization technique expedites the process and results in faster model compilation. The compiled graphs are then stored on the Fsx for Lustre shared storage volume, accessible by worker nodes during model training. This efficient approach streamlines the training process and improves overall performance, making the most of PyTorch Neuron's capabilities.  Execute the following commands.This script prompts the user to configure their kubeconfig and verifies the presence of the lib folder with trn1_dist_ddp.py. It sets up Docker credentials, installs the TorchX client for Kubernetes. Using TorchX, the script runs a Kubernetes job to compile BERT graphs with optimized performance. Additionally, TorchX creates another Docker image and pushes it to the ECR repository within the same repo. This image is utilized in the subsequent pre-compiling pods, optimizing the overall BERT model training process.  cd infra/trainium-inferentia/examples/dp-bert-large-pretrain chmod +x 2-bert-pretrain-precompile.sh ./2-bert-pretrain-precompile.sh   You can verify the pods status by running kubectl get pods or kubectl get vcjob. Successful output looks like below.    You can also verify the logs for the Pod once they are Succeeded. The precompilation job will run for ~15 minutes. Once complete, you will see the following in the output:  2023-07-29 09:42:42.000310: INFO ||PARALLEL_COMPILE||: Starting parallel compilations of the extracted graphs2023-07-29 09:42:42.000312: INFO ||PARALLEL_COMPILE||: Compiling /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.406_16969875447143373016.hlo.pb using following command: neuronx-cc compile ‚Äîtarget=trn1 ‚Äîframework XLA /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.406_16969875447143373016.hlo.pb ‚Äîmodel-type=transformer ‚Äîverbose=35 ‚Äîoutput /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.406_16969875447143373016.neff 2023-07-29 09:42:42.000313: INFO ||PARALLEL_COMPILE||: Compiling /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.22250_9219523464496887986.hlo.pb using following command: neuronx-cc compile ‚Äîtarget=trn1 ‚Äîframework XLA /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.22250_9219523464496887986.hlo.pb ‚Äîmodel-type=transformer ‚Äîverbose=35 ‚Äîoutput /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.22250_9219523464496887986.neff 2023-07-29 09:42:42.000314: INFO ||PARALLEL_COMPILE||: Compiling /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.25434_3000743782456078279.hlo.pb using following command: neuronx-cc compile ‚Äîtarget=trn1 ‚Äîframework XLA /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.25434_3000743782456078279.hlo.pb ‚Äîmodel-type=transformer ‚Äîverbose=35 ‚Äîoutput /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.25434_3000743782456078279.neff 2023-07-29 09:42:42.000315: INFO ||PARALLEL_COMPILE||: Compiling /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.25637_13822314547392343350.hlo.pb using following command: neuronx-cc compile ‚Äîtarget=trn1 ‚Äîframework XLA /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.25637_13822314547392343350.hlo.pb ‚Äîmodel-type=transformer ‚Äîverbose=35 ‚Äîoutput /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.25637_13822314547392343350.neff 2023-07-29 09:42:42.000316: INFO ||PARALLEL_COMPILE||: Compiling /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.21907_15179678551789598088.hlo.pb using following command: neuronx-cc compile ‚Äîtarget=trn1 ‚Äîframework XLA /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.21907_15179678551789598088.hlo.pb ‚Äîmodel-type=transformer ‚Äîverbose=35 ‚Äîoutput /tmp/parallel_compile_workdir/MODULE_SyncTensorsGraph.21907_15179678551789598088.neff ..... Compiler status PASS   New pre-training cache files are stored under FSx for Lustre.    Step4: Launch BERT pretraining job using 64 Neuron cores with two trn1.32xlarge instances‚Äã  We are now in the final step of training the BERT-large model with WikiCorpus data.  cd infra/trainium-inferentia/examples/dp-bert-large-pretrain chmod +x 3-bert-pretrain.sh ./3-bert-pretrain.sh   Monitor the job with the following commands. This job may take several hours as its training 30GB+ worth of the data.  kubectl get vcjob kubectl get pods # Two pods will be running in default namespace   To monitor Neuron usage, you can log in to one of the Trainium EC2 instances using SSM (Systems Manager) from the EC2 console. Once logged in, run the command neuron-ls, and you will receive an output similar to the following.  [root@ip-100-64-229-201 aws-efa-installer]# neuron-ls instance-type: trn1.32xlarge instance-id: i-04b476a6a0e686980 +--------+--------+--------+---------------+---------+--------+------------------------------------------+---------+ | NEURON | NEURON | NEURON | CONNECTED | PCI | PID | COMMAND | RUNTIME | | DEVICE | CORES | MEMORY | DEVICES | BDF | | | VERSION | +--------+--------+--------+---------------+---------+--------+------------------------------------------+---------+ | 0 | 2 | 32 GB | 12, 3, 4, 1 | 10:1c.0 | 109459 | python3 -m torch_neuronx.distributed.... | 2.15.11 | | 1 | 2 | 32 GB | 13, 0, 5, 2 | 10:1d.0 | 109459 | python3 -m torch_neuronx.distributed.... | 2.15.11 | | 2 | 2 | 32 GB | 14, 1, 6, 3 | a0:1c.0 | 109459 | python3 -m torch_neuronx.distributed.... | 2.15.11 | | 3 | 2 | 32 GB | 15, 2, 7, 0 | a0:1d.0 | 109459 | python3 -m torch_neuronx.distributed.... | 2.15.11 | | 4 | 2 | 32 GB | 0, 7, 8, 5 | 20:1b.0 | 109459 | python3 -m torch_neuronx.distributed.... | 2.15.11 | | 5 | 2 | 32 GB | 1, 4, 9, 6 | 20:1c.0 | 109459 | python3 -m torch_neuronx.distributed.... | 2.15.11 | | 6 | 2 | 32 GB | 2, 5, 10, 7 | 90:1b.0 | 109459 | python3 -m torch_neuronx.distributed.... | 2.15.11 | | 7 | 2 | 32 GB | 3, 6, 11, 4 | 90:1c.0 | 109459 | python3 -m torch_neuronx.distributed.... | 2.15.11 | | 8 | 2 | 32 GB | 4, 11, 12, 9 | 20:1d.0 | 109459 | python3 -m torch_neuronx.distributed.... | 2.15.11 | | 9 | 2 | 32 GB | 5, 8, 13, 10 | 20:1e.0 | 109459 | python3 -m torch_neuronx.distributed.... | 2.15.11 | | 10 | 2 | 32 GB | 6, 9, 14, 11 | 90:1d.0 | 109459 | python3 -m torch_neuronx.distributed.... | 2.15.11 | | 11 | 2 | 32 GB | 7, 10, 15, 8 | 90:1e.0 | 109459 | python3 -m torch_neuronx.distributed.... | 2.15.11 | | 12 | 2 | 32 GB | 8, 15, 0, 13 | 10:1e.0 | 109459 | python3 -m torch_neuronx.distributed.... | 2.15.11 | | 13 | 2 | 32 GB | 9, 12, 1, 14 | 10:1b.0 | 109459 | python3 -m torch_neuronx.distributed.... | 2.15.11 | | 14 | 2 | 32 GB | 10, 13, 2, 15 | a0:1e.0 | 109459 | python3 -m torch_neuronx.distributed.... | 2.15.11 | | 15 | 2 | 32 GB | 11, 14, 3, 12 | a0:1b.0 | 109459 | python3 -m torch_neuronx.distributed.... | 2.15.11 | +--------+--------+--------+---------------+---------+--------+------------------------------------------+---------+   You can also run neuron-top which provides the live usage of neuron cores. The below shows the usage of all 32 neuron cores.    If you wish to terminate the job, you can execute the following commands:  kubectl get vcjob # Get a job name kubectl delete &lt;ENTER_JOB_NAME&gt;   Cleanup üëà  caution To avoid unwanted charges to your AWS account, delete all the AWS resources created during this deployment ","version":"Next","tagName":"h3"},{"title":"Troubleshooting","type":0,"sectionRef":"#","url":"/ai-on-eks/docs/infra/troubleshooting","content":"","keywords":"","version":"Next"},{"title":"Error: local-exec provisioner error‚Äã","type":1,"pageTitle":"Troubleshooting","url":"/ai-on-eks/docs/infra/troubleshooting#error-local-exec-provisioner-error","content":" If you encounter the following error during the execution of the local-exec provisioner:  Error: local-exec provisioner error \\ with module.eks-blueprints.module.emr_on_eks[&quot;data_team_b&quot;].null_resource.update_trust_policy,\\ on .terraform/modules/eks-blueprints/modules/emr-on-eks/main.tf line 105, in resource &quot;null_resource&quot; \\ &quot;update_trust_policy&quot;:‚îÇ 105: provisioner &quot;local-exec&quot; {‚îÇ ‚îÇ Error running command 'set -e‚îÇ ‚îÇ aws emr-containers update-role-trust-policy \\ ‚îÇ --cluster-name emr-on-eks \\‚îÇ --namespace emr-data-team-b \\‚îÇ --role-name emr-on-eks-emr-eks-data-team-b   ","version":"Next","tagName":"h2"},{"title":"Issue Description:‚Äã","type":1,"pageTitle":"Troubleshooting","url":"/ai-on-eks/docs/infra/troubleshooting#issue-description","content":" The error message indicates that the emr-containers command is not present in the AWS CLI version being used. This issue has been addressed and fixed in AWS CLI version 2.0.54.  ","version":"Next","tagName":"h3"},{"title":"Solution‚Äã","type":1,"pageTitle":"Troubleshooting","url":"/ai-on-eks/docs/infra/troubleshooting#solution","content":" To resolve the issue, update your AWS CLI version to 2.0.54 or a later version by executing the following command:  pip install --upgrade awscliv2   By updating the AWS CLI version, you will ensure that the necessary emr-containers command is available and can be executed successfully during the provisioning process.  If you continue to experience any issues or require further assistance, please consult the AWS CLI GitHub issue for more details or contact our support team for additional guidance.  ","version":"Next","tagName":"h3"},{"title":"Timeouts during Terraform Destroy‚Äã","type":1,"pageTitle":"Troubleshooting","url":"/ai-on-eks/docs/infra/troubleshooting#timeouts-during-terraform-destroy","content":" ","version":"Next","tagName":"h2"},{"title":"Issue Description:‚Äã","type":1,"pageTitle":"Troubleshooting","url":"/ai-on-eks/docs/infra/troubleshooting#issue-description-1","content":" Customers may experience timeouts during the deletion of their environments, specifically when VPCs are being deleted. This is a known issue related to the vpc-cni component.  ","version":"Next","tagName":"h3"},{"title":"Symptoms:‚Äã","type":1,"pageTitle":"Troubleshooting","url":"/ai-on-eks/docs/infra/troubleshooting#symptoms","content":" ENIs (Elastic Network Interfaces) remain attached to subnets even after the environment is destroyed. The EKS managed security group associated with the ENI cannot be deleted by EKS.  ","version":"Next","tagName":"h3"},{"title":"Solution:‚Äã","type":1,"pageTitle":"Troubleshooting","url":"/ai-on-eks/docs/infra/troubleshooting#solution-1","content":" To overcome this issue, follow the recommended solution below:  Utilize the provided cleanup.sh scripts to ensure a proper cleanup of resources. Run the `cleanup.sh`` script, which is included in the blueprint. This script will handle the removal of any lingering ENIs and associated security groups.  ","version":"Next","tagName":"h3"},{"title":"Error: could not download chart‚Äã","type":1,"pageTitle":"Troubleshooting","url":"/ai-on-eks/docs/infra/troubleshooting#error-could-not-download-chart","content":" If you encounter the following error while attempting to download a chart:  ‚îÇ Error: could not download chart: failed to download &quot;oci://public.ecr.aws/karpenter/karpenter&quot; at version &quot;v0.18.1&quot; ‚îÇ ‚îÇ with module.eks_blueprints_kubernetes_addons.module.karpenter[0].module.helm_addon.helm_release.addon[0], ‚îÇ on .terraform/modules/eks_blueprints_kubernetes_addons/modules/kubernetes-addons/helm-addon/main.tf line 1, in resource &quot;helm_release&quot; &quot;addon&quot;: ‚îÇ 1: resource &quot;helm_release&quot; &quot;addon&quot; { ‚îÇ   Follow the steps below to resolve the issue:  ","version":"Next","tagName":"h2"},{"title":"Issue Description:‚Äã","type":1,"pageTitle":"Troubleshooting","url":"/ai-on-eks/docs/infra/troubleshooting#issue-description-2","content":" The error message indicates that there was a failure in downloading the specified chart. This issue can occur due to a bug in Terraform during the installation of Karpenter.  ","version":"Next","tagName":"h3"},{"title":"Solution:‚Äã","type":1,"pageTitle":"Troubleshooting","url":"/ai-on-eks/docs/infra/troubleshooting#solution-2","content":" To resolve the issue, you can try the following steps:  Authenticate with ECR: Run the following command to authenticate with the ECR (Elastic Container Registry) where the chart is located:  aws ecr-public get-login-password --region us-east-1 | docker login --username AWS --password-stdin public.ecr.aws   Re-run terraform apply: Execute the terraform apply command again with the --auto-approve flag to reapply the Terraform configuration:  terraform apply --auto-approve   By authenticating with ECR and re-running the terraform apply command, you will ensure that the necessary chart can be downloaded successfully during the installation process.  ","version":"Next","tagName":"h3"},{"title":"Terraform apply/destroy error to authenticate with EKS Cluster‚Äã","type":1,"pageTitle":"Troubleshooting","url":"/ai-on-eks/docs/infra/troubleshooting#terraform-applydestroy-error-to-authenticate-with-eks-cluster","content":" ERROR: ‚ï∑ ‚îÇ Error: Get &quot;http://localhost/api/v1/namespaces/kube-system/configmaps/aws-auth&quot;: dial tcp [::1]:80: connect: connection refused ‚îÇ ‚îÇ with module.eks.kubernetes_config_map_v1_data.aws_auth[0], ‚îÇ on .terraform/modules/eks/main.tf line 550, in resource &quot;kubernetes_config_map_v1_data&quot; &quot;aws_auth&quot;: ‚îÇ 550: resource &quot;kubernetes_config_map_v1_data&quot; &quot;aws_auth&quot; { ‚îÇ ‚ïµ   Solution:In this situation Terraform is unable to refresh the data resources and authenticate with EKS Cluster. See the discussion here  Try this approach first by using exec plugin.  provider &quot;kubernetes&quot; { host = module.eks_blueprints.eks_cluster_endpoint cluster_ca_certificate = base64decode(module.eks_blueprints.eks_cluster_certificate_authority_data) exec { api_version = &quot;client.authentication.k8s.io/v1beta1&quot; command = &quot;aws&quot; args = [&quot;eks&quot;, &quot;get-token&quot;, &quot;--cluster-name&quot;, module.eks_blueprints.eks_cluster_id] } }   If the issue still persists even after the above change then you can use alternative approach of using local kube config file. NOTE: This approach might not be ideal for production. It helps you to apply/destroy clusters with your local kube config.  Create a local kubeconfig for your cluster  aws eks update-kubeconfig --name &lt;EKS_CLUSTER_NAME&gt; --region &lt;CLUSTER_REGION&gt;   Update the providers.tf file with the below config by just using the config_path.  provider &quot;kubernetes&quot; { config_path = &quot;&lt;HOME_PATH&gt;/.kube/config&quot; } provider &quot;helm&quot; { kubernetes { config_path = &quot;&lt;HOME_PATH&gt;/.kube/config&quot; } } provider &quot;kubectl&quot; { config_path = &quot;&lt;HOME_PATH&gt;/.kube/config&quot; }   ","version":"Next","tagName":"h2"},{"title":"EMR Containers Virtual Cluster (dhwtlq9yx34duzq5q3akjac00) delete: unexpected state 'ARRESTED'‚Äã","type":1,"pageTitle":"Troubleshooting","url":"/ai-on-eks/docs/infra/troubleshooting#emr-containers-virtual-cluster-dhwtlq9yx34duzq5q3akjac00-delete-unexpected-state-arrested","content":" If you encounter an error message stating &quot;waiting for EMR Containers Virtual Cluster (xwbc22787q6g1wscfawttzzgb) delete: unexpected state 'ARRESTED', wanted target ''. last error: %!s(nil)&quot;, you can follow the steps below to resolve the issue:  Note: Replace &lt;REGION&gt; with the appropriate AWS region where the virtual cluster is located.  Open a terminal or command prompt.Run the following command to list the virtual clusters in the &quot;ARRESTED&quot; state:  aws emr-containers list-virtual-clusters --region &lt;REGION&gt; --states ARRESTED \\ --query 'virtualClusters[0].id' --output text   This command retrieves the ID of the virtual cluster in the &quot;ARRESTED&quot; state.  Run the following command to delete the virtual cluster:  aws emr-containers list-virtual-clusters --region &lt;REGION&gt; --states ARRESTED \\ --query 'virtualClusters[0].id' --output text | xargs -I{} aws emr-containers delete-virtual-cluster \\ --region &lt;REGION&gt; --id {}   Replace &lt;VIRTUAL_CLUSTER_ID&gt; with the ID of the virtual cluster obtained from the previous step.  By executing these commands, you will be able to delete the virtual cluster that is in the &quot;ARRESTED&quot; state. This should resolve the unexpected state issue and allow you to proceed with further operations.  ","version":"Next","tagName":"h2"},{"title":"Terminating namespace issue‚Äã","type":1,"pageTitle":"Troubleshooting","url":"/ai-on-eks/docs/infra/troubleshooting#terminating-namespace-issue","content":" If you encounter the issue where a namespace is stuck in the &quot;Terminating&quot; state and cannot be deleted, you can use the following command to remove the finalizers on the namespace:  Note: Replace &lt;namespace&gt; with the name of the namespace you want to delete.  NAMESPACE=&lt;namespace&gt; kubectl get namespace $NAMESPACE -o json | sed 's/&quot;kubernetes&quot;//' | kubectl replace --raw &quot;/api/v1/namespaces/$NAMESPACE/finalize&quot; -f -   This command retrieves the namespace details in JSON format, removes the &quot;kubernetes&quot; finalizer, and performs a replace operation to remove the finalizer from the namespace. This should allow the namespace to complete the termination process and be successfully deleted.  Please ensure that you have the necessary permissions to perform this operation. If you continue to experience issues or require further assistance, please reach out to our support team for additional guidance and troubleshooting steps.  ","version":"Next","tagName":"h2"},{"title":"KMS Alias AlreadyExistsException‚Äã","type":1,"pageTitle":"Troubleshooting","url":"/ai-on-eks/docs/infra/troubleshooting#kms-alias-alreadyexistsexception","content":" During your Terraform installation or redeployment, you might encounter an error saying: AlreadyExistsException: An alias with the name ... already exists. This happens when the KMS alias you're trying to create already exists in your AWS account.  ‚îÇ Error: creating KMS Alias (alias/eks/trainium-inferentia): AlreadyExistsException: An alias with the name arn:aws:kms:us-west-2:23423434:alias/eks/trainium-inferentia already exists ‚îÇ ‚îÇ with module.eks.module.kms.aws_kms_alias.this[&quot;cluster&quot;], ‚îÇ on .terraform/modules/eks.kms/main.tf line 452, in resource &quot;aws_kms_alias&quot; &quot;this&quot;: ‚îÇ 452: resource &quot;aws_kms_alias&quot; &quot;this&quot; { ‚îÇ   Solution:  To resolve this, delete the existing KMS alias using the aws kms delete-alias command. Remember to update the alias name and region in the command before running it.  aws kms delete-alias --alias-name &lt;KMS_ALIAS_NAME&gt; --region &lt;ENTER_REGION&gt;   ","version":"Next","tagName":"h2"},{"title":"Error: creating CloudWatch Logs Log Group‚Äã","type":1,"pageTitle":"Troubleshooting","url":"/ai-on-eks/docs/infra/troubleshooting#error-creating-cloudwatch-logs-log-group","content":" Terraform cannot create a CloudWatch Logs log group because it already exists in your AWS account.  ‚ï∑ ‚îÇ Error: creating CloudWatch Logs Log Group (/aws/eks/trainium-inferentia/cluster): operation error CloudWatch Logs: CreateLogGroup, https response error StatusCode: 400, RequestID: 5c34c47a-72c6-44b2-a345-925824f24d38, ResourceAlreadyExistsException: The specified log group already exists ‚îÇ ‚îÇ with module.eks.aws_cloudwatch_log_group.this[0], ‚îÇ on .terraform/modules/eks/main.tf line 106, in resource &quot;aws_cloudwatch_log_group&quot; &quot;this&quot;: ‚îÇ 106: resource &quot;aws_cloudwatch_log_group&quot; &quot;this&quot; {   Solution:  Delete the existing log group by updating log group name and the region.  aws logs delete-log-group --log-group-name &lt;LOG_GROUP_NAME&gt; --region &lt;ENTER_REGION&gt;   ","version":"Next","tagName":"h2"},{"title":"Karpenter Error - Missing Service Linked Role‚Äã","type":1,"pageTitle":"Troubleshooting","url":"/ai-on-eks/docs/infra/troubleshooting#karpenter-error---missing-service-linked-role","content":" Karpenter throws below error while trying to create new instances.  &quot;error&quot;:&quot;launching nodeclaim, creating instance, with fleet error(s), AuthFailure.ServiceLinkedRoleCreationNotPermitted: The provided credentials do not have permission to create the service-linked role for EC2 Spot Instances.&quot;}   Solution:  You will need to create the service linked role in the AWS account you're using to avoid ServiceLinkedRoleCreationNotPermitted error.  aws iam create-service-linked-role --aws-service-name spot.amazonaws.com   ","version":"Next","tagName":"h2"},{"title":"Error: AmazonEKS_CNI_IPv6_Policy does not exist‚Äã","type":1,"pageTitle":"Troubleshooting","url":"/ai-on-eks/docs/infra/troubleshooting#error-amazoneks_cni_ipv6_policy-does-not-exist","content":" If you encounter the error below when deploying a solution that supports IPv6:  ‚îÇ Error: attaching IAM Policy (arn:aws:iam::1234567890:policy/AmazonEKS_CNI_IPv6_Policy) to IAM Role (core-node-group-eks-node-group-20241111182906854800000003): operation error IAM: AttachRolePolicy, https response error StatusCode: 404, RequestID: 9c99395a-ce3d-4a05-b119-538470a3a9f7, NoSuchEntity: Policy arn:aws:iam::1234567890:policy/AmazonEKS_CNI_IPv6_Policy does not exist or is not attachable.   ","version":"Next","tagName":"h2"},{"title":"Issue Description:‚Äã","type":1,"pageTitle":"Troubleshooting","url":"/ai-on-eks/docs/infra/troubleshooting#issue-description-3","content":" The Amazon VPC CNI plugin requires IAM permission to assign IPv6 addresses so you must create an IAM policy and associate it with the role that the CNI will use. However, each IAM policy name must be unique in the same AWS account. This causes a conflict if the policy is created as part of the terraform stack and it is deployed multiple times.  To resolve this error you will need to create the Policy with the commands below. You should only need to do this once per AWS account.  ","version":"Next","tagName":"h3"},{"title":"Solution:‚Äã","type":1,"pageTitle":"Troubleshooting","url":"/ai-on-eks/docs/infra/troubleshooting#solution-3","content":" Copy the following text and save it to a file named vpc-cni-ipv6-policy.json.  { &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Effect&quot;: &quot;Allow&quot;, &quot;Action&quot;: [ &quot;ec2:AssignIpv6Addresses&quot;, &quot;ec2:DescribeInstances&quot;, &quot;ec2:DescribeTags&quot;, &quot;ec2:DescribeNetworkInterfaces&quot;, &quot;ec2:DescribeInstanceTypes&quot; ], &quot;Resource&quot;: &quot;&quot; }, { &quot;Effect&quot;: &quot;Allow&quot;, &quot;Action&quot;: [ &quot;ec2:CreateTags&quot; ], &quot;Resource&quot;: [ &quot;arn:aws:ec2::*:network-interface/*&quot; ] } ] }   Create the IAM policy.  aws iam create-policy --policy-name AmazonEKS_CNI_IPv6_Policy --policy-document file://vpc-cni-ipv6-policy.json   Re-run the install.sh script for the blueprint ","version":"Next","tagName":"h3"},{"title":"intro","type":0,"sectionRef":"#","url":"/ai-on-eks/docs/intro","content":"intro","keywords":"","version":"Next"},{"title":"Bin packing for Amazon EKS","type":0,"sectionRef":"#","url":"/ai-on-eks/docs/resources/binpacking-custom-scheduler-eks","content":"","keywords":"","version":"Next"},{"title":"Introduction‚Äã","type":1,"pageTitle":"Bin packing for Amazon EKS","url":"/ai-on-eks/docs/resources/binpacking-custom-scheduler-eks#introduction","content":" In this post, we will show you how to enable a custom scheduler with Amazon EKS when running DoEKS especially for Spark on EKS, including OSS Spark and EMR on EKS. The custom scheduler is a custom Kubernetes scheduler with MostAllocated strategy running in data plane.  ","version":"Next","tagName":"h2"},{"title":"Why bin packing‚Äã","type":1,"pageTitle":"Bin packing for Amazon EKS","url":"/ai-on-eks/docs/resources/binpacking-custom-scheduler-eks#why-bin-packing","content":" By default, the scheduling-plugin NodeResourcesFit use the LeastAllocated for score strategies. For the long running workloads, that is good because of high availability. But for batch jobs, like Spark workloads, this would lead high cost. By changing the from LeastAllocated to MostAllocated, it avoids spreading pods across all running nodes, leading to higher resource utilization and better cost efficiency.  Batch jobs like Spark are running on demand with limited or predicted time. With MostAllocated strategy, Spark executors are always bin packing into one node util the node can not host any pods. You can see the following picture shows the  MostAllocated in EMR on EKS.    LeastAllocated in EMR on EKS    ","version":"Next","tagName":"h3"},{"title":"Pros‚Äã","type":1,"pageTitle":"Bin packing for Amazon EKS","url":"/ai-on-eks/docs/resources/binpacking-custom-scheduler-eks#pros","content":" Improve the node utilizationsSave the cost  ","version":"Next","tagName":"h3"},{"title":"Considerations‚Äã","type":1,"pageTitle":"Bin packing for Amazon EKS","url":"/ai-on-eks/docs/resources/binpacking-custom-scheduler-eks#considerations","content":" Although we have provided upgrade guidance, support matrix and high availability design, but maintaining a custom scheduler in data plane needs effort including:  Upgrade operations. Plan the upgrading along with your batch jobs, make sure the scheduler are running as desired.Monitoring the scheduler. Monitoring and alerting are required for production purpose.Adjust the scheduler pod resource and other customizations regarding your requirements.  ","version":"Next","tagName":"h3"},{"title":"Deploying the Solution‚Äã","type":1,"pageTitle":"Bin packing for Amazon EKS","url":"/ai-on-eks/docs/resources/binpacking-custom-scheduler-eks#deploying-the-solution","content":" ","version":"Next","tagName":"h2"},{"title":"Clone the repo‚Äã","type":1,"pageTitle":"Bin packing for Amazon EKS","url":"/ai-on-eks/docs/resources/binpacking-custom-scheduler-eks#clone-the-repo","content":" git clone https://github.com/aws-samples/custom-scheduler-eks cd custom-scheduler-eks   ","version":"Next","tagName":"h3"},{"title":"Manifests‚Äã","type":1,"pageTitle":"Bin packing for Amazon EKS","url":"/ai-on-eks/docs/resources/binpacking-custom-scheduler-eks#manifests","content":" Amazon EKS 1.24  kubectl apply -f deploy/manifests/custom-scheduler/amazon-eks-1.24-custom-scheduler.yaml   Amazon EKS 1.29  kubectl apply -f deploy/manifests/custom-scheduler/amazon-eks-1.29-custom-scheduler.yaml   Other Amazon EKS versions  replace the related image URL(https://gallery.ecr.aws/eks-distro/kubernetes/kube-scheduler)  Please refer to custom-scheduler for more info.  ","version":"Next","tagName":"h3"},{"title":"Set up pod template to use the custom scheduler for Spark‚Äã","type":1,"pageTitle":"Bin packing for Amazon EKS","url":"/ai-on-eks/docs/resources/binpacking-custom-scheduler-eks#set-up-pod-template-to-use-the-custom-scheduler-for-spark","content":" We should add custom scheduler name to the pod template as follows  kind: Pod spec: schedulerName: custom-k8s-scheduler volumes: - name: spark-local-dir-1 hostPath: path: /local1 initContainers: - name: volume-permission image: public.ecr.aws/docker/library/busybox # grant volume access to hadoop user command: ['sh', '-c', 'if [ ! -d /data1 ]; then mkdir /data1;fi; chown -R 999:1000 /data1'] volumeMounts: - name: spark-local-dir-1 mountPath: /data1 containers: - name: spark-kubernetes-executor volumeMounts: - name: spark-local-dir-1 mountPath: /data1   ","version":"Next","tagName":"h3"},{"title":"Verification and Monitor via eks-node-viewer‚Äã","type":1,"pageTitle":"Bin packing for Amazon EKS","url":"/ai-on-eks/docs/resources/binpacking-custom-scheduler-eks#verification-and-monitor-via-eks-node-viewer","content":" Before apply the change in the pod template    After the change: Higher CPU usage at pod schedule time  ","version":"Next","tagName":"h2"},{"title":"Conclusion‚Äã","type":1,"pageTitle":"Bin packing for Amazon EKS","url":"/ai-on-eks/docs/resources/binpacking-custom-scheduler-eks#conclusion","content":" By using the custom scheduler, we can fully improve the node utilizations for the Spark workloads which will save the cost by triggering node scale in.  For the users that running Spark on EKS, we recommend you adopt this custom scheduler before Amazon EKS officially support the kube-scheduler customization. ","version":"Next","tagName":"h2"},{"title":"EKS Best Practices","type":0,"sectionRef":"#","url":"/ai-on-eks/docs/resources/eks-best-practices","content":"EKS Best Practices Amazon EKS Best Practices guide for AI/ML applications is located and manged at the AWS documentation site for Amazon EKS. Please use this resource for any best practice recommendations.","keywords":"","version":"Next"},{"title":"Introduction to Resources","type":0,"sectionRef":"#","url":"/ai-on-eks/docs/resources/intro","content":"Introduction to Resources Welcome to the Resources section. This area is dedicated to providing you with a wealth of information, insights, and learning materials to enhance your understanding and skills in Data and AI workloads running on EKS. This area is a hub of valuable information, offering a range of materials from insightful mini blogs and AWS workshop links to engaging video content and reInvent talks.","keywords":"","version":"Next"},{"title":"Observability","type":0,"sectionRef":"#","url":"/ai-on-eks/docs/resources/observability","content":"","keywords":"","version":"Next"},{"title":"Architecture‚Äã","type":1,"pageTitle":"Observability","url":"/ai-on-eks/docs/resources/observability#architecture","content":"   ","version":"Next","tagName":"h2"},{"title":"What's Included‚Äã","type":1,"pageTitle":"Observability","url":"/ai-on-eks/docs/resources/observability#whats-included","content":" PrometheusOpenSearchFluentBitKube State MetricsMetrics ServerAlertmanagerGrafanaPod/Service monitors for AI/ML workloadsAI/ML Dashboards  ","version":"Next","tagName":"h2"},{"title":"Why‚Äã","type":1,"pageTitle":"Observability","url":"/ai-on-eks/docs/resources/observability#why","content":" Understanding the performance of AI/ML workloads is challenging: Is the GPU getting data fast enough? Is the CPU the bottleneck? Is the storage fast enough? These are questions that are hard to answer in isolation. The more of the picture one is able to see, the more clarity there is in identifying performance bottlenecks.  ","version":"Next","tagName":"h2"},{"title":"How‚Äã","type":1,"pageTitle":"Observability","url":"/ai-on-eks/docs/resources/observability#how","content":" The JARK infrastructure already comes with this architecture enabled by default, if you would like to add it to your infrastructure, you need to ensure 2 variables are set to true in blueprint.tfvars:  enable_argocd = true enable_ai_ml_observability_stack = true   The first variable deploys ArgoCD, which is used to deploy the observability architecture, the second variable deploys the architecture.  ","version":"Next","tagName":"h2"},{"title":"Usage‚Äã","type":1,"pageTitle":"Observability","url":"/ai-on-eks/docs/resources/observability#usage","content":" The architecture is entirely deployed into the monitoring namespace. To access Grafana: kubectl port-forward -n monitoring svc/kube-prometheus-stack-grafana 3000:80. You can then open https://localhost:3000 to log into grafana with username admin and password prom-operator. You can refer to the security section in the Readme to see how to change the username/password  ","version":"Next","tagName":"h2"},{"title":"Training‚Äã","type":1,"pageTitle":"Observability","url":"/ai-on-eks/docs/resources/observability#training","content":" Ray training job logs and metrics will be automatically collected by the Observability architecture and can be found in the training dashboard.  Example‚Äã  A full example of this can be found in the AI/ML observability repo. We will also be updating the Blueprints here to make use of this architecture.  ","version":"Next","tagName":"h3"},{"title":"Inference‚Äã","type":1,"pageTitle":"Observability","url":"/ai-on-eks/docs/resources/observability#inference","content":" Ray inference metrics should be automatically picked up by the observability infrastructure and can be found in the inference dashboard. To instrument your inference workloads for logging, you will need to add a few items:  FluentBit Config‚Äã  apiVersion: v1 kind: ConfigMap metadata: name: fluentbit-config namespace: default data: fluent-bit.conf: |- [INPUT] Name tail Path /tmp/ray/session_latest/logs/* Tag ray Path_Key true Refresh_Interval 5 [FILTER] Name modify Match ray Add POD_LABELS ${POD_LABELS} [OUTPUT] Name stdout Format json   Deploy this into the namespace in which you intend to run your inference workload. You only need one in each namespace to tell the FluentBit sidecar how to output the logs.  FluentBit Sidecar‚Äã  We will need to add a sidecar to the Ray inference service so FluentBit can write the logs to STDOUT   - name: fluentbit image: fluent/fluent-bit:3.2.2 env: - name: POD_LABELS valueFrom: fieldRef: fieldPath: metadata.labels['ray.io/cluster'] resources: requests: cpu: 100m memory: 128Mi limits: cpu: 100m memory: 128Mi volumeMounts: - mountPath: /tmp/ray name: ray-logs - mountPath: /fluent-bit/etc/fluent-bit.conf subPath: fluent-bit.conf name: fluentbit-config   Add this section to the workerGroupSpecs containers  FluentBit Volume‚Äã  Finally, we need to add the configmap volume to our volumes section:   - name: fluentbit-config configMap: name: fluentbit-config   vLLM Metrics‚Äã  vLLM also outputs useful metrics like the Time to First Token, throughput, latencies, cache utilization and more. To get access to these metrics, we need to add a route to our pod for the metrics path:  # Imports import re from prometheus_client import make_asgi_app from fastapi import FastAPI from starlette.routing import Mount app = FastAPI() class Deployment: def _init__(selfself, **kwargs): ... route = Mount(&quot;/metrics&quot;, make_asgi_app()) # Workaround for 307 Redirect for /metrics route.path_regex = re.compile('^/metrics(?P&lt;path&gt;.*)$') app.routes.append(route)   This will allow the deployed monitor to collect the vLLM metrics and display them in the inference dashboard.  Example‚Äã  A full example of this can be found in the AI/ML observability repo. We will also be updating the Blueprints here to make use of this architecture. ","version":"Next","tagName":"h3"},{"title":"Mounpoint S3: Enhancing Amazon S3 File Access for Data & AI Workloads on Amazon EKS","type":0,"sectionRef":"#","url":"/ai-on-eks/docs/resources/mountpoint-s3","content":"","keywords":"","version":"Next"},{"title":"What is Mountpoint-S3?‚Äã","type":1,"pageTitle":"Mounpoint S3: Enhancing Amazon S3 File Access for Data & AI Workloads on Amazon EKS","url":"/ai-on-eks/docs/resources/mountpoint-s3#what-is-mountpoint-s3","content":" Mountpoint-S3 is an open-source file client developed by AWS that translates file operations into S3 API calls, enabling your applications to interact with Amazon S3 buckets as if they were local disks. Mountpoint for Amazon S3 is optimized for applications that need high read throughput to large objects, potentially from many clients at once, and to write new objects sequentially from a single client at a time. It offers significant performance gains compared to traditional S3 access methods, making it ideal for data-intensive workloads and AI/ML training.  A key feature of Mountpoint-S3 is its compatibility with both Amazon S3 Standrad and Amazon S3 Express One Zone. S3 Express One Zone is a high-performance storage class designed for single-Availability Zone deployment. It offers consistent single-digit millisecond data access, making it ideal for frequently accessed data and latency-sensitive applications. S3 Express One Zone is known for delivering data access speeds up to 10 times faster and at up to 50% lower request costs compared to S3 Standard. This storage class enables users to co-locate storage and compute resources in the same Availability Zone, optimizing performance and potentially reducing compute costs.  The integration with S3 Express One Zone enhances Mountpoint-S3's capabilities, particularly for machine learning and analytics workloads, as it can be used with services like Amazon EKS, Amazon SageMaker Model Training, Amazon Athena, Amazon EMR, and AWS Glue Data Catalog. The automatic scaling of storage based on consumption in S3 Express One Zone simplifies management for low-latency workloads, making Mountpoint-S3 a highly effective tool for a wide range of data-intensive tasks and AI/ML training environments.  warning Mountpoint-S3 does not support file renaming operations, which may limit its applicability in scenarios where such functionality is essential.  ","version":"Next","tagName":"h2"},{"title":"Deploying Mountpoint-S3 on Amazon EKS:‚Äã","type":1,"pageTitle":"Mounpoint S3: Enhancing Amazon S3 File Access for Data & AI Workloads on Amazon EKS","url":"/ai-on-eks/docs/resources/mountpoint-s3#deploying-mountpoint-s3-on-amazon-eks","content":" ","version":"Next","tagName":"h2"},{"title":"Step 1: Set Up IAM Role for S3 CSI Driver‚Äã","type":1,"pageTitle":"Mounpoint S3: Enhancing Amazon S3 File Access for Data & AI Workloads on Amazon EKS","url":"/ai-on-eks/docs/resources/mountpoint-s3#step-1-set-up-iam-role-for-s3-csi-driver","content":" Create an IAM role using Terraform with the necessary permissions for the S3 CSI driver. This step is critical for ensuring secure and efficient communication between EKS and S3.   #--------------------------------------------------------------- # IRSA for Mountpoint for Amazon S3 CSI Driver #--------------------------------------------------------------- module &quot;s3_csi_driver_irsa&quot; { source = &quot;terraform-aws-modules/iam/aws//modules/iam-role-for-service-accounts-eks&quot; version = &quot;~&gt; 5.34&quot; role_name_prefix = format(&quot;%s-%s-&quot;, local.name, &quot;s3-csi-driver&quot;) role_policy_arns = { # WARNING: Demo purpose only. Bring your own IAM policy with least privileges s3_csi_driver = &quot;arn:aws:iam::aws:policy/AmazonS3FullAccess&quot; } oidc_providers = { main = { provider_arn = module.eks.oidc_provider_arn namespace_service_accounts = [&quot;kube-system:s3-csi-driver-sa&quot;] } } tags = local.tags }   ","version":"Next","tagName":"h3"},{"title":"Step 2: Configure EKS Blueprints Addons‚Äã","type":1,"pageTitle":"Mounpoint S3: Enhancing Amazon S3 File Access for Data & AI Workloads on Amazon EKS","url":"/ai-on-eks/docs/resources/mountpoint-s3#step-2-configure-eks-blueprints-addons","content":" Configure EKS Blueprints Addons Terraform module to utilize this role for the Amazon EKS Add-on of S3 CSI driver.  #--------------------------------------------------------------- # EKS Blueprints Addons #--------------------------------------------------------------- module &quot;eks_blueprints_addons&quot; { source = &quot;aws-ia/eks-blueprints-addons/aws&quot; version = &quot;~&gt; 1.2&quot; cluster_name = module.eks.cluster_name cluster_endpoint = module.eks.cluster_endpoint cluster_version = module.eks.cluster_version oidc_provider_arn = module.eks.oidc_provider_arn #--------------------------------------- # Amazon EKS Managed Add-ons #--------------------------------------- eks_addons = { aws-mountpoint-s3-csi-driver = { service_account_role_arn = module.s3_csi_driver_irsa.iam_role_arn } } }   ","version":"Next","tagName":"h3"},{"title":"Step 3: Define a PersistentVolume‚Äã","type":1,"pageTitle":"Mounpoint S3: Enhancing Amazon S3 File Access for Data & AI Workloads on Amazon EKS","url":"/ai-on-eks/docs/resources/mountpoint-s3#step-3-define-a-persistentvolume","content":" Specify the S3 bucket, region details and access modes in a PersistentVolume (PV) configuration. This step is crucial for defining how EKS interacts with the S3 bucket.  --- apiVersion: v1 kind: PersistentVolume metadata: name: s3-pv spec: capacity: storage: 1200Gi # ignored, required accessModes: - ReadWriteMany # supported options: ReadWriteMany / ReadOnlyMany mountOptions: - uid=1000 - gid=2000 - allow-other - allow-delete - region &lt;ENTER_REGION&gt; csi: driver: s3.csi.aws.com # required volumeHandle: s3-csi-driver-volume volumeAttributes: bucketName: &lt;ENTER_S3_BUCKET_NAME&gt;   ","version":"Next","tagName":"h3"},{"title":"Step 4: Create a PersistentVolumeClaim‚Äã","type":1,"pageTitle":"Mounpoint S3: Enhancing Amazon S3 File Access for Data & AI Workloads on Amazon EKS","url":"/ai-on-eks/docs/resources/mountpoint-s3#step-4-create-a-persistentvolumeclaim","content":" Establish a PersistentVolumeClaim (PVC) to utilize the defined PV, specifying access modes and static provisioning requirements.  --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: s3-claim namespace: spark-team-a spec: accessModes: - ReadWriteMany # supported options: ReadWriteMany / ReadOnlyMany storageClassName: &quot;&quot; # required for static provisioning resources: requests: storage: 1200Gi # ignored, required volumeName: s3-pv   ","version":"Next","tagName":"h3"},{"title":"Step 5: Using PVC with Pod Definition‚Äã","type":1,"pageTitle":"Mounpoint S3: Enhancing Amazon S3 File Access for Data & AI Workloads on Amazon EKS","url":"/ai-on-eks/docs/resources/mountpoint-s3#step-5-using-pvc-with-pod-definition","content":" After setting up the PersistentVolumeClaim (PVC), the next step is to utilize it within a Pod definition. This allows your applications running in Kubernetes to access the data stored in the S3 bucket. Below are examples demonstrating how to reference the PVC in a Pod definition for different scenarios.  Example 1: Basic Pod Using PVC for Storage‚Äã  This example shows a basic Pod definition that mounts the s3-claim PVC to a directory within the container.  In this example, the PVC s3-claim is mounted to the /data directory of the nginx container. This setup allows the application running within the container to read and write data to the S3 bucket as if it were a local directory.   apiVersion: v1 kind: Pod metadata: name: example-pod namespace: spark-team-a spec: containers: - name: app-container image: nginx # Example image volumeMounts: - name: s3-storage mountPath: &quot;/data&quot; # The path where the S3 bucket will be mounted volumes: - name: s3-storage persistentVolumeClaim: claimName: s3-claim   Example 2: AI/ML Training Job Using PVC‚Äã  In AI/ML training scenarios, data accessibility and throughput are critical. This example demonstrates a Pod configuration for a machine learning training job that accesses datasets stored in S3.   apiVersion: v1 kind: Pod metadata: name: ml-training-pod namespace: spark-team-a spec: containers: - name: training-container image: ml-training-image # Replace with your ML training image volumeMounts: - name: dataset-storage mountPath: &quot;/datasets&quot; # Mount path for training data volumes: - name: dataset-storage persistentVolumeClaim: claimName: s3-claim   warning Mountpoint S3, when used with Amazon S3 or S3 express, may not be suitable as a shuffle storage for Spark workloads. This limitation arises due to the nature of shuffle operations in Spark, which often involve multiple clients reading and writing to the same location simultaneously. Additionally, Mountpoint S3 does not support file renaming, a critical feature required for efficient shuffle operations in Spark. This lack of renaming capability can lead to operational challenges and potential performance bottlenecks in data processing tasks.  ","version":"Next","tagName":"h3"},{"title":"Next Steps:‚Äã","type":1,"pageTitle":"Mounpoint S3: Enhancing Amazon S3 File Access for Data & AI Workloads on Amazon EKS","url":"/ai-on-eks/docs/resources/mountpoint-s3#next-steps","content":" Explore the provided Terraform code snippet for detailed deployment instructions.Refer to the official Mountpoint-S3 documentation for further configuration options and limitations.Utilize Mountpoint-S3 to unlock high-performance, scalable S3 access within your EKS applications.  By understanding Mountpoint-S3's capabilities and limitations, you can make informed decisions to optimize your data-driven workloads on Amazon EKS. ","version":"Next","tagName":"h2"},{"title":"Networking for AI","type":0,"sectionRef":"#","url":"/ai-on-eks/docs/resources/networking","content":"","keywords":"","version":"Next"},{"title":"VPC and IP Considerations‚Äã","type":1,"pageTitle":"Networking for AI","url":"/ai-on-eks/docs/resources/networking#vpc-and-ip-considerations","content":" ","version":"Next","tagName":"h2"},{"title":"Plan for a large amount of IP address usage in your EKS clusters.‚Äã","type":1,"pageTitle":"Networking for AI","url":"/ai-on-eks/docs/resources/networking#plan-for-a-large-amount-of-ip-address-usage-in-your-eks-clusters","content":" The AWS VPC CNI maintains a ‚Äúwarm pool‚Äù of IP addresses on the EKS worker nodes to assign to Pods. When more IP addresses are needed for your Pods, the CNI must communicate with EC2 APIs to assign the addresses to your nodes. During periods of high churn or large scale out these EC2 API calls can be rate throttled, which will delay the provisioning of Pods and thus delay the execution of workloads. When designing the VPC for your environment plan for more IP addresses than just your pods to accommodate this warm pool.  With the default VPC CNI configuration larger nodes will consume more IP addresses. For example a m5.8xlarge node that is running 10 pods will hold 60 IPs total (to satisfy WARM_ENI=1). However a m5.16xlarge node would hold 100 IPs. Configuring the VPC CNI to minimize this warm pool can increase the EC2 API calls from your nodes and increase the risk of rate throttling. Planning for this extra IP address usage can avoid rate throttling problems and managing the IP address usage.  ","version":"Next","tagName":"h3"},{"title":"Consider using a secondary CIDR if your IP space is constrained.‚Äã","type":1,"pageTitle":"Networking for AI","url":"/ai-on-eks/docs/resources/networking#consider-using-a-secondary-cidr-if-your-ip-space-is-constrained","content":" If you are working with a network that spans multiple connected VPCs or sites the routable address space may be limited. For example, your VPC may be limited to small subnets like below. In this VPC we wouldn‚Äôt be able to run more than one m5.16xlarge node without adjusting the CNI configuration.    You can add additional VPC CIDRs from a range that is not routable across VPCs (such as the RFC 6598 range, 100.64.0.0/10). In this case we added 100.64.0.0/16, 100.65.0.0/16, and 100.66.0.0/16 to the VPC (as this is the maximum CIDR size), then created new subnets with those CIDRs. Finally we recreated the node groups in the new subnets, leaving the existing EKS cluster control plane in place.    With this configuration you can still communicate with the EKS cluster control plane from connected VPCs but your nodes and pods have plenty of IP addresses to accommodate your workloads and the warm pool.  ","version":"Next","tagName":"h3"},{"title":"Tuning the VPC CNI‚Äã","type":1,"pageTitle":"Networking for AI","url":"/ai-on-eks/docs/resources/networking#tuning-the-vpc-cni","content":" ","version":"Next","tagName":"h2"},{"title":"VPC CNI and EC2 Rate Throttling‚Äã","type":1,"pageTitle":"Networking for AI","url":"/ai-on-eks/docs/resources/networking#vpc-cni-and-ec2-rate-throttling","content":" When an EKS worker node is launched it initially has a single ENI with a single IP address attached for the EC2 instance to communicate. As the VPC CNI launches it tries to provision a Warm Pool of IP addresses that can be assigned to Kubernetes Pods (More details in the EKS Best Practices Guide).  The VPC CNI must make AWS EC2 API calls (like AssignPrivateIpV4Address and DescribeNetworkInterfaces) to assign those additional IPs and ENIs to the worker node. When the EKS cluster scales out the number of Nodes or Pods there could be a spike in the number of these EC2 API calls. This surge of calls could encounter rate throttling from the EC2 API to help the performance of the service, and to ensure fair usage for all Amazon EC2 customers. This rate throttling can cause the pool of IP address to be exhausted while the CNI tries to allocate more IPs.  These failures will cause errors like the one below, indicating that the provisioning of the container network namespace has failed because the VPC CNI could not provision an IP address.  Failed to create pod sandbox: rpc error: code = Unknown desc = failed to set up sandbox container &quot;xxxxxxxxxxxxxxxxxxxxxx&quot; network for pod &quot;test-pod&quot;: networkPlugin cni failed to set up pod test-pod_default&quot; network: add cmd: failed to assign an IP address to container   This failure delays the launch of the Pod and adds pressure to the kubelet and worker node as this action is retried until the IP address is assigned. To avoid this delay you can configure the CNI to reduce the number of EC2 API calls needed.  ","version":"Next","tagName":"h3"},{"title":"Avoid using WARM_IP_TARGET in large clusters, or cluster with a lot of churn‚Äã","type":1,"pageTitle":"Networking for AI","url":"/ai-on-eks/docs/resources/networking#avoid-using-warm_ip_target-in-large-clusters-or-cluster-with-a-lot-of-churn","content":" WARM_IP_TARGET can help limit the ‚Äúwasted‚Äù IPs for small clusters, or clusters that has very low pod churn. However, this environment variable on the VPC CNI needs to be carefully configured in large clusters as it may increase the number of EC2 API calls, increasing the risk and impact of rate throttling.  For clusters that have a lot of Pod churn, it is recommended to set MINIMUM_IP_TARGET to a value slightly higher than the expected number of pods you plan to run on each node. This will allow the CNI to provision all of those IP addresses in a single (or few) calls.   [...] # EKS Addons cluster_addons = { vpc-cni = { configuration_values = jsonencode({ env = { MINIMUM_IP_TARGET = &quot;30&quot; } }) } } [...]   ","version":"Next","tagName":"h3"},{"title":"Limit the number of IPs per node on large instance types with MAX_ENI and max-pods‚Äã","type":1,"pageTitle":"Networking for AI","url":"/ai-on-eks/docs/resources/networking#limit-the-number-of-ips-per-node-on-large-instance-types-with-max_eni-and-max-pods","content":" When using larger instance types such as 16xlarge or 24xlarge the number of IP addresses that can be assigned per ENI can be fairly large. For example, a c5.18xlarge instance type with the default CNI configuration of WARM_ENI=1 would end up holding 100 IP addresses (50 IPs per ENI * 2 ENIs) when running a handful of pods.  For some workloads the CPU, Memory, or other resource will limit the number of Pods on that c5.18xlarge before we need more than 50 IPs. In this case you may want to be able to run 30-40 pods maximum on that instance.   [...] # EKS Addons cluster_addons = { vpc-cni = { configuration_values = jsonencode({ env = { MAX_ENI = &quot;1&quot; } }) } } [...]   Setting the MAX_ENI=1 option on the CNI and that this will limit the number of IP addresses each node is able to provision, but it does not limit the number of pod that kubernetes will try to schedule to the nodes. This can lead to a situation where pods are scheduled to nodes that are unable to provision more IP addresses.  To limit the IPs and stop k8s from scheduling too many pods you will need to:  Update the CNI configuration environment variables to set MAX_ENI=1Update the --max-pods option for the kubelet on the worker nodes.  To configure the --max-pods option you can update the userdata for your worker nodes to set this option via the --kubelet -extra-args in the bootstrap.sh script. By default this script configures the max-pods value for the kubelet, the --use-max-pods false` option disables this behavior when providing your own value:   eks_managed_node_groups = { system = { instance_types = [&quot;m5.xlarge&quot;] min_size = 0 max_size = 5 desired_size = 3 pre_bootstrap_user_data = &lt;&lt;-EOT EOT bootstrap_extra_args = &quot;--use-max-pods false --kubelet-extra-args '--max-pods=&lt;your_value&gt;'&quot; }   One problem is the number of IPs per ENI is different based on the Instance type (for example a m5d.2xlarge can have 15 IPs per ENI, where a m5d.4xlarge can hold 30 IPs per ENI). This means hard-coding a value for max-pods may cause problems if you change instance types or in mixed-instance environments.  In the EKS Optimized AMI releases there is a script included that can be used to help calculate the AWS Recommended max-pods value. If you‚Äôd like to automate this calculation for mixed instances you will also need to update the userdata for your instances to use the --instance-type-from-imds flag to autodiscover the instance type from instance metadata.   eks_managed_node_groups = { system = { instance_types = [&quot;m5.xlarge&quot;] min_size = 0 max_size = 5 desired_size = 3 pre_bootstrap_user_data = &lt;&lt;-EOT /etc/eks/max-pod-calc.sh --instance-type-from-imds ‚Äîcni-version 1.13.4 ‚Äîcni-max-eni 1 EOT bootstrap_extra_args = &quot;--use-max-pods false --kubelet-extra-args '--max-pods=&lt;your_value&gt;'&quot; }   Maxpods with Karpenter‚Äã  By default, Nodes provisioned by Karpenter will have the max pods on a node based on the node instance type. To configure the --max-pods option as mentioned above by defining at the Provisioner level by specifying maxPods within the .spec.kubeletConfiguration . This value will be used during Karpenter pod scheduling and passed through to --max-pods on kubelet startup.  Below is the example Provisioner spec:  apiVersion: karpenter.sh/v1alpha5 kind: Provisioner metadata: name: default spec: providerRef: name: default requirements: - key: &quot;karpenter.k8s.aws/instance-category&quot; operator: In values: [&quot;c&quot;, &quot;m&quot;, &quot;r&quot;] - key: &quot;karpenter.sh/capacity-type&quot; # If not included, the webhook for the AWS cloud provider will default to on-demand operator: In values: [&quot;spot&quot;, &quot;on-demand&quot;] # Karpenter provides the ability to specify a few additional Kubelet args. # These are all optional and provide support for additional customization and use cases. kubeletConfiguration: maxPods: 30   ","version":"Next","tagName":"h3"},{"title":"Application‚Äã","type":1,"pageTitle":"Networking for AI","url":"/ai-on-eks/docs/resources/networking#application","content":" ","version":"Next","tagName":"h2"},{"title":"DNS Lookups and ndots‚Äã","type":1,"pageTitle":"Networking for AI","url":"/ai-on-eks/docs/resources/networking#dns-lookups-and-ndots","content":" In Kubernetes Pods with the default DNS configuration have a resolv.conf file like so:  nameserver 10.100.0.10 search namespace.svc.cluster.local svc.cluster.local cluster.local ec2.internal options ndots:5   The domain names listed in the search line are appended to DNS names that are not fully qualified domain names (FQDN). For example, if a pod tries to connect to a Kubernetes service using servicename.namespace the domains would be appended in order until the DNS name matched the full kubernetes service name:  servicename.namespace.namespace.svc.cluster.local &lt;--- Fails with NXDOMAIN servicename.namespace.svc.cluster.local &lt;-- Succeed   Whether or not a domain is fully qualified is determined by the ndots option in the resolv.conf. This option defines the number of dots that must be in a domain name before the search domains are skipped. These additional searches can add latency to connections to external resources like S3 and RDS endpoints.  The default ndots setting in Kubernetes is five, if your application isn‚Äôt talking to other pods in the cluster, we can set the ndots to a low value like ‚Äú2‚Äù. This is a good starting point, because it still allows your application to do service discovery within the same namespace and in other namespaces within the cluster, but allows a domain like s3.us-east-2.amazonaws.com to be recognized as a FQDN (skipping the search domains).  Here‚Äôs an example pod manifest from the Kubernetes documentation with ndots set to ‚Äú2‚Äù:  apiVersion: v1 kind: Pod metadata: namespace: default name: dns-example spec: containers: - name: test image: nginx dnsConfig: options: - name: ndots value: &quot;2&quot;   info While setting ndots to ‚Äú2‚Äù in your pod deployment is a reasonable place to start, this will not universally work in all situations and shouldn‚Äôt be applied across the entire cluster. The ndots configuration needs to be configured at the Pod or Deployment level. Reducing this setting at the Cluster level CoreDNS configuration is not recommended.  ","version":"Next","tagName":"h3"},{"title":"Inter AZ Network Optimization‚Äã","type":1,"pageTitle":"Networking for AI","url":"/ai-on-eks/docs/resources/networking#inter-az-network-optimization","content":" Some workloads may need to exchange data between Pods in the cluster, like Spark executors during the shuffle stage. If the Pods are spread across multiple Availability Zones (AZs), this shuffle operation can turn out to be very expensive, especially on Network I/O front. Hence, for these workloads, it is recommended to colocate executors or worker pods in the same AZ. Colocating workloads in the same AZ serves two main purposes:  Reduce inter-AZ traffic costsReduce network latency between executors/Pods  To have pods co-located on the same AZ, we can use podAffinity based scheduling constraints. The scheduling constraint preferredDuringSchedulingIgnoredDuringExecution can be enforced in the Pod spec. For example, ins Spark we can use a custom template for our driver and executor pods:  spec: executor: affinity: podAffinity: preferredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: sparkoperator.k8s.io/app-name operator: In values: - &lt;&lt;spark-app-name&gt;&gt; topologyKey: topology.kubernetes.io/zone ...   You can also leverage Kubernetes Topology Aware Routing to have Kubernetes services route traffic in more efficient means once pods have been created: https://aws.amazon.com/blogs/containers/exploring-the-effect-of-topology-aware-hints-on-network-traffic-in-amazon-elastic-kubernetes-service/  info Having all executors located in a single AZ, means that AZ will be a single point of failure. This is a trade off you should consider between lowering network cost and latency, and the event of an AZ failure interrupting workloads. If your workload is running on instances with constrained capacity you may consider using multiple AZs to avoid Insufficient Capacity errors. ","version":"Next","tagName":"h3"},{"title":"Dynamic Resource Allocation for GPUs on Amazon EKS","type":0,"sectionRef":"#","url":"/ai-on-eks/docs/resources/dynamic-resource-allocation","content":"","keywords":"","version":"Next"},{"title":"The GPU Scheduling Challenge in Kubernetes‚Äã","type":1,"pageTitle":"Dynamic Resource Allocation for GPUs on Amazon EKS","url":"/ai-on-eks/docs/resources/dynamic-resource-allocation#the-gpu-scheduling-challenge-in-kubernetes","content":" ","version":"Next","tagName":"h2"},{"title":"Current State: Traditional GPU Allocation‚Äã","type":1,"pageTitle":"Dynamic Resource Allocation for GPUs on Amazon EKS","url":"/ai-on-eks/docs/resources/dynamic-resource-allocation#current-state-traditional-gpu-allocation","content":" Kubernetes has rapidly evolved into the de facto standard for orchestrating AI/ML workloads across enterprise environments, with Amazon EKS emerging as the leading platform for managing GPU-accelerated infrastructure at scale. Organizations are running everything from small inference services to massive distributed training jobs on EKS clusters, leveraging GPU instances like P4d, P5, and the latest P6 series to power their machine learning pipelines.  However, despite Kubernetes' sophistication in managing containerized workloads, the traditional GPU scheduling model remains surprisingly primitive and creates significant operational challenges. The current approach treats GPUs as simple, atomic resources that can only be allocated in whole units, fundamentally mismatched with the diverse and evolving needs of modern AI workloads.  How Traditional GPU Scheduling Works:  Pods request GPUs using simple integer values: nvidia.com/gpu: 1Scheduler treats GPUs as opaque, indivisible resourcesEach workload gets exclusive access to entire GPU devicesNo awareness of actual resource requirements or GPU topology  The Problem with This Approach:Modern AI workloads have diverse requirements that don't fit this binary model:  Small inference jobs need only 2-4GB GPU memory but get allocated entire 80GB A100sLarge training jobs require coordinated multi-GPU communication via NVLink or IMEXMixed workloads could share GPUs efficiently but are forced into separate devices  ","version":"Next","tagName":"h3"},{"title":"The GPU Utilization Crisis‚Äã","type":1,"pageTitle":"Dynamic Resource Allocation for GPUs on Amazon EKS","url":"/ai-on-eks/docs/resources/dynamic-resource-allocation#the-gpu-utilization-crisis","content":" Critical Inefficiency in Production Even in high-demand clusters, GPU utilization frequently remains below 40%. This isn't a configuration issue: it's a fundamental limitation of how Kubernetes abstracts GPU resources.  Common symptoms of inefficient GPU allocation:  Queue starvation - Small inference jobs wait behind long-running training tasksResource fragmentation - GPU memory is stranded in unusable chunks across nodesTopology blindness - Multi-GPU jobs get suboptimal placement, degrading NVLink performanceCost explosion - Organizations overprovision GPUs to work around scheduling inefficiencies  üíé  ","version":"Next","tagName":"h3"},{"title":"Enter Dynamic Resource Allocation (DRA)‚Äã","type":1,"pageTitle":"Dynamic Resource Allocation for GPUs on Amazon EKS","url":"/ai-on-eks/docs/resources/dynamic-resource-allocation#enter-dynamic-resource-allocation-dra","content":" ","version":"Next","tagName":"h2"},{"title":"What DRA Changes‚Äã","type":1,"pageTitle":"Dynamic Resource Allocation for GPUs on Amazon EKS","url":"/ai-on-eks/docs/resources/dynamic-resource-allocation#what-dra-changes","content":" Dynamic Resource Allocation fundamentally transforms GPU scheduling in Kubernetes from a rigid, device-centric model to a flexible, workload-aware approach:  Traditional Approach:  resources: limits: nvidia.com/gpu: 1 # Get entire GPU, no customization   DRA Approach:  resourceClaims: - name: gpu-claim source: resourceClaimTemplateName: gpu-template # Detailed requirements   See examples section below for ResourceClaimTemplate configurations.  Namespace Requirement Critical: ResourceClaims must exist in the same namespace as the Pods that reference them. Cross-namespace resource claims are not supported.  ","version":"Next","tagName":"h3"},{"title":"Key DRA Innovations‚Äã","type":1,"pageTitle":"Dynamic Resource Allocation for GPUs on Amazon EKS","url":"/ai-on-eks/docs/resources/dynamic-resource-allocation#key-dra-innovations","content":" üéØ Fine-grained Resource Control Request specific GPU memory amounts (e.g., 16Gi out of 80Gi available)Specify compute requirements independent of memory needsDefine topology constraints for multi-GPU workloads Note: ResourceClaims and Pods must be in the same namespace üîÑ Per-Workload Sharing Strategies MPS - Concurrent small workloads with memory isolation Time-slicing - Workloads with different peak usage patterns MIG - Hardware-level isolation in multi-tenant environments Exclusive - Performance-critical training jobs üåê Topology-Aware Scheduling Understands NVLink connections between GPUsLeverages IMEX for Amazon EC2 P6e-GB200 UltraServer clustersOptimizes placement for distributed training workloads üöÄ Future-Proof Architecture Required for next-generation systems like Amazon EC2 P6e-GB200 UltraServersEnables advanced features like Multi-Node NVLinkSupports emerging GPU architectures and sharing technologies  .dra-innovations-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(320px, 1fr)); gap: 1.5rem; margin: 2rem 0; } .innovation-card { background: var(--ifm-background-surface-color); border: 1px solid var(--ifm-color-emphasis-300); border-radius: 12px; padding: 1.5rem; transition: all 0.3s ease; box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1); } .innovation-card:hover { transform: translateY(-4px); box-shadow: 0 8px 25px rgba(0, 0, 0, 0.15); } .innovation-card--primary { border-left: 4px solid var(--ifm-color-primary); } .innovation-card--secondary { border-left: 4px solid var(--ifm-color-secondary); } .innovation-card--success { border-left: 4px solid var(--ifm-color-success); } .innovation-card--warning { border-left: 4px solid var(--ifm-color-warning); } .innovation-card__header { display: flex; align-items: center; margin-bottom: 1rem; } .innovation-card__icon { font-size: 2rem; margin-right: 0.75rem; } .innovation-card__header h4 { margin: 0; font-size: 1.25rem; font-weight: 600; } .innovation-card__content { color: var(--ifm-color-content-secondary); } .innovation-card__features { margin: 0; padding-left: 1rem; } .innovation-card__features li { margin-bottom: 0.5rem; } .innovation-card__note { background: var(--ifm-color-warning-contrast-background); border: 1px solid var(--ifm-color-warning-contrast-border); border-radius: 6px; padding: 0.75rem; margin-top: 1rem; font-size: 0.875rem; } .strategy-grid { display: grid; grid-template-columns: 1fr 1fr; gap: 0.75rem; } .strategy-item { background: var(--ifm-color-emphasis-100); padding: 0.75rem; border-radius: 6px; font-size: 0.875rem; border-left: 3px solid var(--ifm-color-secondary); } @media (max-width: 768px) { .dra-innovations-grid { grid-template-columns: 1fr; } .strategy-grid { grid-template-columns: 1fr; } }  ","version":"Next","tagName":"h3"},{"title":"Understanding IMEX, ComputeDomains, and Amazon EC2 P6e-GB200 Multi-Node Scheduling‚Äã","type":1,"pageTitle":"Dynamic Resource Allocation for GPUs on Amazon EKS","url":"/ai-on-eks/docs/resources/dynamic-resource-allocation#understanding-imex-computedomains-and-amazon-ec2-p6e-gb200-multi-node-scheduling","content":" IMEX (NVIDIA Internode Memory Exchange/Management Service) is NVIDIA's orchestration service for GPU memory sharing across NVLink multi-node deployments. In Amazon EC2 P6e-GB200 UltraServer configurations, IMEX coordinates memory export and import operations between nodes, enabling direct GPU-to-GPU memory access across multiple compute nodes for massive AI model training with billions of parameters.  ComputeDomains represent logical groupings of interconnected GPUs that can communicate efficiently through high-bandwidth connections like NVLink or IMEX. DRA uses ComputeDomains to understand GPU topology and ensure workloads requiring multi-GPU coordination are scheduled on appropriately connected hardware.  Amazon EC2 P6e-GB200 Multi-Node Scheduling leverages DRA's topology awareness to coordinate workloads across multiple superchip nodes. Traditional GPU scheduling cannot understand these complex interconnect relationships, making DRA essential for optimal placement of distributed training jobs on Amazon EC2 P6e-GB200 UltraServer systems where proper GPU topology selection directly impacts training performance.  For detailed configuration examples and implementation guidance, see the AWS EKS AI/ML Best Practices documentation.  ","version":"Next","tagName":"h3"},{"title":"Implementation Considerations for EKS‚Äã","type":1,"pageTitle":"Dynamic Resource Allocation for GPUs on Amazon EKS","url":"/ai-on-eks/docs/resources/dynamic-resource-allocation#implementation-considerations-for-eks","content":" Now that we understand DRA's capabilities and advanced features like IMEX and ComputeDomains, let's explore the practical considerations for implementing DRA on Amazon EKS. The following sections address key decisions around node provisioning, migration strategies, and EKS-specific configurations that will determine your DRA deployment success.  ","version":"Next","tagName":"h2"},{"title":"Managed Node Groups vs Karpenter for P-Series GPU Instances and DRA‚Äã","type":1,"pageTitle":"Dynamic Resource Allocation for GPUs on Amazon EKS","url":"/ai-on-eks/docs/resources/dynamic-resource-allocation#managed-node-groups-vs-karpenter-for-p-series-gpu-instances-and-dra","content":" The choice between node provisioning methods for DRA isn't just about technical compatibility. It's fundamentally about how GPU capacity is purchased and utilized in enterprise AI workloads. Managed and Self-Managed Node Groups are currently the recommended approach for DRA because they align with the economics and operational patterns of high-end GPU instances.  Here's why: The majority of large GPU instances (P4d (A100), P5 (H100), P6 with B200, and P6e with GB200) are primarily available through AWS Capacity Block Reservations rather than on-demand pricing. When organizations purchase Capacity Blocks, they commit to paying for every second of GPU time until the reservation expires, regardless of whether the GPUs are actively utilized. This creates a fundamental mismatch with Karpenter's core value proposition of dynamic scaling based on workload demand. Spinning nodes down during low-demand periods doesn't save money. It actually wastes the reserved capacity you're already paying for.  Additionally, Karpenter doesn't yet support DRA scheduling (Issue #1231 tracks active development), making it incompatible with production DRA workloads. While Karpenter excels at cost optimization through dynamic scaling for general compute workloads, Capacity Block reservations require an &quot;always-on&quot; utilization strategy to maximize ROI: exactly what Managed Node Groups provide with their static capacity model.  The future picture is more optimistic: Karpenter's roadmap includes static node features that would make it suitable for Capacity Block scenarios. The community is actively working on manual node provisioning without workloads and static provisioning capabilities through RFCs like static provisioning and manual node provisioning. Once DRA support is added alongside these static provisioning capabilities, Karpenter could become the preferred choice for DRA workloads with Capacity Block ML reserved instances. Until then, Managed Node Groups with EKS-optimized AMIs (which come with pre-installed NVIDIA drivers) provide the most reliable foundation for DRA implementations.  ","version":"Next","tagName":"h3"},{"title":"DRA and Traditional GPU Allocation Coexistence‚Äã","type":1,"pageTitle":"Dynamic Resource Allocation for GPUs on Amazon EKS","url":"/ai-on-eks/docs/resources/dynamic-resource-allocation#dra-and-traditional-gpu-allocation-coexistence","content":" Yes, but with careful configuration to avoid conflicts. DRA and traditional GPU allocation can coexist on the same cluster, but this requires thoughtful setup to prevent resource double-allocation issues. NVIDIA's DRA driver is designed as an additional component alongside the GPU Operator, with selective enablement to avoid conflicts.  The recommended approach for gradual migration: Configure the NVIDIA DRA driver to enable only specific subsystems initially. For example, you can set resources.gpus.enabled=false to use traditional device plugins for GPU allocation while enabling DRA's ComputeDomain subsystem for Multi-Node NVLink capabilities. This allows teams to gain operational experience with DRA's advanced features without risking established GPU allocation workflows.  Key considerations for coexistence:  Avoid same-device conflicts: DRA and device plugins should not manage the same GPU devices simultaneouslySelective component enablement: Use NVIDIA DRA driver's modular design to enable features graduallyNode selector management: Configure node selectors carefully to prevent resource allocation conflictsTechnology Preview status: GPU allocation and sharing features are in Technology Preview (check NVIDIA DRA Driver GitHub for updates)  For migration planning, start with DRA's production-ready features like ComputeDomains for Multi-Node NVLink, while keeping traditional device plugins for core GPU allocation. Once DRA's GPU allocation reaches full support, gradually migrate workloads starting with development and inference services before moving mission-critical training jobs. NVIDIA and the Kubernetes community have designed DRA as the eventual replacement for device plugins, but the transition requires careful orchestration to maintain cluster stability.  ","version":"Next","tagName":"h3"},{"title":"Visual Comparison: Traditional vs DRA‚Äã","type":1,"pageTitle":"Dynamic Resource Allocation for GPUs on Amazon EKS","url":"/ai-on-eks/docs/resources/dynamic-resource-allocation#visual-comparison-traditional-vs-dra","content":" The diagram below illustrates how DRA fundamentally changes the scheduling flow:  Traditional Model: The pod directly requests an entire GPU via the node resource model. Scheduling and allocation are static, with no room for partial usage or workload intent.DRA Model: Pods express intent via templates; claims are dynamically generated and resolved with the help of a DRA-aware scheduler and device driver. Multiple workloads can share GPUs safely and efficiently, maximizing utilization.    ","version":"Next","tagName":"h3"},{"title":"Technical Capabilities Comparison‚Äã","type":1,"pageTitle":"Dynamic Resource Allocation for GPUs on Amazon EKS","url":"/ai-on-eks/docs/resources/dynamic-resource-allocation#technical-capabilities-comparison","content":" Capability üî¥ Traditional Device Plugin üü¢ Dynamic Resource Allocation (DRA) Resource Request Model ‚ùå Simple integers nvidia.com/gpu: 1 ‚úÖ Structured claims via ResourceClaimTemplate GPU Memory Specification ‚ùå All-or-nothing allocation ‚úÖ Memory-based constraints and selectors Sharing Configuration ‚ö†Ô∏è Static cluster-wide ConfigMaps ‚úÖ Per-workload sharing strategies Multi-GPU Topology Awareness ‚ùå No topology coordination ‚úÖ DeviceClass selectors for NVLink, IMEX Runtime Reconfiguration ‚ùå Requires pod deletion and redeployment ‚úÖ Dynamic reallocation without restarts MIG Support ‚ö†Ô∏è Limited - static partitions, manual setup ‚úÖ Full MIG profiles via dynamic claims  ‚öôÔ∏è  ","version":"Next","tagName":"h3"},{"title":"How DRA Actually Works: The Complete Technical Flow‚Äã","type":1,"pageTitle":"Dynamic Resource Allocation for GPUs on Amazon EKS","url":"/ai-on-eks/docs/resources/dynamic-resource-allocation#how-dra-actually-works-the-complete-technical-flow","content":" Dynamic Resource Allocation (DRA) extends Kubernetes scheduling with a modular, pluggable mechanism for handling GPU and other device resources. Rather than allocating integer units of opaque hardware, DRA introduces ResourceClaims, ResourceClaimTemplates, DeviceClasses, and ResourceSlices to express, match, and provision device requirements at runtime.  ","version":"Next","tagName":"h2"},{"title":"Step-by-step DRA Workflow‚Äã","type":1,"pageTitle":"Dynamic Resource Allocation for GPUs on Amazon EKS","url":"/ai-on-eks/docs/resources/dynamic-resource-allocation#step-by-step-dra-workflow","content":" DRA fundamentally changes how Kubernetes manages GPU resources through sophisticated orchestration:  1. Resource Discovery and Advertisement‚Äã  When NVIDIA DRA driver starts, it discovers available GPUs on each node and creates ResourceSlices that advertise device capabilities to the Kubernetes API server.  2. DeviceClass Registration‚Äã  The driver registers one or more DeviceClass objects to logically group GPU resources:  gpu.nvidia.com: Standard GPU resourcesmig.nvidia.com: Multi-Instance GPU partitionscompute-domain.nvidia.com: Cross-node GPU coordination  3. Resource Claim Creation‚Äã  ResourceClaimTemplates generate individual ResourceClaims for each pod, specifying:  Specific GPU memory requirementsSharing strategy (MPS, time-slicing, exclusive)Driver versions and compute capabilitiesTopology constraints for multi-GPU workloads  4. Intelligent Scheduling‚Äã  The DRA-aware scheduler evaluates pending ResourceClaims and queries available ResourceSlices across nodes:  Matches device properties and constraints using CEL expressionsEnsures sharing strategy compatibility with other running podsSelects optimal nodes considering topology, availability, and policy  5. Dynamic Allocation‚Äã  On the selected node, the DRA driver:  Sets up device access for the container (e.g., mounts MIG instance or configures MPS)Allocates shared vs. exclusive access as per claim configurationIsolates GPU slices securely between concurrent workloads  ","version":"Next","tagName":"h3"},{"title":"Deploying the Solution‚Äã","type":1,"pageTitle":"Dynamic Resource Allocation for GPUs on Amazon EKS","url":"/ai-on-eks/docs/resources/dynamic-resource-allocation#deploying-the-solution","content":" üëá In this example, you will provision JARK Cluster on Amazon EKS with DRA support 1 Prerequisites Install required tools and dependencies 2 Deploy Configure and run JARK stack installation 3 Verify Test your DRA deployment and validate functionality Prerequisites‚Äã Ensure that you have installed the following tools on your machine: AWS CLI - AWS Command Line Interfacekubectl - Kubernetes command-line toolterraform - Infrastructure as Code tool Deploy‚Äã 1. Clone the repository:‚Äã Clone the repository git clone https://github.com/awslabs/ai-on-eks.git Authentication Profile If you are using a profile for authentication, set your export AWS_PROFILE=&quot;&lt;PROFILE_name&gt;&quot; to the desired profile name 2. Review and customize configurations:‚Äã Check available addons in infra/base/terraform/variables.tfModify addon settings in infra/jark-stack/terraform/blueprint.tfvars as neededUpdate the AWS region in blueprint.tfvars Enable DRA Components: In the blueprint.tfvars file, uncomment the following lines: blueprint.tfvars enable_nvidia_dra_driver = true enable_nvidia_gpu_operator = true Automated Setup The NVIDIA GPU Operator includes all necessary components: NVIDIA Device PluginDCGM ExporterMIG ManagerGPU Feature DiscoveryNode Feature Discovery The NVIDIA DRA Driver is deployed as a separate Helm chart parallel to the GPU Operator. 1 Prerequisites Install required tools and dependencies 2 Deploy Configure and run JARK stack installation 3 Verify Test your DRA deployment and validate functionality 3. Navigate to the deployment directory and run the install script:‚Äã Deploy JARK Stack with DRA cd ai-on-eks/infra/jark-stack &amp;&amp; chmod +x install.sh ./install.sh This script will automatically provision and configure the following components: Amazon EKS Cluster with DRA (Dynamic Resource Allocation) feature gates enabled.Two GPU-managed node groups using Amazon Linux 2023 GPU AMIs:G6 Node Group: Intended for testing MPS and time-slicing strategies.P4d(e) Node Group: Intended for testing MIG-based GPU partitioning. ‚ö†Ô∏è Both node groups are initialized with zero nodes to avoid unnecessary cost. To test MPS/time-slicing, manually update the g6 node group‚Äôs min_size and desired_size via the EKS console.To test MIG, you need at least one p4d or p4de instance, which requires a Capacity Block Reservation (CBR). Edit the file: infra/base/terraform/eks.tf. Set your actual capacity_reservation_id and change the min_size for the MIG node group to 1 1 Prerequisites Install required tools and dependencies 2 Deploy Configure and run JARK stack installation 3 Verify Test your DRA deployment and validate functionality 4. Verify Deployment‚Äã Follow these verification steps to ensure your DRA deployment is working correctly: Step 1: Configure kubectl access Update your local kubeconfig to access the Kubernetes cluster: aws eks update-kubeconfig --name jark-stack # Replace with your EKS cluster name Step 2: Verify worker nodes First, let's verify that worker nodes are running in the cluster: kubectl get nodes Expected output: You should see two x86 instances from the core node group, plus any GPU instances (g6, p4d, etc.) that you manually scaled up via the EKS console. Step 3: Verify DRA components Run this command to verify all deployments, including the NVIDIA GPU Operator and NVIDIA DRA Driver: kubectl get deployments -A Expected output: All pods should be in Running state before proceeding to test the examples below. Instance compatibility for testing: Time-slicing and MPS: Any G5 or G6 instanceMIG partitioning: P-series instances (P4d or higher)IMEX use cases: P6e-GB200 UltraServers Once all components are running, you can start testing the various DRA examples mentioned in the following sections.  ","version":"Next","tagName":"h2"},{"title":"Component Architecture‚Äã","type":1,"pageTitle":"Dynamic Resource Allocation for GPUs on Amazon EKS","url":"/ai-on-eks/docs/resources/dynamic-resource-allocation#component-architecture","content":"   NVIDIA Tools The NVIDIA DRA Driver runs as an independent Helm chart parallel to the NVIDIA GPU Operator, not as part of it. Both components work together to provide comprehensive GPU management capabilities.  üé≤  ","version":"Next","tagName":"h3"},{"title":"GPU Sharing Strategies: Technical Deep Dive‚Äã","type":1,"pageTitle":"Dynamic Resource Allocation for GPUs on Amazon EKS","url":"/ai-on-eks/docs/resources/dynamic-resource-allocation#gpu-sharing-strategies-technical-deep-dive","content":" Understanding GPU sharing technologies is crucial for optimizing resource utilization. Each strategy provides different benefits and addresses specific use cases.  üíé Basic Allocation‚åõ Time-Slicingüåä Multi-Process Service (MPS)üèóÔ∏è Multi-Instance GPU (MIG) Basic GPU Allocation‚Äã Standard GPU allocation without sharing - each workload gets exclusive access to a complete GPU. This is the traditional model that provides maximum performance isolation. How to Deploy Basic Allocation: ResourceClaimTemplateBasic Pod basic-gpu-claim-template.yaml apiVersion: v1 kind: Namespace metadata: name: gpu-test1 --- apiVersion: resource.k8s.io/v1beta1 kind: ResourceClaimTemplate metadata: namespace: gpu-test1 name: single-gpu spec: spec: devices: requests: - name: gpu deviceClassName: gpu.nvidia.com Deploy the Example: Deploy Basic GPU Allocation kubectl apply -f basic-gpu-claim-template.yaml kubectl apply -f basic-gpu-pod.yaml kubectl get pods -n gpu-test1 -w Best For: Large model training requiring full GPU resourcesWorkloads that fully utilize GPU compute and memoryApplications requiring maximum performance isolationLegacy applications not designed for GPU sharing Strategy Selection Guide‚Äã Workload Type\tRecommended Strategy\tKey BenefitSmall Inference Jobs\tTime-slicing or MPS\tHigher GPU utilization Concurrent Small Models\tMPS\tTrue parallelism Production Multi-tenant\tMIG\tHardware isolation Large Model Training\tBasic Allocation\tMaximum performance Development/Testing\tTime-slicing\tFlexibility and simplicity    ","version":"Next","tagName":"h2"},{"title":"Cleanup‚Äã","type":1,"pageTitle":"Dynamic Resource Allocation for GPUs on Amazon EKS","url":"/ai-on-eks/docs/resources/dynamic-resource-allocation#cleanup","content":" ","version":"Next","tagName":"h2"},{"title":"Removing DRA Components‚Äã","type":1,"pageTitle":"Dynamic Resource Allocation for GPUs on Amazon EKS","url":"/ai-on-eks/docs/resources/dynamic-resource-allocation#removing-dra-components","content":" 1Ô∏è‚É£ Clean Up DRA Examples2Ô∏è‚É£ JARK Stack Cleanup Remove all DRA example workloads: Clean up DRA workloads # Delete all pods first to ensure proper cleanup kubectl delete pod inference-pod-1 -n timeslicing-gpu --ignore-not-found kubectl delete pod training-pod-2 -n timeslicing-gpu --ignore-not-found kubectl delete pod mps-workload -n mps-gpu --ignore-not-found kubectl delete pod mig-workload -n mig-gpu --ignore-not-found kubectl delete pod basic-gpu-pod -n gpu-test1 --ignore-not-found # Delete ResourceClaimTemplates kubectl delete resourceclaimtemplate timeslicing-gpu-template -n timeslicing-gpu --ignore-not-found kubectl delete resourceclaimtemplate mps-gpu-template -n mps-gpu --ignore-not-found kubectl delete resourceclaimtemplate mig-gpu-template -n mig-gpu --ignore-not-found kubectl delete resourceclaimtemplate basic-gpu-template -n gpu-test1 --ignore-not-found # Delete any remaining ResourceClaims kubectl delete resourceclaims --all --all-namespaces --ignore-not-found # Delete ConfigMaps (contain scripts) kubectl delete configmap timeslicing-scripts-configmap -n timeslicing-gpu --ignore-not-found # Finally delete namespaces kubectl delete namespace timeslicing-gpu --ignore-not-found kubectl delete namespace mps-gpu --ignore-not-found kubectl delete namespace mig-gpu --ignore-not-found kubectl delete namespace gpu-test1 --ignore-not-found # Verify cleanup kubectl get resourceclaims --all-namespaces kubectl get resourceclaimtemplates --all-namespaces   üîß Troubleshooting Common Issues üîç Pods Stuck in Pending‚ö†Ô∏è GPU Sharing Conflictsüìä Performance Issues Issue: Pods with ResourceClaims stuck in Pending state Diagnosis: # Check ResourceClaim status kubectl get resourceclaims --all-namespaces -o wide # Check DRA driver logs kubectl logs -n gpu-operator -l app=nvidia-dra-driver --tail=100 # Verify DeviceClasses exist kubectl get deviceclasses Resolution: # Restart DRA driver pods kubectl delete pods -n gpu-operator -l app=nvidia-dra-driver # Check node GPU availability kubectl describe nodes | grep -A 10 &quot;Allocatable&quot;     ","version":"Next","tagName":"h3"},{"title":"Conclusion‚Äã","type":1,"pageTitle":"Dynamic Resource Allocation for GPUs on Amazon EKS","url":"/ai-on-eks/docs/resources/dynamic-resource-allocation#conclusion","content":" Dynamic Resource Allocation represents a fundamental shift from rigid GPU allocation to intelligent, workload-aware resource management. By leveraging structured ResourceClaims and vendor-specific drivers, DRA unlocks the GPU utilization rates necessary for cost-effective AI/ML operations at enterprise scale.  üöÄ Ready to Transform Your GPU Infrastructure? With the simplified JARK-based deployment approach, organizations can implement production-grade DRA capabilities in three steps, transforming their GPU infrastructure from a static resource pool into a dynamic, intelligent platform optimized for modern AI workloads.  The combination of EKS's managed infrastructure, NVIDIA's driver ecosystem, and Kubernetes' declarative model creates a powerful foundation for next-generation AI workloads - from small inference jobs to multi-node distributed training on GB200 superchips. ","version":"Next","tagName":"h2"}],"options":{"id":"default"}}